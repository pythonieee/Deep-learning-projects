{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7cf0052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "741488ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"For DeepLearning.csv\")\n",
    "df.drop(columns=[\"Unnamed: 0\"] , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78d1ab4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# stop_words = stopwords.words(\"english\")\n",
    "# # stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0337dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    if tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    if tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    if tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ccb98f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is for the classical nlp approach \n",
    "# import re\n",
    "# from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "# from nltk import pos_tag \n",
    "# lematizer = WordNetLemmatizer()\n",
    "# stemmer = SnowballStemmer(\"english\")\n",
    "# end = df.index.stop\n",
    "# for i in range(end) :\n",
    "#     sentence = re.sub(r\"\\s+\", \" \", re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", re.sub(r\"<[^>]+>\",\" \", df.iloc[i,0]))).strip().lower()\n",
    "#     sentence = sentence.split()\n",
    "#     cleaned_sentence = [word for word in sentence if word.lower() not in stop_words and word!=\"\" ]\n",
    "#     pos_tags = pos_tag(cleaned_sentence)\n",
    "#     cleaned_sentence = [lematizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "#     df.iloc[i,0] = cleaned_sentence\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4671f6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.read_csv(\"IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c640a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 =  pd.read_csv(\"IMDB Dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87c0285d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2cd47f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\site-packages\\transformers\\activations_tf.py:22\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m      4\u001b[39m model = SentenceTransformer(\u001b[33m\"\u001b[39m\u001b[33mall-MiniLM-L6-v2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(df.index.stop)) : \n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\site-packages\\sentence_transformers\\__init__.py:15\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     export_dynamic_quantized_onnx_model,\n\u001b[32m     12\u001b[39m     export_optimized_onnx_model,\n\u001b[32m     13\u001b[39m     export_static_quantized_openvino_model,\n\u001b[32m     14\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcross_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     CrossEncoder,\n\u001b[32m     17\u001b[39m     CrossEncoderModelCardData,\n\u001b[32m     18\u001b[39m     CrossEncoderTrainer,\n\u001b[32m     19\u001b[39m     CrossEncoderTrainingArguments,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mLoggingHandler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mCrossEncoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_card\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderModelCardData\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:35\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_onnx_model, load_openvino_model\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcross_encoder\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfit_mixin\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FitMixin\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcross_encoder\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_card\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderModelCardData, generate_model_card\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcross_encoder\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     38\u001b[39m     cross_encoder_init_args_decorator,\n\u001b[32m     39\u001b[39m     cross_encoder_predict_rank_args_decorator,\n\u001b[32m     40\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\fit_mixin.py:20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenization_utils_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BatchEncoding\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcross_encoder\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtraining_args\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainingArguments\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mNoDuplicatesDataLoader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mSentenceLabelDataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceLabelDataset\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mSentenceEvaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceEvaluator\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\site-packages\\sentence_transformers\\datasets\\__init__.py:13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mDenoisingAutoEncoderDataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DenoisingAutoEncoderDataset\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mNoDuplicatesDataLoader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mParallelSentencesDataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mSentenceLabelDataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceLabelDataset\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mSentencesDataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentencesDataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\site-packages\\sentence_transformers\\datasets\\ParallelSentencesDataset.py:19\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InputExample\n\u001b[32m     22\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:36\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdynamic_module_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_class_from_dynamic_module, get_relative_import_files\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_card\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformerModelCardData, generate_model_card\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Router\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mModule\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Module\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\site-packages\\sentence_transformers\\model_card.py:26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautonotebook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainerCallback\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CodeCarbonCallback\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodelcard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_markdown_table\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer_callback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainerControl, TrainerState\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2317\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2347\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2344\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\importlib\\__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\site-packages\\transformers\\integrations\\integration_utils.py:60\u001b[39m\n\u001b[32m     57\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TFPreTrainedModel\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2317\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2347\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2344\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\importlib\\__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:38\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataCollatorWithPadding, DefaultDataCollator\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations_tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdynamic_module_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m custom_object_save\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\site-packages\\transformers\\activations_tf.py:27\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras.__version__).major > \u001b[32m2\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     28\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`pip install tf-keras`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m         )\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_gelu\u001b[39m(x):\n\u001b[32m     35\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[33;03m    Gaussian Error Linear Unit. Original Implementation of the gelu activation function in Google Bert repo when\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03m    initially created. For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03m    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) Also see\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33;03m    https://huggingface.co/papers/1606.08415\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "for i in tqdm(range(df.index.stop)) : \n",
    "    embeddings = model.encode(df.iloc[i,0], convert_to_tensor=True)\n",
    "    df.iloc[i,0] = embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9f5698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "3733f2eb-a045-457a-876f-4bfc38def0bc",
       "rows": [
        [
         "0",
         "tensor([ 3.0099e-02,  5.0418e-02, -7.6095e-02,  6.3799e-02,  6.9763e-02,\n         5.7326e-02,  4.5658e-02,  1.1051e-02,  5.5232e-02, -5.3259e-02,\n        -2.9711e-02, -3.0100e-02, -2.6937e-02,  1.6186e-02, -6.4985e-02,\n         1.6402e-02,  1.0893e-01, -1.2097e-01, -6.4850e-02,  7.3704e-02,\n        -1.1406e-02, -7.5563e-03,  1.0752e-01, -1.1337e-02, -4.6153e-03,\n         6.7254e-02,  6.2314e-03,  4.7682e-02, -1.6508e-02, -1.9542e-02,\n         2.7021e-02,  2.7865e-02,  6.6362e-02,  1.6526e-02, -2.9187e-02,\n        -1.0602e-01,  7.4955e-02,  2.5691e-02,  2.4066e-02, -1.4622e-02,\n         3.6206e-02,  6.4527e-02, -9.0263e-03, -2.1518e-02, -7.2759e-02,\n        -7.3675e-02,  1.0067e-02,  3.2432e-02, -2.2223e-02, -6.5740e-02,\n         3.8527e-02, -7.5015e-03,  1.6177e-02, -8.9402e-03, -3.0642e-02,\n        -5.0795e-02,  3.6789e-02,  2.2168e-02, -3.5665e-02, -9.8492e-03,\n         6.6504e-02, -2.7434e-02,  1.6399e-02,  6.9970e-02,  2.3416e-02,\n        -3.3984e-02, -3.0171e-02,  3.4930e-02,  1.9099e-02, -5.1639e-02,\n        -3.9281e-02, -4.4749e-02,  7.5614e-02, -2.2741e-02,  1.8410e-02,\n         7.6977e-02,  1.6856e-02, -7.5526e-02, -1.1743e-02,  2.2873e-02,\n         3.9359e-02, -6.8305e-02, -3.7189e-02, -6.9390e-03,  3.8142e-02,\n        -4.1968e-02, -4.0951e-02, -1.1051e-01, -1.9946e-02, -5.1842e-03,\n        -3.5154e-02,  6.7910e-02,  6.3426e-02, -3.3260e-02,  1.3342e-01,\n         3.2383e-02,  2.5193e-02,  2.0217e-02, -2.0506e-02,  3.9508e-02,\n         8.3616e-03, -1.5387e-02,  3.5607e-02, -1.0996e-01,  8.6032e-03,\n        -7.1756e-02,  2.4937e-02, -7.5596e-03, -6.0474e-02,  5.3733e-02,\n         1.0578e-02, -6.1856e-02,  2.8084e-02,  3.5226e-02,  9.8510e-02,\n         1.5347e-02, -1.2418e-02,  1.4956e-02, -4.7003e-02,  9.0169e-02,\n         6.2954e-02,  2.8817e-03, -6.6397e-02,  1.0392e-01,  3.9068e-02,\n         4.5342e-02, -3.2427e-02,  1.7277e-33, -6.2551e-02, -3.3349e-02,\n        -1.2824e-01, -2.1742e-02,  9.6932e-02, -7.0246e-03, -8.0609e-02,\n        -4.2923e-02,  4.7197e-02,  9.3356e-02, -3.9672e-02, -9.8568e-04,\n         1.1395e-03, -4.7286e-02,  1.6130e-02,  1.9257e-02, -2.1820e-02,\n        -7.9979e-03, -8.2588e-02,  3.0865e-02,  1.1358e-03,  4.9238e-02,\n        -9.9018e-02, -5.5245e-02, -1.3390e-01,  4.9634e-02, -2.3603e-02,\n        -2.4817e-02,  6.7058e-02,  8.3905e-03, -8.4714e-03,  1.0902e-01,\n        -4.1057e-02, -1.1444e-02, -2.6930e-02,  6.4524e-03,  5.0224e-02,\n        -4.6924e-02, -5.6887e-03, -9.1722e-03, -2.5520e-02,  4.6116e-02,\n        -4.3962e-02,  1.0932e-01,  5.9496e-02,  7.3358e-02, -2.5479e-02,\n         4.4813e-02, -1.8439e-02,  3.6511e-02, -6.9661e-02, -1.0062e-01,\n         3.1419e-02,  9.1499e-02,  2.6327e-04,  3.5412e-02, -1.1430e-01,\n         2.6386e-02,  5.6322e-02,  3.0241e-02, -6.7144e-02,  6.3035e-02,\n         1.8545e-02,  5.1433e-02, -4.9530e-02, -2.2040e-02, -1.5061e-02,\n         4.5522e-02, -2.0758e-02,  2.7131e-02, -3.5625e-02,  2.5833e-02,\n         6.1324e-02,  2.4698e-02,  3.7101e-03, -3.2064e-02,  9.7256e-02,\n         3.6317e-02,  3.3223e-02,  1.7042e-02,  3.1009e-02, -8.3873e-03,\n         6.9510e-02, -4.2763e-02,  5.4201e-04, -8.3956e-02,  5.2633e-02,\n        -1.2086e-02,  1.3429e-02,  6.4846e-03,  2.3157e-02, -5.9032e-02,\n         6.5395e-04, -7.9438e-03, -6.3596e-02, -2.7412e-33,  5.7644e-02,\n        -1.1716e-01, -4.0148e-02, -5.4270e-02,  9.5401e-03,  4.3639e-02,\n        -9.9392e-02,  3.7762e-02,  3.8480e-02, -1.9777e-02, -1.2442e-02,\n        -2.2227e-02,  6.4597e-02,  5.7315e-02,  3.6486e-02, -1.0564e-01,\n         5.6160e-02,  4.2551e-03,  4.8713e-04,  4.6626e-02,  4.7851e-03,\n        -5.3338e-02, -1.2755e-01, -5.4217e-02,  4.5744e-02,  3.3658e-02,\n        -3.2486e-02, -3.8606e-02, -5.5939e-02,  7.6759e-02,  8.4918e-03,\n         8.3712e-02, -2.2937e-02,  3.2007e-02, -5.5804e-02,  7.0457e-02,\n         1.2794e-01, -5.1909e-02, -1.2934e-01, -5.5049e-02, -2.6248e-02,\n        -7.1278e-02, -1.3629e-01,  2.0266e-02,  4.8496e-03,  6.5965e-02,\n        -3.8816e-02,  6.3841e-02, -4.8602e-02, -7.3339e-02, -3.5964e-02,\n         2.2835e-02, -3.7281e-02,  4.6933e-02,  2.8374e-02, -3.5145e-02,\n         1.7536e-02, -2.8572e-02, -9.2981e-02,  1.7798e-02, -2.2092e-02,\n        -8.7576e-03, -2.2646e-02, -6.4827e-02, -7.5242e-03, -7.7001e-02,\n         9.9308e-04, -3.2649e-02,  6.1298e-03,  3.2791e-03, -5.0194e-02,\n        -1.7842e-02, -3.5611e-02, -1.7976e-03,  7.5653e-02,  5.1162e-02,\n         2.7053e-02, -7.6900e-02, -4.5563e-02, -5.2648e-03, -1.3701e-02,\n         2.6058e-02, -1.6585e-02,  2.7553e-02,  7.2984e-02,  9.7142e-02,\n         2.0667e-03,  6.6996e-02, -1.6902e-02,  6.7556e-02, -4.7407e-03,\n        -6.7417e-02, -3.0446e-02, -5.0149e-02, -2.2970e-02, -5.9512e-08,\n        -1.5858e-02,  3.6317e-02,  6.1557e-02,  1.3310e-03,  3.9949e-02,\n         6.9640e-02, -5.8504e-02,  1.0511e-02, -1.7574e-02,  8.0536e-02,\n        -1.5743e-02,  2.5834e-02, -9.1821e-03, -1.0931e-02,  9.7229e-03,\n         2.6044e-02,  5.0618e-02, -6.4747e-02, -1.0395e-02,  4.8319e-02,\n        -3.9521e-02, -1.7926e-02,  6.5153e-02, -5.0216e-02,  6.7726e-02,\n         2.7494e-03, -9.1587e-02, -1.8038e-02, -1.1848e-02,  2.4374e-02,\n        -7.3641e-03, -2.2113e-02, -2.7647e-02,  8.1886e-02, -6.9093e-02,\n        -5.6135e-02,  4.0634e-02,  2.5630e-02, -2.1962e-02,  3.6284e-02,\n        -5.2998e-03, -1.1517e-01,  1.7082e-02, -1.6796e-02,  3.4880e-02,\n         1.5015e-02, -2.4544e-02,  1.3244e-02,  4.1450e-02, -8.0464e-02,\n         2.0808e-02, -5.1248e-02, -5.4735e-02,  6.9769e-02,  7.0753e-02,\n        -7.3306e-02, -3.6767e-02,  1.1863e-02, -2.8055e-04, -1.3242e-02,\n         7.7603e-02, -1.1902e-02, -4.6354e-02, -2.7616e-02], device='cuda:0')",
         null
        ],
        [
         "1",
         "tensor([-1.2202e-02,  5.1961e-02, -1.8610e-02, -5.5581e-02,  2.9653e-02,\n         1.0538e-01,  2.4230e-02,  4.3511e-03, -2.5548e-02, -4.9326e-03,\n        -3.3596e-03,  5.2264e-03, -1.9769e-02,  2.8789e-02, -8.0749e-03,\n        -3.5371e-03,  1.2245e-01, -2.3102e-02, -3.4610e-02,  3.4977e-02,\n        -9.7596e-03, -7.8659e-02,  6.4165e-02,  2.9475e-02, -5.9611e-02,\n        -7.0487e-02, -7.4703e-03,  7.4173e-04,  8.7083e-04, -5.6269e-02,\n        -6.2634e-02,  9.7703e-02, -2.5919e-02, -1.5210e-02, -2.4055e-02,\n         7.6886e-02,  4.4073e-02,  3.5080e-02,  2.4751e-02,  8.0017e-03,\n         2.4454e-02, -2.8947e-02, -6.2127e-03, -3.8492e-02,  2.2768e-03,\n        -7.9292e-02,  5.7749e-02, -8.5563e-02,  2.3015e-02, -6.6948e-02,\n        -7.6366e-02,  4.8356e-02,  3.2766e-02, -4.8333e-02, -1.5546e-02,\n        -5.0458e-02,  3.5356e-02, -8.4823e-02,  3.6311e-02, -3.9576e-02,\n        -5.8158e-03, -1.4235e-02, -6.5573e-05,  4.0963e-02,  1.9146e-02,\n        -7.2840e-02, -2.9894e-02,  1.1329e-03,  6.7281e-02, -2.5249e-02,\n        -1.0396e-01, -1.5630e-02, -1.4649e-02, -4.4672e-02, -4.8893e-03,\n        -2.4745e-02, -4.6159e-02, -3.8389e-02, -6.9934e-02, -8.8795e-02,\n         3.6710e-02, -1.0148e-01, -1.0319e-02,  4.9698e-02, -7.4041e-02,\n        -1.3029e-02,  1.2607e-01, -9.6526e-02,  4.2068e-03,  7.4937e-02,\n        -2.8180e-02,  3.5342e-02, -9.3860e-02, -2.3943e-02,  5.1029e-02,\n        -1.4374e-02, -1.9811e-02, -2.0592e-02, -4.6143e-02,  7.0720e-02,\n         9.5764e-02, -7.5486e-02, -1.6829e-04, -1.0122e-01, -2.1916e-02,\n        -7.1921e-02,  7.6121e-02, -5.8824e-03, -2.7054e-02, -6.0227e-02,\n         2.1202e-02,  2.5436e-02, -8.0517e-03,  8.0842e-03,  7.2622e-02,\n        -2.4636e-03, -1.0262e-02, -1.3783e-02,  1.4151e-02,  6.3827e-02,\n         1.3350e-01,  6.6784e-02, -2.9548e-02,  9.0447e-02, -1.4708e-02,\n        -1.2357e-02,  1.3859e-02,  2.3769e-33,  7.1673e-02,  1.0722e-01,\n        -1.0383e-02, -7.5821e-03,  5.4777e-02, -1.3089e-02, -1.3397e-02,\n        -4.2472e-02, -1.3034e-02, -3.8344e-03,  7.2317e-02, -3.1530e-02,\n        -2.7653e-02, -5.1660e-02, -3.5363e-02,  9.3803e-03, -4.9914e-02,\n        -4.2305e-02,  5.4693e-02,  6.7604e-02, -1.0635e-01,  1.2743e-01,\n        -9.1775e-03,  1.2638e-03, -3.2196e-02,  4.8426e-02,  5.8376e-02,\n         3.1942e-02, -1.4783e-02,  4.0746e-02, -4.6913e-02,  7.1010e-02,\n        -4.2328e-02, -1.9144e-02,  7.1185e-02,  2.8078e-02, -6.6405e-02,\n        -4.7851e-02,  6.0660e-02,  2.1727e-02, -5.5434e-02,  4.3191e-02,\n        -3.5663e-02,  5.4475e-03, -8.6097e-02,  2.3711e-02,  1.7359e-02,\n         6.4106e-02, -4.1678e-03,  6.7298e-02,  3.8894e-02,  3.0803e-02,\n         3.2206e-02, -4.9683e-02, -4.4678e-02,  4.9372e-02, -9.5456e-03,\n        -4.9999e-02,  7.1535e-02, -7.9063e-02,  5.6948e-02,  7.4916e-02,\n        -3.4041e-02,  6.1752e-02, -2.8611e-02,  3.6152e-02,  1.3751e-02,\n         2.1895e-02, -5.4461e-02, -3.8381e-02, -1.5769e-01, -2.5025e-02,\n         4.2773e-02, -7.9569e-02,  5.0020e-02, -3.9418e-02,  9.4431e-03,\n        -3.9295e-02, -2.1709e-02,  2.5349e-02, -3.3245e-02,  8.9721e-02,\n         3.6111e-02, -3.8101e-02, -6.2043e-02, -1.1324e-01,  4.6981e-02,\n        -2.4567e-03,  1.6670e-03,  8.1581e-02,  1.0713e-02, -9.2170e-02,\n         4.9818e-03, -2.3785e-02, -4.5587e-02, -4.0195e-33,  4.4654e-02,\n        -3.6726e-02, -4.8192e-03,  4.2278e-02, -4.4430e-02, -1.2202e-02,\n        -4.3726e-02,  5.6567e-02,  7.0185e-02,  8.1902e-02, -1.6724e-02,\n         1.2838e-02, -2.6574e-02, -3.9085e-02, -8.0290e-02, -7.5310e-02,\n         5.9504e-02,  2.9267e-02,  5.0663e-02,  3.8725e-02,  1.7921e-01,\n        -1.3910e-02, -5.9963e-03, -5.9402e-03, -2.6792e-02,  1.0853e-01,\n         1.1617e-02,  6.6935e-02, -9.9070e-02, -6.1811e-02,  3.7757e-02,\n        -3.8086e-02, -5.0761e-02, -5.7661e-02, -7.0349e-02,  4.1629e-02,\n         3.3998e-02,  1.9783e-02, -3.6349e-02, -6.4034e-03, -5.1863e-03,\n        -8.3018e-02,  3.2586e-02,  3.1161e-02, -2.2900e-02, -1.9344e-02,\n         2.5500e-02,  4.1857e-02, -1.0445e-01,  1.3107e-02, -7.2198e-02,\n         6.3370e-02, -8.7676e-02, -3.8550e-02, -2.6816e-02, -3.9313e-02,\n        -4.5321e-03, -6.2096e-03,  4.8157e-02,  8.4699e-02, -7.7460e-02,\n         2.1994e-03, -4.7010e-02, -1.0184e-01, -1.6413e-03,  3.3520e-02,\n         9.4265e-03, -2.9427e-02,  3.2157e-03,  3.1183e-02, -1.5719e-02,\n        -9.8625e-03, -2.2071e-02, -4.4514e-02, -2.8614e-02,  4.6753e-02,\n         3.2671e-02, -5.9914e-02, -1.3296e-02,  3.1080e-02,  1.3566e-02,\n        -1.1788e-01, -1.9992e-02,  1.6310e-02,  9.8482e-02,  1.1903e-01,\n        -2.8468e-02,  8.8206e-02, -3.6722e-02,  8.3873e-02,  5.3040e-02,\n         4.9243e-02,  3.6739e-02, -4.7181e-02,  1.5730e-02, -6.0497e-08,\n        -2.8531e-03, -3.1693e-02,  2.8870e-04, -4.4239e-02, -6.0015e-02,\n        -6.0738e-02, -2.4138e-02, -9.9230e-03, -5.1586e-02,  8.7920e-03,\n         6.4986e-02, -8.7743e-02,  5.9209e-02, -3.9494e-02,  4.1297e-02,\n         1.8928e-02, -9.7522e-03,  4.1353e-02, -5.7576e-02,  2.5495e-02,\n         2.7430e-02, -6.1349e-02,  6.4260e-02, -2.1059e-02,  1.3346e-02,\n         3.6384e-02,  1.9588e-02, -1.1483e-01, -5.3411e-02,  2.8463e-02,\n         5.4992e-02,  4.4851e-02, -2.3156e-03,  4.7600e-02, -2.7749e-02,\n        -4.2209e-02,  5.2001e-02,  8.1305e-03,  6.1803e-02, -3.1287e-02,\n        -1.2799e-02, -1.0239e-02, -4.5996e-03,  4.4934e-02,  9.5099e-03,\n         8.5835e-03,  5.2165e-02, -2.5876e-02,  2.2911e-02, -7.1129e-03,\n         3.9498e-02, -2.6454e-03, -2.1997e-03,  9.3417e-02,  2.3332e-02,\n         4.9877e-02,  4.4755e-02,  2.6594e-03, -6.3592e-02,  1.0959e-02,\n         4.3024e-02,  1.8271e-02, -6.0139e-02,  1.1601e-02], device='cuda:0')",
         null
        ],
        [
         "2",
         "tensor([ 1.4258e-02, -7.9138e-02,  3.3383e-04,  9.6844e-03,  7.0698e-02,\n         7.2463e-02,  8.6674e-02,  6.7928e-02, -2.8781e-02,  1.0053e-02,\n        -6.8389e-02,  9.7887e-02,  5.0277e-02,  2.9016e-02,  4.7163e-02,\n         3.6098e-02,  6.4138e-02, -7.8392e-02,  7.8001e-03,  7.2492e-02,\n         4.1920e-02,  1.9853e-02,  5.1500e-02, -3.9018e-02, -3.8949e-02,\n        -8.9998e-02,  3.3891e-02, -2.3949e-02, -1.0110e-01,  5.2227e-02,\n         2.5043e-02,  5.4034e-02, -2.0563e-02, -4.1746e-02, -2.7160e-02,\n        -2.1112e-02,  4.2897e-02,  1.2751e-03, -2.7279e-02,  1.7860e-02,\n        -2.8197e-02,  1.4456e-02, -3.9559e-02, -7.4536e-03, -4.7240e-02,\n        -1.3005e-01, -1.1987e-02, -4.7168e-02, -1.0476e-02, -3.7110e-02,\n        -3.1944e-02,  2.2533e-02,  4.9780e-02, -1.9828e-02, -5.2006e-02,\n         8.4801e-02, -2.9861e-03, -7.8550e-03,  5.4513e-02, -1.1072e-02,\n        -2.1307e-02, -6.2813e-02, -1.1264e-03, -4.7024e-02,  7.6430e-02,\n        -6.0417e-02, -3.9071e-02,  2.1235e-02, -2.0818e-02,  4.9923e-02,\n        -5.3954e-02, -1.7818e-02, -7.2374e-02,  1.0068e-02, -3.8073e-02,\n        -2.9994e-02, -5.6235e-02, -2.7575e-02,  4.8994e-02,  3.7508e-02,\n        -1.7273e-02, -6.5101e-02,  5.4991e-02,  1.0389e-02, -6.6584e-02,\n         2.7138e-02, -1.2127e-02, -1.3768e-02, -1.2348e-02,  1.4242e-02,\n        -1.1196e-01, -1.9857e-03,  6.6195e-03,  2.1892e-02,  8.9295e-02,\n         1.8978e-02, -5.7424e-02, -8.1473e-02, -3.8044e-02,  1.7095e-02,\n         2.7434e-02,  1.9128e-04,  6.5044e-03, -8.3137e-02,  4.1239e-02,\n        -1.2344e-02,  6.0856e-02,  6.2328e-03,  3.3053e-02,  2.2409e-02,\n         5.3160e-02,  3.2591e-02,  3.5001e-02,  2.4205e-02,  8.5266e-02,\n         1.3505e-02,  4.7990e-02,  3.3436e-02,  7.6083e-03, -2.0830e-03,\n         1.6111e-01,  7.6289e-02,  1.7994e-02,  2.5358e-02,  4.2197e-03,\n        -8.1412e-02, -1.8852e-04,  4.4446e-33, -3.8746e-02,  4.9203e-02,\n         4.0180e-02,  2.0891e-03,  5.0434e-03,  9.2730e-02,  1.2586e-03,\n         2.7539e-02,  1.4275e-02, -2.4341e-02, -5.7556e-02, -5.8565e-02,\n        -1.0961e-01, -1.6518e-02, -1.4274e-02,  7.5259e-02, -1.9173e-02,\n         2.0189e-02,  1.0684e-01,  5.6778e-02,  8.1522e-03,  9.8632e-02,\n         1.2568e-03, -4.3492e-03, -1.4467e-01, -1.3450e-02,  4.2977e-02,\n        -6.3270e-03,  3.6513e-02,  1.5802e-02, -3.4130e-02,  8.3569e-02,\n         1.7272e-02, -3.7140e-02,  4.4542e-02, -3.7669e-02, -8.3881e-02,\n        -1.6763e-02,  9.6296e-03,  5.1167e-02, -2.6302e-03, -1.4810e-02,\n        -1.2974e-02,  1.0692e-01, -9.3739e-02,  1.0031e-01,  1.8356e-02,\n         6.4478e-03, -5.5272e-02,  6.4284e-02, -4.2481e-02,  7.0434e-02,\n         4.9591e-02, -4.5099e-03, -2.3478e-02,  4.9930e-02,  2.7445e-02,\n        -6.7039e-02,  5.5280e-02, -8.9384e-02,  6.4394e-02, -2.8607e-03,\n         1.6635e-02, -9.5438e-04, -3.4459e-02,  6.9320e-02,  2.8600e-02,\n        -1.2258e-02, -5.6206e-02,  1.9709e-02, -6.4904e-02,  3.8220e-02,\n        -4.1953e-02, -2.7609e-02, -1.2331e-02,  5.9472e-03, -1.2021e-02,\n        -3.9293e-02, -1.6126e-02, -3.3818e-02,  5.7729e-03, -1.9247e-02,\n        -4.9914e-02,  3.2717e-02,  9.3239e-03, -1.0535e-01,  3.9321e-02,\n        -4.5439e-02,  1.3303e-02,  2.6285e-02,  1.1824e-01, -1.1792e-01,\n         4.5028e-02, -1.2110e-02, -1.2043e-02, -4.3924e-33,  1.9749e-02,\n        -8.0763e-02, -2.6612e-02,  6.0704e-03,  4.6046e-02, -8.0582e-03,\n        -9.1492e-02, -5.2192e-02,  5.1605e-02, -5.0656e-02, -1.2960e-02,\n        -3.1781e-02, -5.1451e-02, -2.0233e-02,  5.2170e-02, -4.5959e-04,\n         4.2382e-02,  1.6282e-03, -5.0651e-02,  1.8800e-02,  4.3829e-02,\n         3.9694e-02, -2.5386e-02, -3.2216e-02, -8.3965e-02,  5.7558e-02,\n         1.6238e-02, -7.3964e-03, -2.6110e-02,  7.6833e-02, -1.8988e-02,\n         9.8132e-02,  4.0550e-02, -1.8594e-02, -9.0549e-03,  1.9089e-02,\n        -2.0842e-03, -1.0371e-01,  5.4000e-02, -5.3098e-02,  4.3699e-02,\n        -1.4624e-02, -1.2617e-02,  1.1322e-03,  1.8808e-02, -1.0501e-01,\n        -3.7221e-02, -2.1355e-02, -7.1289e-02,  5.5532e-02, -1.0864e-01,\n         5.8840e-02, -5.9779e-02,  1.5594e-02, -2.3305e-02, -1.3208e-01,\n        -1.4912e-02,  6.3980e-02,  4.2337e-02,  2.6825e-02, -1.5848e-01,\n        -2.2836e-02, -6.4694e-02, -2.5974e-02, -1.0226e-02, -5.4103e-02,\n         2.1500e-02, -3.1145e-02, -2.7288e-02, -2.5120e-02,  7.9215e-03,\n        -6.9414e-02, -4.6017e-03,  7.0498e-02,  3.5950e-03, -5.5876e-02,\n         2.7633e-02, -5.5332e-03, -5.4838e-02,  5.0354e-03, -5.1879e-02,\n         8.7368e-03,  2.6980e-02,  3.8047e-02,  2.6811e-02,  1.1870e-01,\n        -4.8318e-02,  1.6160e-02, -4.4070e-02,  1.1047e-01,  6.1322e-02,\n        -2.7261e-02,  1.7717e-02, -7.0405e-03,  3.7227e-02, -6.3497e-08,\n        -7.1054e-02, -1.3707e-02, -6.2049e-02, -3.4849e-02,  2.6591e-02,\n        -2.9888e-02, -2.2785e-02,  9.5946e-02,  2.1663e-02,  2.3636e-02,\n         2.7258e-02,  5.5175e-02,  8.0142e-02,  2.9829e-02, -4.9486e-03,\n         7.0982e-02,  4.7479e-02,  6.4778e-03, -1.1223e-02,  5.1237e-02,\n         1.5896e-02, -3.0070e-02,  1.2220e-01, -1.3255e-02, -8.8448e-02,\n         8.1165e-03, -2.5366e-02, -6.7754e-02,  5.6718e-02,  2.2032e-02,\n        -7.1934e-03,  1.6900e-02, -6.8501e-02, -6.1825e-03, -1.1956e-01,\n        -3.5267e-02,  1.0464e-01, -2.2593e-02,  2.2985e-02,  3.8157e-02,\n         1.1877e-02, -5.2731e-02,  1.1793e-03,  2.3660e-02,  5.8878e-02,\n        -5.9265e-03,  4.0343e-02,  9.6563e-04,  7.1337e-02,  5.6298e-02,\n        -4.7868e-02,  5.7931e-02,  3.6687e-02,  7.3872e-03, -2.3973e-02,\n        -8.3601e-02, -1.3390e-01,  6.8398e-02, -6.6574e-02,  4.5020e-02,\n         6.6704e-02, -3.0550e-02, -2.1409e-04, -7.9596e-02], device='cuda:0')",
         null
        ],
        [
         "3",
         "tensor([-4.1720e-02,  1.0464e-02, -4.8342e-02, -1.7788e-02,  5.7979e-02,\n         7.0723e-02,  2.1130e-02, -2.0535e-02,  9.2240e-02, -4.5720e-02,\n         5.0700e-02, -2.0757e-02, -1.4438e-02,  1.1684e-02, -2.6287e-02,\n         4.4090e-02,  9.1269e-02, -1.4993e-03, -3.8511e-02,  7.1604e-02,\n        -7.1205e-03, -8.2940e-02,  6.3151e-02,  1.6734e-03,  9.7731e-04,\n        -7.0754e-03,  2.4587e-02, -1.7377e-02, -5.7831e-02, -2.5757e-02,\n         7.2805e-02, -3.9580e-02, -4.2118e-02, -5.8598e-02,  3.2671e-03,\n         4.8061e-03,  1.0885e-03, -3.4902e-02, -1.3259e-02,  2.2695e-03,\n         2.6847e-02,  4.1013e-02,  6.7385e-02, -8.6836e-02,  3.3426e-02,\n        -4.1370e-02, -3.3602e-02, -4.7760e-02,  9.1488e-02, -7.8722e-02,\n        -7.9137e-02, -1.0993e-01,  1.0090e-02,  1.8614e-02,  2.0001e-02,\n        -4.2152e-02, -1.7103e-03, -4.9994e-02, -2.2753e-02, -1.3412e-02,\n        -5.8443e-02,  3.0688e-02, -1.5131e-02,  7.5499e-02,  6.3809e-02,\n         9.6498e-03, -2.9523e-02, -3.5630e-03,  1.7472e-02,  1.0653e-01,\n        -6.7670e-02,  2.6033e-02,  4.4461e-02,  1.6386e-02, -6.8150e-02,\n        -5.3103e-02, -2.5868e-02, -1.5335e-02,  1.6030e-02, -5.8239e-02,\n         3.8415e-02, -9.4584e-02, -3.6320e-02, -7.0203e-03, -9.0684e-03,\n        -6.9910e-03,  6.4695e-02,  1.0328e-02, -1.8054e-02,  8.0118e-02,\n        -1.2001e-01, -3.0573e-02, -4.0478e-02,  6.8879e-02,  5.0063e-02,\n        -6.3888e-03, -5.8782e-02,  1.4299e-02, -3.5688e-02, -7.9724e-03,\n         3.4968e-03, -6.6385e-02,  4.3448e-02, -4.7192e-02,  6.3728e-02,\n         1.3759e-02,  3.1411e-02, -1.6635e-02, -5.5925e-03, -1.5522e-03,\n        -6.7314e-02,  5.0287e-02,  3.3801e-02,  5.3534e-02,  4.3035e-02,\n        -4.1593e-02,  4.8004e-02, -2.3541e-02, -4.9956e-03,  6.2710e-04,\n         1.5302e-01,  2.3461e-02, -6.1165e-02,  2.4465e-02,  4.2547e-02,\n        -2.9688e-03, -1.7838e-02,  1.5031e-33,  7.7752e-02, -1.4081e-03,\n        -4.9293e-02,  6.0524e-02,  1.2969e-02, -7.0989e-03, -7.9764e-02,\n         5.1576e-04, -2.9663e-02,  9.1535e-02, -4.5457e-02, -9.5792e-02,\n         2.2128e-02,  3.8300e-02,  1.2206e-02,  8.0197e-03, -8.3685e-02,\n        -2.0175e-02,  4.4854e-02,  1.2719e-02, -9.5625e-02,  1.5969e-02,\n        -1.3566e-01,  1.7387e-02, -1.6561e-02,  3.7770e-02,  1.9083e-02,\n         8.2377e-02, -3.0463e-02,  2.0357e-03, -4.7565e-02,  1.0622e-01,\n        -3.6077e-02, -2.8636e-02,  4.4432e-02, -1.0142e-03, -2.1638e-02,\n         3.2572e-02, -8.1590e-03,  3.6260e-02, -3.5920e-02,  6.6549e-02,\n        -2.1805e-01,  1.4223e-02, -7.2649e-04,  4.4975e-02, -4.1860e-04,\n         4.7900e-02, -9.3387e-03,  3.7945e-02,  1.0195e-01, -5.3510e-02,\n         4.0246e-02,  3.8545e-02, -3.8577e-02,  1.1648e-01,  3.2631e-02,\n        -9.2405e-02,  1.2640e-02,  9.7643e-03,  1.2888e-01, -2.9382e-02,\n        -1.3281e-02, -1.2904e-02, -7.1030e-02,  3.0132e-02,  5.4223e-02,\n        -2.3154e-02,  3.5157e-02, -6.5584e-02, -5.1055e-02, -4.8941e-02,\n        -3.1326e-02, -6.6556e-02,  4.4051e-02,  2.7009e-02, -2.8710e-02,\n         1.5862e-02, -2.0067e-02,  6.8960e-03,  2.8195e-02, -4.4057e-02,\n        -8.8994e-03, -5.4367e-03, -7.8820e-02, -3.9467e-02, -3.1065e-02,\n        -3.1309e-02, -6.7754e-02,  3.9647e-02, -1.8728e-03, -1.6194e-02,\n         7.9090e-03, -3.7770e-02,  5.8835e-02, -4.2039e-33,  8.5381e-02,\n        -1.1519e-02, -7.4341e-02, -4.7468e-02,  6.5319e-03, -4.7816e-02,\n        -2.7601e-02,  4.7870e-03,  1.7560e-01, -4.2638e-02, -6.5025e-02,\n        -4.0028e-03, -4.0153e-02, -4.3755e-02,  4.1703e-02, -6.1375e-02,\n         4.7898e-03, -7.8654e-03, -4.0707e-02,  3.8555e-03,  5.2479e-02,\n         2.4203e-02, -5.3015e-02, -3.9913e-02,  1.0389e-01,  4.8894e-02,\n        -3.7914e-02,  9.1548e-02, -6.6564e-03, -8.1307e-02, -5.2449e-02,\n        -1.6300e-02,  1.2649e-01,  8.9488e-04, -8.5958e-02,  9.2485e-02,\n         6.4618e-02, -4.6525e-02,  2.1797e-02, -4.9376e-02,  6.3035e-02,\n        -2.0625e-02, -7.4299e-02,  4.5183e-02,  1.5385e-02,  7.3381e-02,\n         4.8597e-02,  2.0757e-02, -1.7756e-02,  7.6343e-02, -5.7688e-02,\n         6.6678e-03, -1.1240e-02, -1.4070e-02, -9.6462e-02, -6.9708e-02,\n        -3.1880e-02, -1.0105e-01,  2.2682e-02,  5.8712e-02,  4.2246e-02,\n        -2.0352e-02, -3.0074e-02, -6.3402e-02, -1.0849e-01,  3.8804e-02,\n        -3.1973e-02, -4.2962e-02,  2.7636e-02,  7.4069e-02, -4.3446e-02,\n         2.9910e-02, -4.9031e-02, -1.1226e-02,  2.7555e-02, -2.4394e-02,\n        -3.4458e-02, -1.4610e-02,  3.0151e-02, -1.5901e-02, -7.2414e-03,\n        -5.4998e-02,  4.5000e-02,  1.9810e-02,  3.3151e-02,  7.7325e-02,\n        -2.7034e-02, -5.4914e-03, -9.7722e-03, -2.6384e-02,  1.5664e-02,\n        -1.4197e-02,  8.5896e-02, -6.0253e-03, -8.7731e-02, -5.9148e-08,\n         1.6313e-02, -4.7506e-02, -8.5338e-02, -5.5241e-02, -1.7265e-02,\n         3.3493e-02, -9.8663e-02,  9.6017e-02,  6.7094e-02,  3.6981e-02,\n        -3.1188e-02,  9.0597e-02, -1.5398e-02, -1.9479e-02, -7.6779e-02,\n         5.4260e-02, -3.2984e-02, -5.2914e-02, -2.7223e-02, -2.1526e-02,\n         3.6196e-02,  4.0562e-03,  8.6401e-03,  6.8019e-02,  4.2257e-02,\n        -3.3061e-02, -4.3443e-02, -1.7957e-02,  2.6966e-03,  1.6675e-02,\n        -3.6302e-02,  6.1381e-02,  6.4312e-04, -9.5852e-03, -3.1222e-02,\n         4.3405e-02,  9.5544e-02,  4.7259e-02,  1.0446e-01, -2.2630e-02,\n         1.2851e-02, -3.7035e-02,  2.5067e-02, -3.2443e-02, -8.1559e-03,\n         1.1721e-02,  8.5603e-02, -4.1716e-02,  5.5981e-02,  5.3762e-03,\n         3.2520e-02, -3.4272e-03, -1.1003e-01,  6.3377e-02,  4.5699e-02,\n        -3.8654e-02,  1.4555e-03, -2.9480e-03,  2.2475e-03,  2.2376e-02,\n         7.3445e-02, -4.2014e-03,  1.8375e-02,  2.0006e-02], device='cuda:0')",
         null
        ],
        [
         "4",
         "tensor([-3.1675e-02,  6.4242e-03, -3.4045e-02, -4.1488e-02, -7.1840e-03,\n         7.5291e-02,  7.0351e-02, -1.1238e-02,  1.0784e-01, -4.0606e-02,\n        -3.5596e-02,  1.1431e-03, -6.3583e-03,  1.5656e-02, -2.3818e-02,\n        -4.6525e-02, -2.7856e-02, -2.9693e-02,  2.1547e-02,  7.1556e-02,\n        -4.5467e-04, -1.0352e-01, -3.5540e-02, -4.5054e-03, -6.9870e-02,\n         1.2137e-02,  5.8655e-02, -2.9207e-02, -1.0989e-01,  5.0651e-02,\n         3.1732e-02,  1.5335e-01, -5.6979e-02, -1.0913e-02, -3.0109e-03,\n         9.0291e-02,  4.2468e-02,  6.3272e-02,  2.4548e-02, -6.3681e-03,\n         2.2305e-02, -4.5503e-02,  6.9217e-02, -3.6645e-02, -6.6215e-02,\n        -6.9563e-02,  3.6141e-02, -4.0011e-02,  1.2584e-02, -7.1741e-02,\n        -8.9849e-02,  1.2470e-02, -1.9063e-02, -1.6091e-02,  1.0584e-02,\n         3.6852e-02,  6.2734e-02, -4.7509e-02, -1.7186e-02, -3.3927e-02,\n        -4.7126e-03, -1.7747e-02,  1.0014e-02,  3.2232e-02,  1.6311e-01,\n        -3.7305e-02, -9.6806e-02,  1.4421e-02, -8.5248e-02, -3.0200e-02,\n         3.6523e-02, -4.0753e-02, -4.1772e-02, -1.0335e-01, -2.0966e-02,\n        -3.7795e-02, -3.5493e-02, -3.0866e-02, -3.8191e-03, -5.9414e-02,\n         5.7259e-02, -9.3762e-02, -3.9587e-02,  6.0759e-02, -2.4580e-02,\n         2.9681e-02,  2.2984e-02, -4.4261e-02,  6.0405e-02,  7.2758e-02,\n        -8.1382e-02,  2.1675e-02, -1.2912e-01,  3.1094e-03,  4.8446e-03,\n        -4.0133e-02, -6.7357e-02,  7.0381e-02,  1.5792e-02,  5.1726e-02,\n        -2.1627e-02, -2.7617e-04,  8.7262e-02, -6.6408e-03,  3.5615e-02,\n        -6.4401e-02, -3.8155e-02,  2.0535e-02, -1.4800e-02, -2.0849e-03,\n        -7.8997e-02, -2.2220e-02, -3.0689e-02,  3.9173e-02,  8.8317e-02,\n        -7.1103e-03, -4.2090e-02, -5.0411e-02,  1.0006e-02,  4.9811e-02,\n         9.7290e-02,  3.6667e-02, -7.8654e-02,  3.8282e-02, -1.8641e-02,\n        -7.3435e-02, -6.0488e-02,  2.4476e-33, -4.1901e-02,  4.6089e-03,\n         6.2927e-02,  5.7185e-02,  2.9811e-02,  2.6262e-02, -7.3162e-02,\n         3.6855e-02, -6.4876e-02,  1.4342e-02, -4.0413e-02, -1.4256e-02,\n        -1.7477e-02, -1.7309e-02, -7.0441e-02, -1.5911e-02, -7.6811e-02,\n        -5.2931e-02,  9.1866e-02, -1.0457e-02,  4.5139e-02,  5.1435e-02,\n        -6.8312e-02, -5.3781e-02, -6.4726e-02, -6.9348e-02, -3.8503e-03,\n        -4.0620e-03,  6.6259e-02, -1.4162e-02, -2.8348e-02,  1.1310e-01,\n         9.5380e-03, -6.4538e-02,  5.1642e-02,  7.7834e-03, -6.6102e-02,\n        -8.9582e-03,  2.5937e-02,  4.9920e-02, -5.8736e-02, -1.7808e-02,\n         1.5339e-03,  4.9231e-02, -3.8737e-02,  1.1425e-01,  2.3449e-02,\n         5.3665e-03, -1.2711e-02,  7.5360e-02,  5.1434e-02, -1.5534e-02,\n        -7.8509e-03, -2.2294e-02, -8.9326e-02, -5.5745e-02,  1.9681e-02,\n        -7.3319e-02, -3.8924e-02, -2.1336e-02,  4.9651e-02, -1.4301e-02,\n        -1.6214e-02, -2.1852e-02,  1.9994e-02,  4.1387e-02,  6.1824e-02,\n        -1.3219e-02,  8.8981e-03,  8.6593e-02, -7.3868e-02,  7.3470e-02,\n         6.9039e-02,  1.9161e-02,  2.7084e-02,  1.2009e-02,  5.1974e-02,\n         5.0864e-02, -2.9580e-02, -1.0844e-02, -6.4020e-02,  5.3557e-02,\n        -2.3031e-02,  4.1567e-02,  4.7532e-02,  1.2891e-02,  4.5086e-02,\n        -9.2438e-02, -1.0528e-02,  1.0651e-01,  8.3728e-02, -8.5319e-02,\n        -3.2150e-02, -4.4005e-03, -1.1719e-02, -5.8345e-33,  2.0655e-02,\n        -1.3813e-01,  8.4925e-03, -2.2248e-02, -1.5858e-02,  8.2375e-03,\n        -4.3558e-02,  7.5955e-02,  1.3006e-01,  7.4444e-02, -4.5286e-02,\n         1.3540e-02,  1.0989e-01,  4.2957e-02, -4.0029e-02, -7.1277e-02,\n         7.1732e-02, -4.9499e-02, -1.4581e-04, -1.8361e-02,  1.0050e-01,\n         5.9481e-02, -6.0972e-02, -3.0280e-02, -5.0811e-02,  4.5138e-02,\n         2.1775e-02,  2.9797e-02, -7.8201e-02, -1.0682e-02, -3.1016e-02,\n        -4.4318e-02, -6.3763e-02, -1.8570e-02, -8.1212e-02,  1.1437e-01,\n         3.2774e-02, -2.3064e-02, -3.9432e-02,  2.4812e-02,  1.4802e-02,\n        -2.7077e-02, -8.2251e-02,  3.1431e-04,  1.3465e-02, -2.1174e-02,\n        -3.6579e-02, -1.5992e-02, -6.6263e-02,  6.1017e-03,  7.4759e-03,\n         9.6475e-03, -3.9846e-02, -6.5782e-02,  5.2077e-03,  3.7048e-02,\n         6.5817e-02, -1.8901e-02,  1.3285e-03,  1.9354e-02, -3.3517e-02,\n         5.5534e-02, -6.8025e-02,  4.8375e-02, -4.3587e-02, -2.9807e-02,\n        -7.7538e-03, -9.1687e-02,  5.9022e-02,  3.7755e-03, -2.2202e-02,\n        -1.3548e-02, -8.2026e-02, -3.7098e-02,  6.9780e-03,  7.2627e-02,\n        -9.8858e-03,  5.1810e-03,  6.8932e-03, -1.0006e-02, -3.1197e-02,\n        -4.0365e-02,  4.8020e-02,  1.9330e-02, -3.7064e-03,  1.2582e-01,\n         1.3924e-02,  9.1516e-03, -2.9718e-02, -2.0512e-02,  7.5322e-03,\n         4.6253e-02,  7.1052e-02, -8.1789e-02, -9.1938e-02, -6.5296e-08,\n        -6.9241e-02, -1.6962e-02, -6.4955e-02, -4.7570e-02, -2.8558e-02,\n        -5.7496e-02,  3.3254e-02, -4.8110e-02,  5.9230e-02,  1.1765e-01,\n        -1.1154e-03,  1.7323e-02,  2.9879e-02,  1.7966e-02, -2.2228e-02,\n         6.1094e-02,  9.0958e-02, -1.1557e-01, -3.9467e-02,  5.2688e-02,\n         9.0666e-03,  3.0920e-02,  1.4010e-02, -1.5343e-02, -8.2158e-03,\n         1.7214e-02, -2.7976e-02, -3.8266e-03, -4.1438e-02,  8.2218e-02,\n        -2.1282e-02,  3.7895e-03, -5.8465e-02, -1.0647e-02,  4.4627e-02,\n        -4.7614e-02,  4.5269e-03,  3.9121e-03,  1.1753e-02,  3.2058e-02,\n         5.9096e-02, -3.0333e-02, -8.1145e-02,  3.8076e-02,  9.8783e-02,\n         1.4575e-02,  3.4668e-02, -1.2722e-02,  2.2095e-02, -2.3817e-02,\n        -2.2049e-02,  7.2114e-02,  1.3627e-02, -4.7414e-02,  9.9553e-04,\n        -3.6740e-02, -4.3364e-02,  1.0013e-01, -3.5285e-02,  3.8236e-03,\n         1.9260e-02,  1.6122e-02, -2.1858e-02,  2.0596e-02], device='cuda:0')",
         null
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensor([ 3.0099e-02,  5.0418e-02, -7.6095e-02,...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensor([-1.2202e-02,  5.1961e-02, -1.8610e-02,...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tensor([ 1.4258e-02, -7.9138e-02,  3.3383e-04,...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tensor([-4.1720e-02,  1.0464e-02, -4.8342e-02,...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tensor([-3.1675e-02,  6.4242e-03, -3.4045e-02,...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  tensor([ 3.0099e-02,  5.0418e-02, -7.6095e-02,...        NaN\n",
       "1  tensor([-1.2202e-02,  5.1961e-02, -1.8610e-02,...        NaN\n",
       "2  tensor([ 1.4258e-02, -7.9138e-02,  3.3383e-04,...        NaN\n",
       "3  tensor([-4.1720e-02,  1.0464e-02, -4.8342e-02,...        NaN\n",
       "4  tensor([-3.1675e-02,  6.4242e-03, -3.4045e-02,...        NaN"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2b21f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2[\"sentiment\"] = df_2[\"sentiment\"].map({\"positive\" :1 , \"negative\" :0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb16c03e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m X = np.vstack(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreview\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      2\u001b[39m y = df_2[\u001b[33m\"\u001b[39m\u001b[33msentiment\u001b[39m\u001b[33m\"\u001b[39m].values\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\site-packages\\pandas\\core\\series.py:4943\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4808\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4810\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4815\u001b[39m     **kwargs,\n\u001b[32m   4816\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4817\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4818\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4819\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4934\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4935\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4936\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4937\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4939\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4941\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\site-packages\\pandas\\core\\apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\site-packages\\pandas\\core\\apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\site-packages\\pandas\\core\\base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vedant\\anaconda3\\envs\\Pytorch_cuda_venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m X = np.vstack(df[\u001b[33m\"\u001b[39m\u001b[33mreview\u001b[39m\u001b[33m\"\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m().cpu().numpy()))\n\u001b[32m      2\u001b[39m y = df_2[\u001b[33m\"\u001b[39m\u001b[33msentiment\u001b[39m\u001b[33m\"\u001b[39m].values\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "X = np.vstack(df[\"review\"].apply(lambda x: x.detach().cpu().numpy()))\n",
    "y = df_2[\"sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e7b3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000,), (50000, 384))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b71620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"For DeepLearning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b18d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00770cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class Classifier_modelV0(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_features=384 , out_features=1)\n",
    "        \n",
    "\n",
    "    def forward(self , x) :\n",
    "        return self.layer1(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e98d699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aecc255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = torch.from_numpy(X).type(torch.float32)\n",
    "X = torch.nn.functional.normalize(X, p=2, dim=1)\n",
    "y = torch.from_numpy(y).type(torch.float32)\n",
    "X_train , X_test , y_train , y_test = train_test_split(X , y , test_size=0.25 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e1a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "model_0 = Classifier_modelV0().to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = Adam(params=model_0.parameters() , lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047d0f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy (a classification metric)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "    acc = (correct / len(y_pred)) * 100 \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b868b8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.69424, Accuracy: 47.09% | Test loss: 0.69376, Test acc: 48.49%\n",
      "Epoch: 10 | Loss: 0.68870, Accuracy: 62.19% | Test loss: 0.68823, Test acc: 62.54%\n",
      "Epoch: 20 | Loss: 0.68330, Accuracy: 68.01% | Test loss: 0.68284, Test acc: 67.97%\n",
      "Epoch: 30 | Loss: 0.67803, Accuracy: 71.22% | Test loss: 0.67760, Test acc: 71.10%\n",
      "Epoch: 40 | Loss: 0.67290, Accuracy: 72.92% | Test loss: 0.67248, Test acc: 73.00%\n",
      "Epoch: 50 | Loss: 0.66789, Accuracy: 73.92% | Test loss: 0.66750, Test acc: 73.98%\n",
      "Epoch: 60 | Loss: 0.66301, Accuracy: 74.46% | Test loss: 0.66265, Test acc: 74.52%\n",
      "Epoch: 70 | Loss: 0.65825, Accuracy: 74.79% | Test loss: 0.65792, Test acc: 74.74%\n",
      "Epoch: 80 | Loss: 0.65362, Accuracy: 75.14% | Test loss: 0.65331, Test acc: 75.14%\n",
      "Epoch: 90 | Loss: 0.64911, Accuracy: 75.34% | Test loss: 0.64882, Test acc: 75.53%\n",
      "Epoch: 100 | Loss: 0.64472, Accuracy: 75.54% | Test loss: 0.64445, Test acc: 75.71%\n",
      "Epoch: 110 | Loss: 0.64044, Accuracy: 75.76% | Test loss: 0.64019, Test acc: 75.92%\n",
      "Epoch: 120 | Loss: 0.63626, Accuracy: 75.86% | Test loss: 0.63604, Test acc: 75.96%\n",
      "Epoch: 130 | Loss: 0.63220, Accuracy: 75.95% | Test loss: 0.63199, Test acc: 76.02%\n",
      "Epoch: 140 | Loss: 0.62824, Accuracy: 76.05% | Test loss: 0.62805, Test acc: 76.06%\n",
      "Epoch: 150 | Loss: 0.62438, Accuracy: 76.15% | Test loss: 0.62421, Test acc: 76.20%\n",
      "Epoch: 160 | Loss: 0.62061, Accuracy: 76.21% | Test loss: 0.62046, Test acc: 76.29%\n",
      "Epoch: 170 | Loss: 0.61694, Accuracy: 76.28% | Test loss: 0.61681, Test acc: 76.47%\n",
      "Epoch: 180 | Loss: 0.61336, Accuracy: 76.37% | Test loss: 0.61325, Test acc: 76.52%\n",
      "Epoch: 190 | Loss: 0.60987, Accuracy: 76.42% | Test loss: 0.60978, Test acc: 76.62%\n",
      "Epoch: 200 | Loss: 0.60647, Accuracy: 76.49% | Test loss: 0.60639, Test acc: 76.66%\n",
      "Epoch: 210 | Loss: 0.60315, Accuracy: 76.55% | Test loss: 0.60309, Test acc: 76.74%\n",
      "Epoch: 220 | Loss: 0.59991, Accuracy: 76.60% | Test loss: 0.59986, Test acc: 76.76%\n",
      "Epoch: 230 | Loss: 0.59675, Accuracy: 76.64% | Test loss: 0.59672, Test acc: 76.82%\n",
      "Epoch: 240 | Loss: 0.59367, Accuracy: 76.72% | Test loss: 0.59365, Test acc: 76.86%\n",
      "Epoch: 250 | Loss: 0.59066, Accuracy: 76.75% | Test loss: 0.59066, Test acc: 76.91%\n",
      "Epoch: 260 | Loss: 0.58772, Accuracy: 76.79% | Test loss: 0.58774, Test acc: 77.01%\n",
      "Epoch: 270 | Loss: 0.58486, Accuracy: 76.86% | Test loss: 0.58489, Test acc: 77.12%\n",
      "Epoch: 280 | Loss: 0.58206, Accuracy: 76.92% | Test loss: 0.58210, Test acc: 77.10%\n",
      "Epoch: 290 | Loss: 0.57932, Accuracy: 76.98% | Test loss: 0.57938, Test acc: 77.22%\n",
      "Epoch: 300 | Loss: 0.57665, Accuracy: 77.01% | Test loss: 0.57672, Test acc: 77.26%\n",
      "Epoch: 310 | Loss: 0.57404, Accuracy: 77.00% | Test loss: 0.57413, Test acc: 77.34%\n",
      "Epoch: 320 | Loss: 0.57149, Accuracy: 77.04% | Test loss: 0.57159, Test acc: 77.38%\n",
      "Epoch: 330 | Loss: 0.56899, Accuracy: 77.11% | Test loss: 0.56911, Test acc: 77.40%\n",
      "Epoch: 340 | Loss: 0.56656, Accuracy: 77.17% | Test loss: 0.56669, Test acc: 77.40%\n",
      "Epoch: 350 | Loss: 0.56417, Accuracy: 77.19% | Test loss: 0.56432, Test acc: 77.49%\n",
      "Epoch: 360 | Loss: 0.56184, Accuracy: 77.26% | Test loss: 0.56200, Test acc: 77.52%\n",
      "Epoch: 370 | Loss: 0.55957, Accuracy: 77.35% | Test loss: 0.55973, Test acc: 77.62%\n",
      "Epoch: 380 | Loss: 0.55734, Accuracy: 77.38% | Test loss: 0.55752, Test acc: 77.68%\n",
      "Epoch: 390 | Loss: 0.55516, Accuracy: 77.43% | Test loss: 0.55535, Test acc: 77.72%\n",
      "Epoch: 400 | Loss: 0.55302, Accuracy: 77.48% | Test loss: 0.55323, Test acc: 77.81%\n",
      "Epoch: 410 | Loss: 0.55093, Accuracy: 77.52% | Test loss: 0.55115, Test acc: 77.82%\n",
      "Epoch: 420 | Loss: 0.54889, Accuracy: 77.57% | Test loss: 0.54912, Test acc: 77.85%\n",
      "Epoch: 430 | Loss: 0.54689, Accuracy: 77.60% | Test loss: 0.54713, Test acc: 77.90%\n",
      "Epoch: 440 | Loss: 0.54493, Accuracy: 77.66% | Test loss: 0.54518, Test acc: 77.92%\n",
      "Epoch: 450 | Loss: 0.54301, Accuracy: 77.71% | Test loss: 0.54327, Test acc: 77.92%\n",
      "Epoch: 460 | Loss: 0.54113, Accuracy: 77.77% | Test loss: 0.54140, Test acc: 77.98%\n",
      "Epoch: 470 | Loss: 0.53929, Accuracy: 77.79% | Test loss: 0.53957, Test acc: 78.04%\n",
      "Epoch: 480 | Loss: 0.53748, Accuracy: 77.85% | Test loss: 0.53777, Test acc: 78.08%\n",
      "Epoch: 490 | Loss: 0.53571, Accuracy: 77.87% | Test loss: 0.53601, Test acc: 78.09%\n",
      "Epoch: 500 | Loss: 0.53398, Accuracy: 77.91% | Test loss: 0.53429, Test acc: 78.13%\n",
      "Epoch: 510 | Loss: 0.53228, Accuracy: 77.95% | Test loss: 0.53260, Test acc: 78.13%\n",
      "Epoch: 520 | Loss: 0.53061, Accuracy: 78.00% | Test loss: 0.53094, Test acc: 78.13%\n",
      "Epoch: 530 | Loss: 0.52898, Accuracy: 78.03% | Test loss: 0.52932, Test acc: 78.15%\n",
      "Epoch: 540 | Loss: 0.52738, Accuracy: 78.06% | Test loss: 0.52772, Test acc: 78.18%\n",
      "Epoch: 550 | Loss: 0.52581, Accuracy: 78.07% | Test loss: 0.52616, Test acc: 78.21%\n",
      "Epoch: 560 | Loss: 0.52427, Accuracy: 78.08% | Test loss: 0.52463, Test acc: 78.26%\n",
      "Epoch: 570 | Loss: 0.52275, Accuracy: 78.11% | Test loss: 0.52312, Test acc: 78.26%\n",
      "Epoch: 580 | Loss: 0.52127, Accuracy: 78.14% | Test loss: 0.52165, Test acc: 78.34%\n",
      "Epoch: 590 | Loss: 0.51981, Accuracy: 78.17% | Test loss: 0.52020, Test acc: 78.38%\n",
      "Epoch: 600 | Loss: 0.51838, Accuracy: 78.18% | Test loss: 0.51878, Test acc: 78.48%\n",
      "Epoch: 610 | Loss: 0.51698, Accuracy: 78.21% | Test loss: 0.51738, Test acc: 78.49%\n",
      "Epoch: 620 | Loss: 0.51560, Accuracy: 78.25% | Test loss: 0.51601, Test acc: 78.54%\n",
      "Epoch: 630 | Loss: 0.51425, Accuracy: 78.29% | Test loss: 0.51466, Test acc: 78.60%\n",
      "Epoch: 640 | Loss: 0.51292, Accuracy: 78.33% | Test loss: 0.51334, Test acc: 78.61%\n",
      "Epoch: 650 | Loss: 0.51161, Accuracy: 78.34% | Test loss: 0.51204, Test acc: 78.68%\n",
      "Epoch: 660 | Loss: 0.51033, Accuracy: 78.37% | Test loss: 0.51077, Test acc: 78.67%\n",
      "Epoch: 670 | Loss: 0.50907, Accuracy: 78.39% | Test loss: 0.50951, Test acc: 78.68%\n",
      "Epoch: 680 | Loss: 0.50783, Accuracy: 78.43% | Test loss: 0.50828, Test acc: 78.73%\n",
      "Epoch: 690 | Loss: 0.50661, Accuracy: 78.45% | Test loss: 0.50707, Test acc: 78.72%\n",
      "Epoch: 700 | Loss: 0.50542, Accuracy: 78.50% | Test loss: 0.50588, Test acc: 78.74%\n",
      "Epoch: 710 | Loss: 0.50424, Accuracy: 78.52% | Test loss: 0.50471, Test acc: 78.76%\n",
      "Epoch: 720 | Loss: 0.50309, Accuracy: 78.53% | Test loss: 0.50356, Test acc: 78.77%\n",
      "Epoch: 730 | Loss: 0.50195, Accuracy: 78.55% | Test loss: 0.50243, Test acc: 78.82%\n",
      "Epoch: 740 | Loss: 0.50083, Accuracy: 78.58% | Test loss: 0.50132, Test acc: 78.89%\n",
      "Epoch: 750 | Loss: 0.49973, Accuracy: 78.64% | Test loss: 0.50023, Test acc: 78.92%\n",
      "Epoch: 760 | Loss: 0.49865, Accuracy: 78.67% | Test loss: 0.49915, Test acc: 78.93%\n",
      "Epoch: 770 | Loss: 0.49759, Accuracy: 78.70% | Test loss: 0.49809, Test acc: 78.95%\n",
      "Epoch: 780 | Loss: 0.49654, Accuracy: 78.75% | Test loss: 0.49705, Test acc: 78.94%\n",
      "Epoch: 790 | Loss: 0.49551, Accuracy: 78.79% | Test loss: 0.49603, Test acc: 79.00%\n",
      "Epoch: 800 | Loss: 0.49450, Accuracy: 78.81% | Test loss: 0.49502, Test acc: 78.98%\n",
      "Epoch: 810 | Loss: 0.49350, Accuracy: 78.81% | Test loss: 0.49403, Test acc: 78.98%\n",
      "Epoch: 820 | Loss: 0.49252, Accuracy: 78.83% | Test loss: 0.49306, Test acc: 79.00%\n",
      "Epoch: 830 | Loss: 0.49156, Accuracy: 78.87% | Test loss: 0.49210, Test acc: 79.01%\n",
      "Epoch: 840 | Loss: 0.49060, Accuracy: 78.89% | Test loss: 0.49115, Test acc: 79.05%\n",
      "Epoch: 850 | Loss: 0.48967, Accuracy: 78.90% | Test loss: 0.49022, Test acc: 79.08%\n",
      "Epoch: 860 | Loss: 0.48875, Accuracy: 78.92% | Test loss: 0.48931, Test acc: 79.10%\n",
      "Epoch: 870 | Loss: 0.48784, Accuracy: 78.92% | Test loss: 0.48840, Test acc: 79.10%\n",
      "Epoch: 880 | Loss: 0.48694, Accuracy: 78.95% | Test loss: 0.48752, Test acc: 79.15%\n",
      "Epoch: 890 | Loss: 0.48606, Accuracy: 78.98% | Test loss: 0.48664, Test acc: 79.15%\n",
      "Epoch: 900 | Loss: 0.48520, Accuracy: 79.01% | Test loss: 0.48578, Test acc: 79.18%\n",
      "Epoch: 910 | Loss: 0.48434, Accuracy: 79.05% | Test loss: 0.48493, Test acc: 79.18%\n",
      "Epoch: 920 | Loss: 0.48350, Accuracy: 79.04% | Test loss: 0.48409, Test acc: 79.18%\n",
      "Epoch: 930 | Loss: 0.48267, Accuracy: 79.05% | Test loss: 0.48327, Test acc: 79.21%\n",
      "Epoch: 940 | Loss: 0.48185, Accuracy: 79.07% | Test loss: 0.48246, Test acc: 79.28%\n",
      "Epoch: 950 | Loss: 0.48105, Accuracy: 79.11% | Test loss: 0.48166, Test acc: 79.27%\n",
      "Epoch: 960 | Loss: 0.48025, Accuracy: 79.12% | Test loss: 0.48087, Test acc: 79.30%\n",
      "Epoch: 970 | Loss: 0.47947, Accuracy: 79.13% | Test loss: 0.48009, Test acc: 79.30%\n",
      "Epoch: 980 | Loss: 0.47870, Accuracy: 79.16% | Test loss: 0.47933, Test acc: 79.30%\n",
      "Epoch: 990 | Loss: 0.47794, Accuracy: 79.18% | Test loss: 0.47857, Test acc: 79.33%\n",
      "Epoch: 1000 | Loss: 0.47719, Accuracy: 79.18% | Test loss: 0.47783, Test acc: 79.36%\n",
      "Epoch: 1010 | Loss: 0.47645, Accuracy: 79.21% | Test loss: 0.47709, Test acc: 79.34%\n",
      "Epoch: 1020 | Loss: 0.47572, Accuracy: 79.23% | Test loss: 0.47637, Test acc: 79.36%\n",
      "Epoch: 1030 | Loss: 0.47500, Accuracy: 79.26% | Test loss: 0.47566, Test acc: 79.37%\n",
      "Epoch: 1040 | Loss: 0.47429, Accuracy: 79.27% | Test loss: 0.47495, Test acc: 79.40%\n",
      "Epoch: 1050 | Loss: 0.47359, Accuracy: 79.27% | Test loss: 0.47426, Test acc: 79.41%\n",
      "Epoch: 1060 | Loss: 0.47290, Accuracy: 79.30% | Test loss: 0.47357, Test acc: 79.42%\n",
      "Epoch: 1070 | Loss: 0.47222, Accuracy: 79.31% | Test loss: 0.47290, Test acc: 79.46%\n",
      "Epoch: 1080 | Loss: 0.47155, Accuracy: 79.32% | Test loss: 0.47223, Test acc: 79.44%\n",
      "Epoch: 1090 | Loss: 0.47088, Accuracy: 79.32% | Test loss: 0.47157, Test acc: 79.44%\n",
      "Epoch: 1100 | Loss: 0.47023, Accuracy: 79.34% | Test loss: 0.47092, Test acc: 79.47%\n",
      "Epoch: 1110 | Loss: 0.46958, Accuracy: 79.36% | Test loss: 0.47028, Test acc: 79.51%\n",
      "Epoch: 1120 | Loss: 0.46895, Accuracy: 79.37% | Test loss: 0.46965, Test acc: 79.52%\n",
      "Epoch: 1130 | Loss: 0.46832, Accuracy: 79.38% | Test loss: 0.46903, Test acc: 79.54%\n",
      "Epoch: 1140 | Loss: 0.46770, Accuracy: 79.38% | Test loss: 0.46841, Test acc: 79.57%\n",
      "Epoch: 1150 | Loss: 0.46708, Accuracy: 79.39% | Test loss: 0.46781, Test acc: 79.57%\n",
      "Epoch: 1160 | Loss: 0.46648, Accuracy: 79.42% | Test loss: 0.46721, Test acc: 79.54%\n",
      "Epoch: 1170 | Loss: 0.46588, Accuracy: 79.45% | Test loss: 0.46662, Test acc: 79.60%\n",
      "Epoch: 1180 | Loss: 0.46529, Accuracy: 79.47% | Test loss: 0.46603, Test acc: 79.60%\n",
      "Epoch: 1190 | Loss: 0.46471, Accuracy: 79.49% | Test loss: 0.46546, Test acc: 79.61%\n",
      "Epoch: 1200 | Loss: 0.46413, Accuracy: 79.49% | Test loss: 0.46489, Test acc: 79.61%\n",
      "Epoch: 1210 | Loss: 0.46356, Accuracy: 79.51% | Test loss: 0.46432, Test acc: 79.62%\n",
      "Epoch: 1220 | Loss: 0.46300, Accuracy: 79.52% | Test loss: 0.46377, Test acc: 79.62%\n",
      "Epoch: 1230 | Loss: 0.46245, Accuracy: 79.54% | Test loss: 0.46322, Test acc: 79.62%\n",
      "Epoch: 1240 | Loss: 0.46190, Accuracy: 79.55% | Test loss: 0.46268, Test acc: 79.62%\n",
      "Epoch: 1250 | Loss: 0.46136, Accuracy: 79.57% | Test loss: 0.46214, Test acc: 79.62%\n",
      "Epoch: 1260 | Loss: 0.46083, Accuracy: 79.57% | Test loss: 0.46161, Test acc: 79.66%\n",
      "Epoch: 1270 | Loss: 0.46030, Accuracy: 79.59% | Test loss: 0.46109, Test acc: 79.66%\n",
      "Epoch: 1280 | Loss: 0.45978, Accuracy: 79.61% | Test loss: 0.46058, Test acc: 79.69%\n",
      "Epoch: 1290 | Loss: 0.45926, Accuracy: 79.62% | Test loss: 0.46007, Test acc: 79.72%\n",
      "Epoch: 1300 | Loss: 0.45875, Accuracy: 79.63% | Test loss: 0.45956, Test acc: 79.74%\n",
      "Epoch: 1310 | Loss: 0.45825, Accuracy: 79.64% | Test loss: 0.45906, Test acc: 79.73%\n",
      "Epoch: 1320 | Loss: 0.45775, Accuracy: 79.65% | Test loss: 0.45857, Test acc: 79.71%\n",
      "Epoch: 1330 | Loss: 0.45726, Accuracy: 79.66% | Test loss: 0.45809, Test acc: 79.71%\n",
      "Epoch: 1340 | Loss: 0.45678, Accuracy: 79.66% | Test loss: 0.45761, Test acc: 79.74%\n",
      "Epoch: 1350 | Loss: 0.45630, Accuracy: 79.67% | Test loss: 0.45713, Test acc: 79.78%\n",
      "Epoch: 1360 | Loss: 0.45582, Accuracy: 79.70% | Test loss: 0.45666, Test acc: 79.79%\n",
      "Epoch: 1370 | Loss: 0.45535, Accuracy: 79.74% | Test loss: 0.45620, Test acc: 79.78%\n",
      "Epoch: 1380 | Loss: 0.45489, Accuracy: 79.75% | Test loss: 0.45574, Test acc: 79.78%\n",
      "Epoch: 1390 | Loss: 0.45443, Accuracy: 79.77% | Test loss: 0.45529, Test acc: 79.78%\n",
      "Epoch: 1400 | Loss: 0.45398, Accuracy: 79.78% | Test loss: 0.45484, Test acc: 79.79%\n",
      "Epoch: 1410 | Loss: 0.45353, Accuracy: 79.78% | Test loss: 0.45440, Test acc: 79.82%\n",
      "Epoch: 1420 | Loss: 0.45308, Accuracy: 79.80% | Test loss: 0.45396, Test acc: 79.85%\n",
      "Epoch: 1430 | Loss: 0.45264, Accuracy: 79.81% | Test loss: 0.45353, Test acc: 79.86%\n",
      "Epoch: 1440 | Loss: 0.45221, Accuracy: 79.84% | Test loss: 0.45310, Test acc: 79.86%\n",
      "Epoch: 1450 | Loss: 0.45178, Accuracy: 79.84% | Test loss: 0.45267, Test acc: 79.83%\n",
      "Epoch: 1460 | Loss: 0.45136, Accuracy: 79.87% | Test loss: 0.45226, Test acc: 79.86%\n",
      "Epoch: 1470 | Loss: 0.45094, Accuracy: 79.88% | Test loss: 0.45184, Test acc: 79.89%\n",
      "Epoch: 1480 | Loss: 0.45052, Accuracy: 79.88% | Test loss: 0.45143, Test acc: 79.89%\n",
      "Epoch: 1490 | Loss: 0.45011, Accuracy: 79.89% | Test loss: 0.45103, Test acc: 79.92%\n",
      "Epoch: 1500 | Loss: 0.44970, Accuracy: 79.89% | Test loss: 0.45063, Test acc: 79.97%\n",
      "Epoch: 1510 | Loss: 0.44930, Accuracy: 79.89% | Test loss: 0.45023, Test acc: 79.99%\n",
      "Epoch: 1520 | Loss: 0.44890, Accuracy: 79.90% | Test loss: 0.44984, Test acc: 80.02%\n",
      "Epoch: 1530 | Loss: 0.44851, Accuracy: 79.90% | Test loss: 0.44945, Test acc: 80.01%\n",
      "Epoch: 1540 | Loss: 0.44812, Accuracy: 79.90% | Test loss: 0.44906, Test acc: 80.02%\n",
      "Epoch: 1550 | Loss: 0.44774, Accuracy: 79.91% | Test loss: 0.44869, Test acc: 80.06%\n",
      "Epoch: 1560 | Loss: 0.44735, Accuracy: 79.93% | Test loss: 0.44831, Test acc: 80.05%\n",
      "Epoch: 1570 | Loss: 0.44698, Accuracy: 79.95% | Test loss: 0.44794, Test acc: 80.06%\n",
      "Epoch: 1580 | Loss: 0.44660, Accuracy: 79.95% | Test loss: 0.44757, Test acc: 80.08%\n",
      "Epoch: 1590 | Loss: 0.44623, Accuracy: 79.96% | Test loss: 0.44720, Test acc: 80.06%\n",
      "Epoch: 1600 | Loss: 0.44587, Accuracy: 79.97% | Test loss: 0.44684, Test acc: 80.07%\n",
      "Epoch: 1610 | Loss: 0.44550, Accuracy: 79.99% | Test loss: 0.44649, Test acc: 80.10%\n",
      "Epoch: 1620 | Loss: 0.44515, Accuracy: 79.99% | Test loss: 0.44613, Test acc: 80.11%\n",
      "Epoch: 1630 | Loss: 0.44479, Accuracy: 79.99% | Test loss: 0.44579, Test acc: 80.10%\n",
      "Epoch: 1640 | Loss: 0.44444, Accuracy: 80.02% | Test loss: 0.44544, Test acc: 80.12%\n",
      "Epoch: 1650 | Loss: 0.44409, Accuracy: 80.03% | Test loss: 0.44510, Test acc: 80.11%\n",
      "Epoch: 1660 | Loss: 0.44375, Accuracy: 80.05% | Test loss: 0.44476, Test acc: 80.12%\n",
      "Epoch: 1670 | Loss: 0.44340, Accuracy: 80.06% | Test loss: 0.44442, Test acc: 80.11%\n",
      "Epoch: 1680 | Loss: 0.44307, Accuracy: 80.07% | Test loss: 0.44409, Test acc: 80.14%\n",
      "Epoch: 1690 | Loss: 0.44273, Accuracy: 80.07% | Test loss: 0.44376, Test acc: 80.14%\n",
      "Epoch: 1700 | Loss: 0.44240, Accuracy: 80.08% | Test loss: 0.44343, Test acc: 80.14%\n",
      "Epoch: 1710 | Loss: 0.44207, Accuracy: 80.09% | Test loss: 0.44311, Test acc: 80.13%\n",
      "Epoch: 1720 | Loss: 0.44175, Accuracy: 80.09% | Test loss: 0.44279, Test acc: 80.14%\n",
      "Epoch: 1730 | Loss: 0.44142, Accuracy: 80.09% | Test loss: 0.44248, Test acc: 80.12%\n",
      "Epoch: 1740 | Loss: 0.44110, Accuracy: 80.10% | Test loss: 0.44216, Test acc: 80.11%\n",
      "Epoch: 1750 | Loss: 0.44079, Accuracy: 80.13% | Test loss: 0.44185, Test acc: 80.12%\n",
      "Epoch: 1760 | Loss: 0.44048, Accuracy: 80.15% | Test loss: 0.44155, Test acc: 80.14%\n",
      "Epoch: 1770 | Loss: 0.44017, Accuracy: 80.15% | Test loss: 0.44124, Test acc: 80.12%\n",
      "Epoch: 1780 | Loss: 0.43986, Accuracy: 80.15% | Test loss: 0.44094, Test acc: 80.14%\n",
      "Epoch: 1790 | Loss: 0.43955, Accuracy: 80.15% | Test loss: 0.44064, Test acc: 80.16%\n",
      "Epoch: 1800 | Loss: 0.43925, Accuracy: 80.17% | Test loss: 0.44035, Test acc: 80.18%\n",
      "Epoch: 1810 | Loss: 0.43895, Accuracy: 80.18% | Test loss: 0.44005, Test acc: 80.24%\n",
      "Epoch: 1820 | Loss: 0.43866, Accuracy: 80.20% | Test loss: 0.43976, Test acc: 80.27%\n",
      "Epoch: 1830 | Loss: 0.43836, Accuracy: 80.21% | Test loss: 0.43948, Test acc: 80.30%\n",
      "Epoch: 1840 | Loss: 0.43807, Accuracy: 80.22% | Test loss: 0.43919, Test acc: 80.30%\n",
      "Epoch: 1850 | Loss: 0.43779, Accuracy: 80.24% | Test loss: 0.43891, Test acc: 80.31%\n",
      "Epoch: 1860 | Loss: 0.43750, Accuracy: 80.25% | Test loss: 0.43863, Test acc: 80.30%\n",
      "Epoch: 1870 | Loss: 0.43722, Accuracy: 80.27% | Test loss: 0.43835, Test acc: 80.30%\n",
      "Epoch: 1880 | Loss: 0.43694, Accuracy: 80.28% | Test loss: 0.43808, Test acc: 80.31%\n",
      "Epoch: 1890 | Loss: 0.43666, Accuracy: 80.29% | Test loss: 0.43781, Test acc: 80.33%\n",
      "Epoch: 1900 | Loss: 0.43638, Accuracy: 80.29% | Test loss: 0.43754, Test acc: 80.32%\n",
      "Epoch: 1910 | Loss: 0.43611, Accuracy: 80.31% | Test loss: 0.43727, Test acc: 80.30%\n",
      "Epoch: 1920 | Loss: 0.43584, Accuracy: 80.33% | Test loss: 0.43701, Test acc: 80.31%\n",
      "Epoch: 1930 | Loss: 0.43557, Accuracy: 80.33% | Test loss: 0.43675, Test acc: 80.31%\n",
      "Epoch: 1940 | Loss: 0.43531, Accuracy: 80.34% | Test loss: 0.43649, Test acc: 80.30%\n",
      "Epoch: 1950 | Loss: 0.43504, Accuracy: 80.35% | Test loss: 0.43623, Test acc: 80.30%\n",
      "Epoch: 1960 | Loss: 0.43478, Accuracy: 80.37% | Test loss: 0.43597, Test acc: 80.30%\n",
      "Epoch: 1970 | Loss: 0.43452, Accuracy: 80.37% | Test loss: 0.43572, Test acc: 80.28%\n",
      "Epoch: 1980 | Loss: 0.43426, Accuracy: 80.37% | Test loss: 0.43547, Test acc: 80.28%\n",
      "Epoch: 1990 | Loss: 0.43401, Accuracy: 80.37% | Test loss: 0.43522, Test acc: 80.30%\n",
      "Epoch: 2000 | Loss: 0.43376, Accuracy: 80.38% | Test loss: 0.43498, Test acc: 80.30%\n",
      "Epoch: 2010 | Loss: 0.43351, Accuracy: 80.41% | Test loss: 0.43473, Test acc: 80.29%\n",
      "Epoch: 2020 | Loss: 0.43326, Accuracy: 80.43% | Test loss: 0.43449, Test acc: 80.30%\n",
      "Epoch: 2030 | Loss: 0.43301, Accuracy: 80.44% | Test loss: 0.43425, Test acc: 80.31%\n",
      "Epoch: 2040 | Loss: 0.43277, Accuracy: 80.45% | Test loss: 0.43401, Test acc: 80.32%\n",
      "Epoch: 2050 | Loss: 0.43253, Accuracy: 80.45% | Test loss: 0.43378, Test acc: 80.34%\n",
      "Epoch: 2060 | Loss: 0.43228, Accuracy: 80.45% | Test loss: 0.43354, Test acc: 80.34%\n",
      "Epoch: 2070 | Loss: 0.43205, Accuracy: 80.45% | Test loss: 0.43331, Test acc: 80.35%\n",
      "Epoch: 2080 | Loss: 0.43181, Accuracy: 80.45% | Test loss: 0.43308, Test acc: 80.36%\n",
      "Epoch: 2090 | Loss: 0.43158, Accuracy: 80.45% | Test loss: 0.43285, Test acc: 80.35%\n",
      "Epoch: 2100 | Loss: 0.43134, Accuracy: 80.45% | Test loss: 0.43263, Test acc: 80.35%\n",
      "Epoch: 2110 | Loss: 0.43111, Accuracy: 80.46% | Test loss: 0.43240, Test acc: 80.35%\n",
      "Epoch: 2120 | Loss: 0.43088, Accuracy: 80.46% | Test loss: 0.43218, Test acc: 80.34%\n",
      "Epoch: 2130 | Loss: 0.43066, Accuracy: 80.46% | Test loss: 0.43196, Test acc: 80.35%\n",
      "Epoch: 2140 | Loss: 0.43043, Accuracy: 80.47% | Test loss: 0.43174, Test acc: 80.34%\n",
      "Epoch: 2150 | Loss: 0.43021, Accuracy: 80.48% | Test loss: 0.43153, Test acc: 80.33%\n",
      "Epoch: 2160 | Loss: 0.42999, Accuracy: 80.47% | Test loss: 0.43131, Test acc: 80.34%\n",
      "Epoch: 2170 | Loss: 0.42977, Accuracy: 80.48% | Test loss: 0.43110, Test acc: 80.37%\n",
      "Epoch: 2180 | Loss: 0.42955, Accuracy: 80.47% | Test loss: 0.43089, Test acc: 80.38%\n",
      "Epoch: 2190 | Loss: 0.42933, Accuracy: 80.48% | Test loss: 0.43068, Test acc: 80.39%\n",
      "Epoch: 2200 | Loss: 0.42912, Accuracy: 80.49% | Test loss: 0.43047, Test acc: 80.39%\n",
      "Epoch: 2210 | Loss: 0.42890, Accuracy: 80.50% | Test loss: 0.43027, Test acc: 80.39%\n",
      "Epoch: 2220 | Loss: 0.42869, Accuracy: 80.52% | Test loss: 0.43006, Test acc: 80.42%\n",
      "Epoch: 2230 | Loss: 0.42848, Accuracy: 80.52% | Test loss: 0.42986, Test acc: 80.42%\n",
      "Epoch: 2240 | Loss: 0.42827, Accuracy: 80.52% | Test loss: 0.42966, Test acc: 80.45%\n",
      "Epoch: 2250 | Loss: 0.42807, Accuracy: 80.52% | Test loss: 0.42946, Test acc: 80.45%\n",
      "Epoch: 2260 | Loss: 0.42786, Accuracy: 80.54% | Test loss: 0.42926, Test acc: 80.47%\n",
      "Epoch: 2270 | Loss: 0.42766, Accuracy: 80.55% | Test loss: 0.42906, Test acc: 80.44%\n",
      "Epoch: 2280 | Loss: 0.42746, Accuracy: 80.55% | Test loss: 0.42887, Test acc: 80.44%\n",
      "Epoch: 2290 | Loss: 0.42726, Accuracy: 80.57% | Test loss: 0.42868, Test acc: 80.44%\n",
      "Epoch: 2300 | Loss: 0.42706, Accuracy: 80.58% | Test loss: 0.42849, Test acc: 80.43%\n",
      "Epoch: 2310 | Loss: 0.42686, Accuracy: 80.59% | Test loss: 0.42830, Test acc: 80.44%\n",
      "Epoch: 2320 | Loss: 0.42666, Accuracy: 80.61% | Test loss: 0.42811, Test acc: 80.46%\n",
      "Epoch: 2330 | Loss: 0.42647, Accuracy: 80.63% | Test loss: 0.42792, Test acc: 80.47%\n",
      "Epoch: 2340 | Loss: 0.42627, Accuracy: 80.64% | Test loss: 0.42773, Test acc: 80.46%\n",
      "Epoch: 2350 | Loss: 0.42608, Accuracy: 80.65% | Test loss: 0.42755, Test acc: 80.46%\n",
      "Epoch: 2360 | Loss: 0.42589, Accuracy: 80.66% | Test loss: 0.42737, Test acc: 80.48%\n",
      "Epoch: 2370 | Loss: 0.42570, Accuracy: 80.67% | Test loss: 0.42719, Test acc: 80.50%\n",
      "Epoch: 2380 | Loss: 0.42551, Accuracy: 80.67% | Test loss: 0.42701, Test acc: 80.50%\n",
      "Epoch: 2390 | Loss: 0.42533, Accuracy: 80.67% | Test loss: 0.42683, Test acc: 80.51%\n",
      "Epoch: 2400 | Loss: 0.42514, Accuracy: 80.69% | Test loss: 0.42665, Test acc: 80.50%\n",
      "Epoch: 2410 | Loss: 0.42496, Accuracy: 80.70% | Test loss: 0.42647, Test acc: 80.50%\n",
      "Epoch: 2420 | Loss: 0.42478, Accuracy: 80.72% | Test loss: 0.42630, Test acc: 80.52%\n",
      "Epoch: 2430 | Loss: 0.42460, Accuracy: 80.71% | Test loss: 0.42613, Test acc: 80.55%\n",
      "Epoch: 2440 | Loss: 0.42442, Accuracy: 80.73% | Test loss: 0.42595, Test acc: 80.57%\n",
      "Epoch: 2450 | Loss: 0.42424, Accuracy: 80.73% | Test loss: 0.42578, Test acc: 80.59%\n",
      "Epoch: 2460 | Loss: 0.42406, Accuracy: 80.73% | Test loss: 0.42561, Test acc: 80.58%\n",
      "Epoch: 2470 | Loss: 0.42388, Accuracy: 80.74% | Test loss: 0.42545, Test acc: 80.59%\n",
      "Epoch: 2480 | Loss: 0.42371, Accuracy: 80.74% | Test loss: 0.42528, Test acc: 80.61%\n",
      "Epoch: 2490 | Loss: 0.42353, Accuracy: 80.73% | Test loss: 0.42511, Test acc: 80.62%\n",
      "Epoch: 2500 | Loss: 0.42336, Accuracy: 80.75% | Test loss: 0.42495, Test acc: 80.62%\n",
      "Epoch: 2510 | Loss: 0.42319, Accuracy: 80.76% | Test loss: 0.42479, Test acc: 80.61%\n",
      "Epoch: 2520 | Loss: 0.42302, Accuracy: 80.77% | Test loss: 0.42463, Test acc: 80.60%\n",
      "Epoch: 2530 | Loss: 0.42285, Accuracy: 80.77% | Test loss: 0.42447, Test acc: 80.62%\n",
      "Epoch: 2540 | Loss: 0.42268, Accuracy: 80.77% | Test loss: 0.42431, Test acc: 80.64%\n",
      "Epoch: 2550 | Loss: 0.42252, Accuracy: 80.78% | Test loss: 0.42415, Test acc: 80.66%\n",
      "Epoch: 2560 | Loss: 0.42235, Accuracy: 80.78% | Test loss: 0.42399, Test acc: 80.66%\n",
      "Epoch: 2570 | Loss: 0.42219, Accuracy: 80.79% | Test loss: 0.42383, Test acc: 80.66%\n",
      "Epoch: 2580 | Loss: 0.42202, Accuracy: 80.81% | Test loss: 0.42368, Test acc: 80.66%\n",
      "Epoch: 2590 | Loss: 0.42186, Accuracy: 80.81% | Test loss: 0.42353, Test acc: 80.69%\n",
      "Epoch: 2600 | Loss: 0.42170, Accuracy: 80.82% | Test loss: 0.42337, Test acc: 80.70%\n",
      "Epoch: 2610 | Loss: 0.42154, Accuracy: 80.82% | Test loss: 0.42322, Test acc: 80.71%\n",
      "Epoch: 2620 | Loss: 0.42138, Accuracy: 80.81% | Test loss: 0.42307, Test acc: 80.74%\n",
      "Epoch: 2630 | Loss: 0.42122, Accuracy: 80.83% | Test loss: 0.42292, Test acc: 80.75%\n",
      "Epoch: 2640 | Loss: 0.42106, Accuracy: 80.84% | Test loss: 0.42277, Test acc: 80.75%\n",
      "Epoch: 2650 | Loss: 0.42091, Accuracy: 80.84% | Test loss: 0.42263, Test acc: 80.75%\n",
      "Epoch: 2660 | Loss: 0.42075, Accuracy: 80.84% | Test loss: 0.42248, Test acc: 80.76%\n",
      "Epoch: 2670 | Loss: 0.42060, Accuracy: 80.85% | Test loss: 0.42234, Test acc: 80.76%\n",
      "Epoch: 2680 | Loss: 0.42044, Accuracy: 80.86% | Test loss: 0.42219, Test acc: 80.75%\n",
      "Epoch: 2690 | Loss: 0.42029, Accuracy: 80.86% | Test loss: 0.42205, Test acc: 80.75%\n",
      "Epoch: 2700 | Loss: 0.42014, Accuracy: 80.88% | Test loss: 0.42191, Test acc: 80.74%\n",
      "Epoch: 2710 | Loss: 0.41999, Accuracy: 80.88% | Test loss: 0.42177, Test acc: 80.75%\n",
      "Epoch: 2720 | Loss: 0.41984, Accuracy: 80.89% | Test loss: 0.42163, Test acc: 80.76%\n",
      "Epoch: 2730 | Loss: 0.41969, Accuracy: 80.90% | Test loss: 0.42149, Test acc: 80.77%\n",
      "Epoch: 2740 | Loss: 0.41955, Accuracy: 80.90% | Test loss: 0.42135, Test acc: 80.77%\n",
      "Epoch: 2750 | Loss: 0.41940, Accuracy: 80.91% | Test loss: 0.42121, Test acc: 80.78%\n",
      "Epoch: 2760 | Loss: 0.41925, Accuracy: 80.91% | Test loss: 0.42108, Test acc: 80.79%\n",
      "Epoch: 2770 | Loss: 0.41911, Accuracy: 80.94% | Test loss: 0.42094, Test acc: 80.80%\n",
      "Epoch: 2780 | Loss: 0.41897, Accuracy: 80.95% | Test loss: 0.42081, Test acc: 80.81%\n",
      "Epoch: 2790 | Loss: 0.41882, Accuracy: 80.94% | Test loss: 0.42067, Test acc: 80.81%\n",
      "Epoch: 2800 | Loss: 0.41868, Accuracy: 80.95% | Test loss: 0.42054, Test acc: 80.80%\n",
      "Epoch: 2810 | Loss: 0.41854, Accuracy: 80.96% | Test loss: 0.42041, Test acc: 80.79%\n",
      "Epoch: 2820 | Loss: 0.41840, Accuracy: 80.95% | Test loss: 0.42028, Test acc: 80.79%\n",
      "Epoch: 2830 | Loss: 0.41826, Accuracy: 80.96% | Test loss: 0.42015, Test acc: 80.79%\n",
      "Epoch: 2840 | Loss: 0.41812, Accuracy: 80.96% | Test loss: 0.42002, Test acc: 80.81%\n",
      "Epoch: 2850 | Loss: 0.41798, Accuracy: 80.97% | Test loss: 0.41989, Test acc: 80.79%\n",
      "Epoch: 2860 | Loss: 0.41785, Accuracy: 80.97% | Test loss: 0.41977, Test acc: 80.79%\n",
      "Epoch: 2870 | Loss: 0.41771, Accuracy: 80.98% | Test loss: 0.41964, Test acc: 80.78%\n",
      "Epoch: 2880 | Loss: 0.41757, Accuracy: 80.98% | Test loss: 0.41951, Test acc: 80.78%\n",
      "Epoch: 2890 | Loss: 0.41744, Accuracy: 80.99% | Test loss: 0.41939, Test acc: 80.81%\n",
      "Epoch: 2900 | Loss: 0.41731, Accuracy: 80.99% | Test loss: 0.41927, Test acc: 80.82%\n",
      "Epoch: 2910 | Loss: 0.41717, Accuracy: 80.99% | Test loss: 0.41914, Test acc: 80.82%\n",
      "Epoch: 2920 | Loss: 0.41704, Accuracy: 81.01% | Test loss: 0.41902, Test acc: 80.82%\n",
      "Epoch: 2930 | Loss: 0.41691, Accuracy: 81.02% | Test loss: 0.41890, Test acc: 80.82%\n",
      "Epoch: 2940 | Loss: 0.41678, Accuracy: 81.02% | Test loss: 0.41878, Test acc: 80.81%\n",
      "Epoch: 2950 | Loss: 0.41665, Accuracy: 81.02% | Test loss: 0.41866, Test acc: 80.80%\n",
      "Epoch: 2960 | Loss: 0.41652, Accuracy: 81.02% | Test loss: 0.41854, Test acc: 80.81%\n",
      "Epoch: 2970 | Loss: 0.41639, Accuracy: 81.02% | Test loss: 0.41842, Test acc: 80.81%\n",
      "Epoch: 2980 | Loss: 0.41626, Accuracy: 81.03% | Test loss: 0.41831, Test acc: 80.82%\n",
      "Epoch: 2990 | Loss: 0.41614, Accuracy: 81.03% | Test loss: 0.41819, Test acc: 80.83%\n",
      "Epoch: 3000 | Loss: 0.41601, Accuracy: 81.06% | Test loss: 0.41808, Test acc: 80.83%\n",
      "Epoch: 3010 | Loss: 0.41588, Accuracy: 81.07% | Test loss: 0.41796, Test acc: 80.82%\n",
      "Epoch: 3020 | Loss: 0.41576, Accuracy: 81.10% | Test loss: 0.41785, Test acc: 80.82%\n",
      "Epoch: 3030 | Loss: 0.41563, Accuracy: 81.10% | Test loss: 0.41773, Test acc: 80.82%\n",
      "Epoch: 3040 | Loss: 0.41551, Accuracy: 81.10% | Test loss: 0.41762, Test acc: 80.82%\n",
      "Epoch: 3050 | Loss: 0.41539, Accuracy: 81.12% | Test loss: 0.41751, Test acc: 80.83%\n",
      "Epoch: 3060 | Loss: 0.41527, Accuracy: 81.13% | Test loss: 0.41740, Test acc: 80.85%\n",
      "Epoch: 3070 | Loss: 0.41515, Accuracy: 81.12% | Test loss: 0.41729, Test acc: 80.84%\n",
      "Epoch: 3080 | Loss: 0.41502, Accuracy: 81.14% | Test loss: 0.41718, Test acc: 80.84%\n",
      "Epoch: 3090 | Loss: 0.41490, Accuracy: 81.14% | Test loss: 0.41707, Test acc: 80.85%\n",
      "Epoch: 3100 | Loss: 0.41479, Accuracy: 81.15% | Test loss: 0.41696, Test acc: 80.84%\n",
      "Epoch: 3110 | Loss: 0.41467, Accuracy: 81.17% | Test loss: 0.41685, Test acc: 80.82%\n",
      "Epoch: 3120 | Loss: 0.41455, Accuracy: 81.17% | Test loss: 0.41675, Test acc: 80.83%\n",
      "Epoch: 3130 | Loss: 0.41443, Accuracy: 81.17% | Test loss: 0.41664, Test acc: 80.82%\n",
      "Epoch: 3140 | Loss: 0.41431, Accuracy: 81.17% | Test loss: 0.41653, Test acc: 80.85%\n",
      "Epoch: 3150 | Loss: 0.41420, Accuracy: 81.18% | Test loss: 0.41643, Test acc: 80.86%\n",
      "Epoch: 3160 | Loss: 0.41408, Accuracy: 81.19% | Test loss: 0.41632, Test acc: 80.87%\n",
      "Epoch: 3170 | Loss: 0.41397, Accuracy: 81.21% | Test loss: 0.41622, Test acc: 80.86%\n",
      "Epoch: 3180 | Loss: 0.41385, Accuracy: 81.21% | Test loss: 0.41612, Test acc: 80.90%\n",
      "Epoch: 3190 | Loss: 0.41374, Accuracy: 81.21% | Test loss: 0.41602, Test acc: 80.89%\n",
      "Epoch: 3200 | Loss: 0.41363, Accuracy: 81.20% | Test loss: 0.41591, Test acc: 80.88%\n",
      "Epoch: 3210 | Loss: 0.41351, Accuracy: 81.21% | Test loss: 0.41581, Test acc: 80.88%\n",
      "Epoch: 3220 | Loss: 0.41340, Accuracy: 81.22% | Test loss: 0.41571, Test acc: 80.87%\n",
      "Epoch: 3230 | Loss: 0.41329, Accuracy: 81.22% | Test loss: 0.41561, Test acc: 80.87%\n",
      "Epoch: 3240 | Loss: 0.41318, Accuracy: 81.23% | Test loss: 0.41551, Test acc: 80.86%\n",
      "Epoch: 3250 | Loss: 0.41307, Accuracy: 81.24% | Test loss: 0.41542, Test acc: 80.87%\n",
      "Epoch: 3260 | Loss: 0.41296, Accuracy: 81.26% | Test loss: 0.41532, Test acc: 80.88%\n",
      "Epoch: 3270 | Loss: 0.41285, Accuracy: 81.27% | Test loss: 0.41522, Test acc: 80.88%\n",
      "Epoch: 3280 | Loss: 0.41274, Accuracy: 81.27% | Test loss: 0.41512, Test acc: 80.89%\n",
      "Epoch: 3290 | Loss: 0.41264, Accuracy: 81.27% | Test loss: 0.41503, Test acc: 80.90%\n",
      "Epoch: 3300 | Loss: 0.41253, Accuracy: 81.27% | Test loss: 0.41493, Test acc: 80.91%\n",
      "Epoch: 3310 | Loss: 0.41242, Accuracy: 81.28% | Test loss: 0.41484, Test acc: 80.91%\n",
      "Epoch: 3320 | Loss: 0.41232, Accuracy: 81.28% | Test loss: 0.41474, Test acc: 80.90%\n",
      "Epoch: 3330 | Loss: 0.41221, Accuracy: 81.28% | Test loss: 0.41465, Test acc: 80.90%\n",
      "Epoch: 3340 | Loss: 0.41210, Accuracy: 81.29% | Test loss: 0.41456, Test acc: 80.90%\n",
      "Epoch: 3350 | Loss: 0.41200, Accuracy: 81.30% | Test loss: 0.41446, Test acc: 80.90%\n",
      "Epoch: 3360 | Loss: 0.41190, Accuracy: 81.31% | Test loss: 0.41437, Test acc: 80.92%\n",
      "Epoch: 3370 | Loss: 0.41179, Accuracy: 81.31% | Test loss: 0.41428, Test acc: 80.93%\n",
      "Epoch: 3380 | Loss: 0.41169, Accuracy: 81.32% | Test loss: 0.41419, Test acc: 80.94%\n",
      "Epoch: 3390 | Loss: 0.41159, Accuracy: 81.34% | Test loss: 0.41410, Test acc: 80.94%\n",
      "Epoch: 3400 | Loss: 0.41149, Accuracy: 81.33% | Test loss: 0.41401, Test acc: 80.94%\n",
      "Epoch: 3410 | Loss: 0.41138, Accuracy: 81.33% | Test loss: 0.41392, Test acc: 80.95%\n",
      "Epoch: 3420 | Loss: 0.41128, Accuracy: 81.34% | Test loss: 0.41383, Test acc: 80.94%\n",
      "Epoch: 3430 | Loss: 0.41118, Accuracy: 81.35% | Test loss: 0.41374, Test acc: 80.96%\n",
      "Epoch: 3440 | Loss: 0.41108, Accuracy: 81.37% | Test loss: 0.41366, Test acc: 80.97%\n",
      "Epoch: 3450 | Loss: 0.41098, Accuracy: 81.37% | Test loss: 0.41357, Test acc: 80.97%\n",
      "Epoch: 3460 | Loss: 0.41088, Accuracy: 81.37% | Test loss: 0.41348, Test acc: 80.99%\n",
      "Epoch: 3470 | Loss: 0.41079, Accuracy: 81.37% | Test loss: 0.41340, Test acc: 81.01%\n",
      "Epoch: 3480 | Loss: 0.41069, Accuracy: 81.37% | Test loss: 0.41331, Test acc: 80.99%\n",
      "Epoch: 3490 | Loss: 0.41059, Accuracy: 81.38% | Test loss: 0.41322, Test acc: 80.99%\n",
      "Epoch: 3500 | Loss: 0.41049, Accuracy: 81.38% | Test loss: 0.41314, Test acc: 81.00%\n",
      "Epoch: 3510 | Loss: 0.41040, Accuracy: 81.39% | Test loss: 0.41306, Test acc: 81.01%\n",
      "Epoch: 3520 | Loss: 0.41030, Accuracy: 81.39% | Test loss: 0.41297, Test acc: 81.04%\n",
      "Epoch: 3530 | Loss: 0.41021, Accuracy: 81.41% | Test loss: 0.41289, Test acc: 81.04%\n",
      "Epoch: 3540 | Loss: 0.41011, Accuracy: 81.41% | Test loss: 0.41281, Test acc: 81.03%\n",
      "Epoch: 3550 | Loss: 0.41002, Accuracy: 81.41% | Test loss: 0.41272, Test acc: 81.02%\n",
      "Epoch: 3560 | Loss: 0.40992, Accuracy: 81.42% | Test loss: 0.41264, Test acc: 81.02%\n",
      "Epoch: 3570 | Loss: 0.40983, Accuracy: 81.43% | Test loss: 0.41256, Test acc: 81.06%\n",
      "Epoch: 3580 | Loss: 0.40973, Accuracy: 81.43% | Test loss: 0.41248, Test acc: 81.07%\n",
      "Epoch: 3590 | Loss: 0.40964, Accuracy: 81.43% | Test loss: 0.41240, Test acc: 81.10%\n",
      "Epoch: 3600 | Loss: 0.40955, Accuracy: 81.42% | Test loss: 0.41232, Test acc: 81.12%\n",
      "Epoch: 3610 | Loss: 0.40946, Accuracy: 81.44% | Test loss: 0.41224, Test acc: 81.14%\n",
      "Epoch: 3620 | Loss: 0.40936, Accuracy: 81.45% | Test loss: 0.41216, Test acc: 81.15%\n",
      "Epoch: 3630 | Loss: 0.40927, Accuracy: 81.45% | Test loss: 0.41208, Test acc: 81.16%\n",
      "Epoch: 3640 | Loss: 0.40918, Accuracy: 81.45% | Test loss: 0.41201, Test acc: 81.16%\n",
      "Epoch: 3650 | Loss: 0.40909, Accuracy: 81.46% | Test loss: 0.41193, Test acc: 81.18%\n",
      "Epoch: 3660 | Loss: 0.40900, Accuracy: 81.46% | Test loss: 0.41185, Test acc: 81.18%\n",
      "Epoch: 3670 | Loss: 0.40891, Accuracy: 81.46% | Test loss: 0.41178, Test acc: 81.18%\n",
      "Epoch: 3680 | Loss: 0.40882, Accuracy: 81.46% | Test loss: 0.41170, Test acc: 81.22%\n",
      "Epoch: 3690 | Loss: 0.40874, Accuracy: 81.47% | Test loss: 0.41162, Test acc: 81.21%\n",
      "Epoch: 3700 | Loss: 0.40865, Accuracy: 81.48% | Test loss: 0.41155, Test acc: 81.22%\n",
      "Epoch: 3710 | Loss: 0.40856, Accuracy: 81.47% | Test loss: 0.41147, Test acc: 81.23%\n",
      "Epoch: 3720 | Loss: 0.40847, Accuracy: 81.48% | Test loss: 0.41140, Test acc: 81.22%\n",
      "Epoch: 3730 | Loss: 0.40839, Accuracy: 81.49% | Test loss: 0.41132, Test acc: 81.23%\n",
      "Epoch: 3740 | Loss: 0.40830, Accuracy: 81.49% | Test loss: 0.41125, Test acc: 81.20%\n",
      "Epoch: 3750 | Loss: 0.40821, Accuracy: 81.49% | Test loss: 0.41118, Test acc: 81.20%\n",
      "Epoch: 3760 | Loss: 0.40813, Accuracy: 81.48% | Test loss: 0.41111, Test acc: 81.19%\n",
      "Epoch: 3770 | Loss: 0.40804, Accuracy: 81.49% | Test loss: 0.41103, Test acc: 81.20%\n",
      "Epoch: 3780 | Loss: 0.40796, Accuracy: 81.50% | Test loss: 0.41096, Test acc: 81.18%\n",
      "Epoch: 3790 | Loss: 0.40787, Accuracy: 81.51% | Test loss: 0.41089, Test acc: 81.16%\n",
      "Epoch: 3800 | Loss: 0.40779, Accuracy: 81.51% | Test loss: 0.41082, Test acc: 81.15%\n",
      "Epoch: 3810 | Loss: 0.40770, Accuracy: 81.51% | Test loss: 0.41075, Test acc: 81.15%\n",
      "Epoch: 3820 | Loss: 0.40762, Accuracy: 81.51% | Test loss: 0.41068, Test acc: 81.15%\n",
      "Epoch: 3830 | Loss: 0.40754, Accuracy: 81.51% | Test loss: 0.41061, Test acc: 81.14%\n",
      "Epoch: 3840 | Loss: 0.40745, Accuracy: 81.51% | Test loss: 0.41054, Test acc: 81.13%\n",
      "Epoch: 3850 | Loss: 0.40737, Accuracy: 81.52% | Test loss: 0.41047, Test acc: 81.14%\n",
      "Epoch: 3860 | Loss: 0.40729, Accuracy: 81.53% | Test loss: 0.41040, Test acc: 81.14%\n",
      "Epoch: 3870 | Loss: 0.40721, Accuracy: 81.53% | Test loss: 0.41033, Test acc: 81.13%\n",
      "Epoch: 3880 | Loss: 0.40713, Accuracy: 81.53% | Test loss: 0.41026, Test acc: 81.13%\n",
      "Epoch: 3890 | Loss: 0.40705, Accuracy: 81.54% | Test loss: 0.41019, Test acc: 81.13%\n",
      "Epoch: 3900 | Loss: 0.40696, Accuracy: 81.53% | Test loss: 0.41013, Test acc: 81.13%\n",
      "Epoch: 3910 | Loss: 0.40688, Accuracy: 81.54% | Test loss: 0.41006, Test acc: 81.14%\n",
      "Epoch: 3920 | Loss: 0.40680, Accuracy: 81.54% | Test loss: 0.40999, Test acc: 81.14%\n",
      "Epoch: 3930 | Loss: 0.40672, Accuracy: 81.55% | Test loss: 0.40993, Test acc: 81.16%\n",
      "Epoch: 3940 | Loss: 0.40665, Accuracy: 81.56% | Test loss: 0.40986, Test acc: 81.16%\n",
      "Epoch: 3950 | Loss: 0.40657, Accuracy: 81.55% | Test loss: 0.40980, Test acc: 81.17%\n",
      "Epoch: 3960 | Loss: 0.40649, Accuracy: 81.56% | Test loss: 0.40973, Test acc: 81.18%\n",
      "Epoch: 3970 | Loss: 0.40641, Accuracy: 81.57% | Test loss: 0.40967, Test acc: 81.18%\n",
      "Epoch: 3980 | Loss: 0.40633, Accuracy: 81.58% | Test loss: 0.40960, Test acc: 81.18%\n",
      "Epoch: 3990 | Loss: 0.40625, Accuracy: 81.59% | Test loss: 0.40954, Test acc: 81.20%\n",
      "Epoch: 4000 | Loss: 0.40618, Accuracy: 81.59% | Test loss: 0.40947, Test acc: 81.21%\n",
      "Epoch: 4010 | Loss: 0.40610, Accuracy: 81.59% | Test loss: 0.40941, Test acc: 81.20%\n",
      "Epoch: 4020 | Loss: 0.40602, Accuracy: 81.59% | Test loss: 0.40935, Test acc: 81.19%\n",
      "Epoch: 4030 | Loss: 0.40595, Accuracy: 81.59% | Test loss: 0.40928, Test acc: 81.21%\n",
      "Epoch: 4040 | Loss: 0.40587, Accuracy: 81.61% | Test loss: 0.40922, Test acc: 81.21%\n",
      "Epoch: 4050 | Loss: 0.40580, Accuracy: 81.60% | Test loss: 0.40916, Test acc: 81.19%\n",
      "Epoch: 4060 | Loss: 0.40572, Accuracy: 81.61% | Test loss: 0.40910, Test acc: 81.20%\n",
      "Epoch: 4070 | Loss: 0.40565, Accuracy: 81.62% | Test loss: 0.40904, Test acc: 81.20%\n",
      "Epoch: 4080 | Loss: 0.40557, Accuracy: 81.62% | Test loss: 0.40897, Test acc: 81.20%\n",
      "Epoch: 4090 | Loss: 0.40550, Accuracy: 81.62% | Test loss: 0.40891, Test acc: 81.22%\n",
      "Epoch: 4100 | Loss: 0.40542, Accuracy: 81.63% | Test loss: 0.40885, Test acc: 81.23%\n",
      "Epoch: 4110 | Loss: 0.40535, Accuracy: 81.64% | Test loss: 0.40879, Test acc: 81.26%\n",
      "Epoch: 4120 | Loss: 0.40528, Accuracy: 81.65% | Test loss: 0.40873, Test acc: 81.27%\n",
      "Epoch: 4130 | Loss: 0.40520, Accuracy: 81.66% | Test loss: 0.40867, Test acc: 81.28%\n",
      "Epoch: 4140 | Loss: 0.40513, Accuracy: 81.65% | Test loss: 0.40861, Test acc: 81.29%\n",
      "Epoch: 4150 | Loss: 0.40506, Accuracy: 81.65% | Test loss: 0.40855, Test acc: 81.29%\n",
      "Epoch: 4160 | Loss: 0.40499, Accuracy: 81.67% | Test loss: 0.40850, Test acc: 81.31%\n",
      "Epoch: 4170 | Loss: 0.40491, Accuracy: 81.68% | Test loss: 0.40844, Test acc: 81.30%\n",
      "Epoch: 4180 | Loss: 0.40484, Accuracy: 81.68% | Test loss: 0.40838, Test acc: 81.30%\n",
      "Epoch: 4190 | Loss: 0.40477, Accuracy: 81.70% | Test loss: 0.40832, Test acc: 81.30%\n",
      "Epoch: 4200 | Loss: 0.40470, Accuracy: 81.70% | Test loss: 0.40826, Test acc: 81.29%\n",
      "Epoch: 4210 | Loss: 0.40463, Accuracy: 81.71% | Test loss: 0.40821, Test acc: 81.29%\n",
      "Epoch: 4220 | Loss: 0.40456, Accuracy: 81.71% | Test loss: 0.40815, Test acc: 81.30%\n",
      "Epoch: 4230 | Loss: 0.40449, Accuracy: 81.71% | Test loss: 0.40809, Test acc: 81.30%\n",
      "Epoch: 4240 | Loss: 0.40442, Accuracy: 81.70% | Test loss: 0.40804, Test acc: 81.29%\n",
      "Epoch: 4250 | Loss: 0.40435, Accuracy: 81.69% | Test loss: 0.40798, Test acc: 81.29%\n",
      "Epoch: 4260 | Loss: 0.40428, Accuracy: 81.69% | Test loss: 0.40793, Test acc: 81.30%\n",
      "Epoch: 4270 | Loss: 0.40421, Accuracy: 81.70% | Test loss: 0.40787, Test acc: 81.30%\n",
      "Epoch: 4280 | Loss: 0.40414, Accuracy: 81.71% | Test loss: 0.40781, Test acc: 81.29%\n",
      "Epoch: 4290 | Loss: 0.40408, Accuracy: 81.72% | Test loss: 0.40776, Test acc: 81.28%\n",
      "Epoch: 4300 | Loss: 0.40401, Accuracy: 81.74% | Test loss: 0.40771, Test acc: 81.28%\n",
      "Epoch: 4310 | Loss: 0.40394, Accuracy: 81.73% | Test loss: 0.40765, Test acc: 81.28%\n",
      "Epoch: 4320 | Loss: 0.40387, Accuracy: 81.73% | Test loss: 0.40760, Test acc: 81.29%\n",
      "Epoch: 4330 | Loss: 0.40381, Accuracy: 81.73% | Test loss: 0.40754, Test acc: 81.28%\n",
      "Epoch: 4340 | Loss: 0.40374, Accuracy: 81.72% | Test loss: 0.40749, Test acc: 81.26%\n",
      "Epoch: 4350 | Loss: 0.40367, Accuracy: 81.72% | Test loss: 0.40744, Test acc: 81.27%\n",
      "Epoch: 4360 | Loss: 0.40361, Accuracy: 81.72% | Test loss: 0.40738, Test acc: 81.29%\n",
      "Epoch: 4370 | Loss: 0.40354, Accuracy: 81.72% | Test loss: 0.40733, Test acc: 81.29%\n",
      "Epoch: 4380 | Loss: 0.40347, Accuracy: 81.73% | Test loss: 0.40728, Test acc: 81.28%\n",
      "Epoch: 4390 | Loss: 0.40341, Accuracy: 81.73% | Test loss: 0.40723, Test acc: 81.27%\n",
      "Epoch: 4400 | Loss: 0.40334, Accuracy: 81.72% | Test loss: 0.40717, Test acc: 81.27%\n",
      "Epoch: 4410 | Loss: 0.40328, Accuracy: 81.73% | Test loss: 0.40712, Test acc: 81.26%\n",
      "Epoch: 4420 | Loss: 0.40321, Accuracy: 81.76% | Test loss: 0.40707, Test acc: 81.27%\n",
      "Epoch: 4430 | Loss: 0.40315, Accuracy: 81.77% | Test loss: 0.40702, Test acc: 81.28%\n",
      "Epoch: 4440 | Loss: 0.40309, Accuracy: 81.78% | Test loss: 0.40697, Test acc: 81.28%\n",
      "Epoch: 4450 | Loss: 0.40302, Accuracy: 81.79% | Test loss: 0.40692, Test acc: 81.28%\n",
      "Epoch: 4460 | Loss: 0.40296, Accuracy: 81.79% | Test loss: 0.40687, Test acc: 81.29%\n",
      "Epoch: 4470 | Loss: 0.40289, Accuracy: 81.78% | Test loss: 0.40682, Test acc: 81.29%\n",
      "Epoch: 4480 | Loss: 0.40283, Accuracy: 81.78% | Test loss: 0.40677, Test acc: 81.30%\n",
      "Epoch: 4490 | Loss: 0.40277, Accuracy: 81.78% | Test loss: 0.40672, Test acc: 81.30%\n",
      "Epoch: 4500 | Loss: 0.40271, Accuracy: 81.79% | Test loss: 0.40667, Test acc: 81.30%\n",
      "Epoch: 4510 | Loss: 0.40264, Accuracy: 81.79% | Test loss: 0.40662, Test acc: 81.30%\n",
      "Epoch: 4520 | Loss: 0.40258, Accuracy: 81.79% | Test loss: 0.40657, Test acc: 81.34%\n",
      "Epoch: 4530 | Loss: 0.40252, Accuracy: 81.80% | Test loss: 0.40652, Test acc: 81.34%\n",
      "Epoch: 4540 | Loss: 0.40246, Accuracy: 81.80% | Test loss: 0.40647, Test acc: 81.33%\n",
      "Epoch: 4550 | Loss: 0.40240, Accuracy: 81.81% | Test loss: 0.40642, Test acc: 81.32%\n",
      "Epoch: 4560 | Loss: 0.40233, Accuracy: 81.82% | Test loss: 0.40638, Test acc: 81.31%\n",
      "Epoch: 4570 | Loss: 0.40227, Accuracy: 81.83% | Test loss: 0.40633, Test acc: 81.30%\n",
      "Epoch: 4580 | Loss: 0.40221, Accuracy: 81.83% | Test loss: 0.40628, Test acc: 81.30%\n",
      "Epoch: 4590 | Loss: 0.40215, Accuracy: 81.83% | Test loss: 0.40623, Test acc: 81.30%\n",
      "Epoch: 4600 | Loss: 0.40209, Accuracy: 81.84% | Test loss: 0.40619, Test acc: 81.30%\n",
      "Epoch: 4610 | Loss: 0.40203, Accuracy: 81.83% | Test loss: 0.40614, Test acc: 81.28%\n",
      "Epoch: 4620 | Loss: 0.40197, Accuracy: 81.82% | Test loss: 0.40609, Test acc: 81.28%\n",
      "Epoch: 4630 | Loss: 0.40191, Accuracy: 81.82% | Test loss: 0.40605, Test acc: 81.27%\n",
      "Epoch: 4640 | Loss: 0.40185, Accuracy: 81.81% | Test loss: 0.40600, Test acc: 81.26%\n",
      "Epoch: 4650 | Loss: 0.40179, Accuracy: 81.81% | Test loss: 0.40595, Test acc: 81.27%\n",
      "Epoch: 4660 | Loss: 0.40174, Accuracy: 81.81% | Test loss: 0.40591, Test acc: 81.29%\n",
      "Epoch: 4670 | Loss: 0.40168, Accuracy: 81.81% | Test loss: 0.40586, Test acc: 81.27%\n",
      "Epoch: 4680 | Loss: 0.40162, Accuracy: 81.82% | Test loss: 0.40582, Test acc: 81.28%\n",
      "Epoch: 4690 | Loss: 0.40156, Accuracy: 81.81% | Test loss: 0.40577, Test acc: 81.28%\n",
      "Epoch: 4700 | Loss: 0.40150, Accuracy: 81.81% | Test loss: 0.40573, Test acc: 81.27%\n",
      "Epoch: 4710 | Loss: 0.40145, Accuracy: 81.81% | Test loss: 0.40568, Test acc: 81.26%\n",
      "Epoch: 4720 | Loss: 0.40139, Accuracy: 81.80% | Test loss: 0.40564, Test acc: 81.28%\n",
      "Epoch: 4730 | Loss: 0.40133, Accuracy: 81.81% | Test loss: 0.40559, Test acc: 81.28%\n",
      "Epoch: 4740 | Loss: 0.40127, Accuracy: 81.81% | Test loss: 0.40555, Test acc: 81.28%\n",
      "Epoch: 4750 | Loss: 0.40122, Accuracy: 81.81% | Test loss: 0.40551, Test acc: 81.28%\n",
      "Epoch: 4760 | Loss: 0.40116, Accuracy: 81.81% | Test loss: 0.40546, Test acc: 81.28%\n",
      "Epoch: 4770 | Loss: 0.40110, Accuracy: 81.81% | Test loss: 0.40542, Test acc: 81.29%\n",
      "Epoch: 4780 | Loss: 0.40105, Accuracy: 81.81% | Test loss: 0.40538, Test acc: 81.30%\n",
      "Epoch: 4790 | Loss: 0.40099, Accuracy: 81.81% | Test loss: 0.40533, Test acc: 81.30%\n",
      "Epoch: 4800 | Loss: 0.40094, Accuracy: 81.81% | Test loss: 0.40529, Test acc: 81.32%\n",
      "Epoch: 4810 | Loss: 0.40088, Accuracy: 81.82% | Test loss: 0.40525, Test acc: 81.31%\n",
      "Epoch: 4820 | Loss: 0.40083, Accuracy: 81.82% | Test loss: 0.40521, Test acc: 81.30%\n",
      "Epoch: 4830 | Loss: 0.40077, Accuracy: 81.82% | Test loss: 0.40516, Test acc: 81.28%\n",
      "Epoch: 4840 | Loss: 0.40072, Accuracy: 81.82% | Test loss: 0.40512, Test acc: 81.29%\n",
      "Epoch: 4850 | Loss: 0.40066, Accuracy: 81.83% | Test loss: 0.40508, Test acc: 81.30%\n",
      "Epoch: 4860 | Loss: 0.40061, Accuracy: 81.83% | Test loss: 0.40504, Test acc: 81.32%\n",
      "Epoch: 4870 | Loss: 0.40055, Accuracy: 81.83% | Test loss: 0.40500, Test acc: 81.33%\n",
      "Epoch: 4880 | Loss: 0.40050, Accuracy: 81.83% | Test loss: 0.40495, Test acc: 81.31%\n",
      "Epoch: 4890 | Loss: 0.40045, Accuracy: 81.84% | Test loss: 0.40491, Test acc: 81.31%\n",
      "Epoch: 4900 | Loss: 0.40039, Accuracy: 81.84% | Test loss: 0.40487, Test acc: 81.30%\n",
      "Epoch: 4910 | Loss: 0.40034, Accuracy: 81.85% | Test loss: 0.40483, Test acc: 81.30%\n",
      "Epoch: 4920 | Loss: 0.40029, Accuracy: 81.85% | Test loss: 0.40479, Test acc: 81.31%\n",
      "Epoch: 4930 | Loss: 0.40023, Accuracy: 81.84% | Test loss: 0.40475, Test acc: 81.31%\n",
      "Epoch: 4940 | Loss: 0.40018, Accuracy: 81.82% | Test loss: 0.40471, Test acc: 81.31%\n",
      "Epoch: 4950 | Loss: 0.40013, Accuracy: 81.83% | Test loss: 0.40467, Test acc: 81.31%\n",
      "Epoch: 4960 | Loss: 0.40008, Accuracy: 81.84% | Test loss: 0.40463, Test acc: 81.33%\n",
      "Epoch: 4970 | Loss: 0.40002, Accuracy: 81.85% | Test loss: 0.40459, Test acc: 81.34%\n",
      "Epoch: 4980 | Loss: 0.39997, Accuracy: 81.85% | Test loss: 0.40455, Test acc: 81.35%\n",
      "Epoch: 4990 | Loss: 0.39992, Accuracy: 81.86% | Test loss: 0.40451, Test acc: 81.36%\n",
      "Epoch: 5000 | Loss: 0.39987, Accuracy: 81.87% | Test loss: 0.40447, Test acc: 81.36%\n",
      "Epoch: 5010 | Loss: 0.39982, Accuracy: 81.87% | Test loss: 0.40444, Test acc: 81.36%\n",
      "Epoch: 5020 | Loss: 0.39977, Accuracy: 81.88% | Test loss: 0.40440, Test acc: 81.36%\n",
      "Epoch: 5030 | Loss: 0.39972, Accuracy: 81.89% | Test loss: 0.40436, Test acc: 81.36%\n",
      "Epoch: 5040 | Loss: 0.39967, Accuracy: 81.90% | Test loss: 0.40432, Test acc: 81.39%\n",
      "Epoch: 5050 | Loss: 0.39962, Accuracy: 81.90% | Test loss: 0.40428, Test acc: 81.41%\n",
      "Epoch: 5060 | Loss: 0.39957, Accuracy: 81.91% | Test loss: 0.40424, Test acc: 81.40%\n",
      "Epoch: 5070 | Loss: 0.39952, Accuracy: 81.91% | Test loss: 0.40421, Test acc: 81.42%\n",
      "Epoch: 5080 | Loss: 0.39947, Accuracy: 81.91% | Test loss: 0.40417, Test acc: 81.42%\n",
      "Epoch: 5090 | Loss: 0.39942, Accuracy: 81.91% | Test loss: 0.40413, Test acc: 81.42%\n",
      "Epoch: 5100 | Loss: 0.39937, Accuracy: 81.92% | Test loss: 0.40409, Test acc: 81.40%\n",
      "Epoch: 5110 | Loss: 0.39932, Accuracy: 81.92% | Test loss: 0.40406, Test acc: 81.41%\n",
      "Epoch: 5120 | Loss: 0.39927, Accuracy: 81.92% | Test loss: 0.40402, Test acc: 81.42%\n",
      "Epoch: 5130 | Loss: 0.39922, Accuracy: 81.93% | Test loss: 0.40398, Test acc: 81.42%\n",
      "Epoch: 5140 | Loss: 0.39917, Accuracy: 81.94% | Test loss: 0.40395, Test acc: 81.43%\n",
      "Epoch: 5150 | Loss: 0.39912, Accuracy: 81.94% | Test loss: 0.40391, Test acc: 81.43%\n",
      "Epoch: 5160 | Loss: 0.39907, Accuracy: 81.94% | Test loss: 0.40387, Test acc: 81.46%\n",
      "Epoch: 5170 | Loss: 0.39903, Accuracy: 81.94% | Test loss: 0.40384, Test acc: 81.46%\n",
      "Epoch: 5180 | Loss: 0.39898, Accuracy: 81.95% | Test loss: 0.40380, Test acc: 81.45%\n",
      "Epoch: 5190 | Loss: 0.39893, Accuracy: 81.94% | Test loss: 0.40377, Test acc: 81.47%\n",
      "Epoch: 5200 | Loss: 0.39888, Accuracy: 81.95% | Test loss: 0.40373, Test acc: 81.47%\n",
      "Epoch: 5210 | Loss: 0.39884, Accuracy: 81.95% | Test loss: 0.40370, Test acc: 81.46%\n",
      "Epoch: 5220 | Loss: 0.39879, Accuracy: 81.95% | Test loss: 0.40366, Test acc: 81.46%\n",
      "Epoch: 5230 | Loss: 0.39874, Accuracy: 81.96% | Test loss: 0.40362, Test acc: 81.45%\n",
      "Epoch: 5240 | Loss: 0.39870, Accuracy: 81.96% | Test loss: 0.40359, Test acc: 81.46%\n",
      "Epoch: 5250 | Loss: 0.39865, Accuracy: 81.96% | Test loss: 0.40355, Test acc: 81.46%\n",
      "Epoch: 5260 | Loss: 0.39860, Accuracy: 81.97% | Test loss: 0.40352, Test acc: 81.46%\n",
      "Epoch: 5270 | Loss: 0.39856, Accuracy: 81.97% | Test loss: 0.40349, Test acc: 81.45%\n",
      "Epoch: 5280 | Loss: 0.39851, Accuracy: 81.97% | Test loss: 0.40345, Test acc: 81.45%\n",
      "Epoch: 5290 | Loss: 0.39846, Accuracy: 81.97% | Test loss: 0.40342, Test acc: 81.45%\n",
      "Epoch: 5300 | Loss: 0.39842, Accuracy: 81.97% | Test loss: 0.40338, Test acc: 81.46%\n",
      "Epoch: 5310 | Loss: 0.39837, Accuracy: 81.97% | Test loss: 0.40335, Test acc: 81.46%\n",
      "Epoch: 5320 | Loss: 0.39833, Accuracy: 81.98% | Test loss: 0.40332, Test acc: 81.47%\n",
      "Epoch: 5330 | Loss: 0.39828, Accuracy: 81.99% | Test loss: 0.40328, Test acc: 81.48%\n",
      "Epoch: 5340 | Loss: 0.39824, Accuracy: 81.99% | Test loss: 0.40325, Test acc: 81.48%\n",
      "Epoch: 5350 | Loss: 0.39819, Accuracy: 81.99% | Test loss: 0.40322, Test acc: 81.47%\n",
      "Epoch: 5360 | Loss: 0.39815, Accuracy: 81.98% | Test loss: 0.40318, Test acc: 81.48%\n",
      "Epoch: 5370 | Loss: 0.39810, Accuracy: 81.99% | Test loss: 0.40315, Test acc: 81.49%\n",
      "Epoch: 5380 | Loss: 0.39806, Accuracy: 82.00% | Test loss: 0.40312, Test acc: 81.49%\n",
      "Epoch: 5390 | Loss: 0.39802, Accuracy: 82.01% | Test loss: 0.40308, Test acc: 81.50%\n",
      "Epoch: 5400 | Loss: 0.39797, Accuracy: 82.02% | Test loss: 0.40305, Test acc: 81.51%\n",
      "Epoch: 5410 | Loss: 0.39793, Accuracy: 82.02% | Test loss: 0.40302, Test acc: 81.51%\n",
      "Epoch: 5420 | Loss: 0.39788, Accuracy: 82.02% | Test loss: 0.40299, Test acc: 81.52%\n",
      "Epoch: 5430 | Loss: 0.39784, Accuracy: 82.02% | Test loss: 0.40295, Test acc: 81.50%\n",
      "Epoch: 5440 | Loss: 0.39780, Accuracy: 82.01% | Test loss: 0.40292, Test acc: 81.50%\n",
      "Epoch: 5450 | Loss: 0.39776, Accuracy: 82.01% | Test loss: 0.40289, Test acc: 81.50%\n",
      "Epoch: 5460 | Loss: 0.39771, Accuracy: 82.02% | Test loss: 0.40286, Test acc: 81.52%\n",
      "Epoch: 5470 | Loss: 0.39767, Accuracy: 82.02% | Test loss: 0.40283, Test acc: 81.52%\n",
      "Epoch: 5480 | Loss: 0.39763, Accuracy: 82.01% | Test loss: 0.40280, Test acc: 81.50%\n",
      "Epoch: 5490 | Loss: 0.39758, Accuracy: 82.01% | Test loss: 0.40276, Test acc: 81.50%\n",
      "Epoch: 5500 | Loss: 0.39754, Accuracy: 82.01% | Test loss: 0.40273, Test acc: 81.50%\n",
      "Epoch: 5510 | Loss: 0.39750, Accuracy: 82.01% | Test loss: 0.40270, Test acc: 81.51%\n",
      "Epoch: 5520 | Loss: 0.39746, Accuracy: 82.01% | Test loss: 0.40267, Test acc: 81.50%\n",
      "Epoch: 5530 | Loss: 0.39742, Accuracy: 82.03% | Test loss: 0.40264, Test acc: 81.50%\n",
      "Epoch: 5540 | Loss: 0.39738, Accuracy: 82.03% | Test loss: 0.40261, Test acc: 81.53%\n",
      "Epoch: 5550 | Loss: 0.39733, Accuracy: 82.02% | Test loss: 0.40258, Test acc: 81.54%\n",
      "Epoch: 5560 | Loss: 0.39729, Accuracy: 82.02% | Test loss: 0.40255, Test acc: 81.54%\n",
      "Epoch: 5570 | Loss: 0.39725, Accuracy: 82.02% | Test loss: 0.40252, Test acc: 81.54%\n",
      "Epoch: 5580 | Loss: 0.39721, Accuracy: 82.03% | Test loss: 0.40249, Test acc: 81.54%\n",
      "Epoch: 5590 | Loss: 0.39717, Accuracy: 82.04% | Test loss: 0.40246, Test acc: 81.52%\n",
      "Epoch: 5600 | Loss: 0.39713, Accuracy: 82.03% | Test loss: 0.40243, Test acc: 81.51%\n",
      "Epoch: 5610 | Loss: 0.39709, Accuracy: 82.03% | Test loss: 0.40240, Test acc: 81.52%\n",
      "Epoch: 5620 | Loss: 0.39705, Accuracy: 82.03% | Test loss: 0.40237, Test acc: 81.52%\n",
      "Epoch: 5630 | Loss: 0.39701, Accuracy: 82.04% | Test loss: 0.40234, Test acc: 81.53%\n",
      "Epoch: 5640 | Loss: 0.39697, Accuracy: 82.05% | Test loss: 0.40231, Test acc: 81.53%\n",
      "Epoch: 5650 | Loss: 0.39693, Accuracy: 82.05% | Test loss: 0.40228, Test acc: 81.53%\n",
      "Epoch: 5660 | Loss: 0.39689, Accuracy: 82.06% | Test loss: 0.40225, Test acc: 81.53%\n",
      "Epoch: 5670 | Loss: 0.39685, Accuracy: 82.07% | Test loss: 0.40222, Test acc: 81.53%\n",
      "Epoch: 5680 | Loss: 0.39681, Accuracy: 82.08% | Test loss: 0.40219, Test acc: 81.54%\n",
      "Epoch: 5690 | Loss: 0.39677, Accuracy: 82.08% | Test loss: 0.40217, Test acc: 81.54%\n",
      "Epoch: 5700 | Loss: 0.39673, Accuracy: 82.09% | Test loss: 0.40214, Test acc: 81.54%\n",
      "Epoch: 5710 | Loss: 0.39669, Accuracy: 82.09% | Test loss: 0.40211, Test acc: 81.54%\n",
      "Epoch: 5720 | Loss: 0.39665, Accuracy: 82.08% | Test loss: 0.40208, Test acc: 81.55%\n",
      "Epoch: 5730 | Loss: 0.39662, Accuracy: 82.08% | Test loss: 0.40205, Test acc: 81.56%\n",
      "Epoch: 5740 | Loss: 0.39658, Accuracy: 82.08% | Test loss: 0.40202, Test acc: 81.55%\n",
      "Epoch: 5750 | Loss: 0.39654, Accuracy: 82.08% | Test loss: 0.40200, Test acc: 81.55%\n",
      "Epoch: 5760 | Loss: 0.39650, Accuracy: 82.09% | Test loss: 0.40197, Test acc: 81.55%\n",
      "Epoch: 5770 | Loss: 0.39646, Accuracy: 82.09% | Test loss: 0.40194, Test acc: 81.54%\n",
      "Epoch: 5780 | Loss: 0.39642, Accuracy: 82.09% | Test loss: 0.40191, Test acc: 81.53%\n",
      "Epoch: 5790 | Loss: 0.39639, Accuracy: 82.10% | Test loss: 0.40189, Test acc: 81.53%\n",
      "Epoch: 5800 | Loss: 0.39635, Accuracy: 82.10% | Test loss: 0.40186, Test acc: 81.51%\n",
      "Epoch: 5810 | Loss: 0.39631, Accuracy: 82.10% | Test loss: 0.40183, Test acc: 81.52%\n",
      "Epoch: 5820 | Loss: 0.39627, Accuracy: 82.10% | Test loss: 0.40180, Test acc: 81.52%\n",
      "Epoch: 5830 | Loss: 0.39624, Accuracy: 82.10% | Test loss: 0.40178, Test acc: 81.52%\n",
      "Epoch: 5840 | Loss: 0.39620, Accuracy: 82.10% | Test loss: 0.40175, Test acc: 81.50%\n",
      "Epoch: 5850 | Loss: 0.39616, Accuracy: 82.10% | Test loss: 0.40172, Test acc: 81.50%\n",
      "Epoch: 5860 | Loss: 0.39613, Accuracy: 82.10% | Test loss: 0.40170, Test acc: 81.51%\n",
      "Epoch: 5870 | Loss: 0.39609, Accuracy: 82.09% | Test loss: 0.40167, Test acc: 81.50%\n",
      "Epoch: 5880 | Loss: 0.39605, Accuracy: 82.09% | Test loss: 0.40164, Test acc: 81.51%\n",
      "Epoch: 5890 | Loss: 0.39602, Accuracy: 82.09% | Test loss: 0.40162, Test acc: 81.52%\n",
      "Epoch: 5900 | Loss: 0.39598, Accuracy: 82.11% | Test loss: 0.40159, Test acc: 81.52%\n",
      "Epoch: 5910 | Loss: 0.39595, Accuracy: 82.11% | Test loss: 0.40156, Test acc: 81.52%\n",
      "Epoch: 5920 | Loss: 0.39591, Accuracy: 82.11% | Test loss: 0.40154, Test acc: 81.52%\n",
      "Epoch: 5930 | Loss: 0.39587, Accuracy: 82.11% | Test loss: 0.40151, Test acc: 81.53%\n",
      "Epoch: 5940 | Loss: 0.39584, Accuracy: 82.12% | Test loss: 0.40149, Test acc: 81.53%\n",
      "Epoch: 5950 | Loss: 0.39580, Accuracy: 82.12% | Test loss: 0.40146, Test acc: 81.53%\n",
      "Epoch: 5960 | Loss: 0.39577, Accuracy: 82.13% | Test loss: 0.40144, Test acc: 81.54%\n",
      "Epoch: 5970 | Loss: 0.39573, Accuracy: 82.14% | Test loss: 0.40141, Test acc: 81.53%\n",
      "Epoch: 5980 | Loss: 0.39570, Accuracy: 82.14% | Test loss: 0.40139, Test acc: 81.53%\n",
      "Epoch: 5990 | Loss: 0.39566, Accuracy: 82.14% | Test loss: 0.40136, Test acc: 81.52%\n",
      "Epoch: 6000 | Loss: 0.39563, Accuracy: 82.14% | Test loss: 0.40133, Test acc: 81.53%\n",
      "Epoch: 6010 | Loss: 0.39559, Accuracy: 82.15% | Test loss: 0.40131, Test acc: 81.50%\n",
      "Epoch: 6020 | Loss: 0.39556, Accuracy: 82.14% | Test loss: 0.40128, Test acc: 81.50%\n",
      "Epoch: 6030 | Loss: 0.39552, Accuracy: 82.14% | Test loss: 0.40126, Test acc: 81.50%\n",
      "Epoch: 6040 | Loss: 0.39549, Accuracy: 82.14% | Test loss: 0.40124, Test acc: 81.50%\n",
      "Epoch: 6050 | Loss: 0.39546, Accuracy: 82.14% | Test loss: 0.40121, Test acc: 81.51%\n",
      "Epoch: 6060 | Loss: 0.39542, Accuracy: 82.14% | Test loss: 0.40119, Test acc: 81.51%\n",
      "Epoch: 6070 | Loss: 0.39539, Accuracy: 82.15% | Test loss: 0.40116, Test acc: 81.52%\n",
      "Epoch: 6080 | Loss: 0.39535, Accuracy: 82.15% | Test loss: 0.40114, Test acc: 81.52%\n",
      "Epoch: 6090 | Loss: 0.39532, Accuracy: 82.15% | Test loss: 0.40111, Test acc: 81.52%\n",
      "Epoch: 6100 | Loss: 0.39529, Accuracy: 82.15% | Test loss: 0.40109, Test acc: 81.52%\n",
      "Epoch: 6110 | Loss: 0.39525, Accuracy: 82.15% | Test loss: 0.40106, Test acc: 81.51%\n",
      "Epoch: 6120 | Loss: 0.39522, Accuracy: 82.15% | Test loss: 0.40104, Test acc: 81.51%\n",
      "Epoch: 6130 | Loss: 0.39519, Accuracy: 82.16% | Test loss: 0.40102, Test acc: 81.52%\n",
      "Epoch: 6140 | Loss: 0.39516, Accuracy: 82.17% | Test loss: 0.40099, Test acc: 81.51%\n",
      "Epoch: 6150 | Loss: 0.39512, Accuracy: 82.16% | Test loss: 0.40097, Test acc: 81.52%\n",
      "Epoch: 6160 | Loss: 0.39509, Accuracy: 82.16% | Test loss: 0.40095, Test acc: 81.52%\n",
      "Epoch: 6170 | Loss: 0.39506, Accuracy: 82.16% | Test loss: 0.40092, Test acc: 81.53%\n",
      "Epoch: 6180 | Loss: 0.39502, Accuracy: 82.16% | Test loss: 0.40090, Test acc: 81.54%\n",
      "Epoch: 6190 | Loss: 0.39499, Accuracy: 82.17% | Test loss: 0.40088, Test acc: 81.54%\n",
      "Epoch: 6200 | Loss: 0.39496, Accuracy: 82.18% | Test loss: 0.40085, Test acc: 81.54%\n",
      "Epoch: 6210 | Loss: 0.39493, Accuracy: 82.19% | Test loss: 0.40083, Test acc: 81.53%\n",
      "Epoch: 6220 | Loss: 0.39490, Accuracy: 82.20% | Test loss: 0.40081, Test acc: 81.53%\n",
      "Epoch: 6230 | Loss: 0.39486, Accuracy: 82.20% | Test loss: 0.40078, Test acc: 81.52%\n",
      "Epoch: 6240 | Loss: 0.39483, Accuracy: 82.21% | Test loss: 0.40076, Test acc: 81.52%\n",
      "Epoch: 6250 | Loss: 0.39480, Accuracy: 82.21% | Test loss: 0.40074, Test acc: 81.52%\n",
      "Epoch: 6260 | Loss: 0.39477, Accuracy: 82.21% | Test loss: 0.40072, Test acc: 81.52%\n",
      "Epoch: 6270 | Loss: 0.39474, Accuracy: 82.22% | Test loss: 0.40069, Test acc: 81.52%\n",
      "Epoch: 6280 | Loss: 0.39471, Accuracy: 82.23% | Test loss: 0.40067, Test acc: 81.54%\n",
      "Epoch: 6290 | Loss: 0.39468, Accuracy: 82.23% | Test loss: 0.40065, Test acc: 81.51%\n",
      "Epoch: 6300 | Loss: 0.39464, Accuracy: 82.23% | Test loss: 0.40063, Test acc: 81.50%\n",
      "Epoch: 6310 | Loss: 0.39461, Accuracy: 82.23% | Test loss: 0.40060, Test acc: 81.50%\n",
      "Epoch: 6320 | Loss: 0.39458, Accuracy: 82.23% | Test loss: 0.40058, Test acc: 81.50%\n",
      "Epoch: 6330 | Loss: 0.39455, Accuracy: 82.23% | Test loss: 0.40056, Test acc: 81.50%\n",
      "Epoch: 6340 | Loss: 0.39452, Accuracy: 82.24% | Test loss: 0.40054, Test acc: 81.49%\n",
      "Epoch: 6350 | Loss: 0.39449, Accuracy: 82.24% | Test loss: 0.40052, Test acc: 81.49%\n",
      "Epoch: 6360 | Loss: 0.39446, Accuracy: 82.25% | Test loss: 0.40049, Test acc: 81.46%\n",
      "Epoch: 6370 | Loss: 0.39443, Accuracy: 82.24% | Test loss: 0.40047, Test acc: 81.47%\n",
      "Epoch: 6380 | Loss: 0.39440, Accuracy: 82.24% | Test loss: 0.40045, Test acc: 81.49%\n",
      "Epoch: 6390 | Loss: 0.39437, Accuracy: 82.23% | Test loss: 0.40043, Test acc: 81.49%\n",
      "Epoch: 6400 | Loss: 0.39434, Accuracy: 82.24% | Test loss: 0.40041, Test acc: 81.49%\n",
      "Epoch: 6410 | Loss: 0.39431, Accuracy: 82.24% | Test loss: 0.40039, Test acc: 81.50%\n",
      "Epoch: 6420 | Loss: 0.39428, Accuracy: 82.25% | Test loss: 0.40036, Test acc: 81.50%\n",
      "Epoch: 6430 | Loss: 0.39425, Accuracy: 82.26% | Test loss: 0.40034, Test acc: 81.51%\n",
      "Epoch: 6440 | Loss: 0.39422, Accuracy: 82.26% | Test loss: 0.40032, Test acc: 81.50%\n",
      "Epoch: 6450 | Loss: 0.39419, Accuracy: 82.25% | Test loss: 0.40030, Test acc: 81.48%\n",
      "Epoch: 6460 | Loss: 0.39416, Accuracy: 82.26% | Test loss: 0.40028, Test acc: 81.49%\n",
      "Epoch: 6470 | Loss: 0.39413, Accuracy: 82.25% | Test loss: 0.40026, Test acc: 81.47%\n",
      "Epoch: 6480 | Loss: 0.39410, Accuracy: 82.25% | Test loss: 0.40024, Test acc: 81.46%\n",
      "Epoch: 6490 | Loss: 0.39408, Accuracy: 82.25% | Test loss: 0.40022, Test acc: 81.47%\n",
      "Epoch: 6500 | Loss: 0.39405, Accuracy: 82.26% | Test loss: 0.40020, Test acc: 81.47%\n",
      "Epoch: 6510 | Loss: 0.39402, Accuracy: 82.26% | Test loss: 0.40018, Test acc: 81.46%\n",
      "Epoch: 6520 | Loss: 0.39399, Accuracy: 82.26% | Test loss: 0.40016, Test acc: 81.46%\n",
      "Epoch: 6530 | Loss: 0.39396, Accuracy: 82.26% | Test loss: 0.40014, Test acc: 81.46%\n",
      "Epoch: 6540 | Loss: 0.39393, Accuracy: 82.26% | Test loss: 0.40012, Test acc: 81.46%\n",
      "Epoch: 6550 | Loss: 0.39390, Accuracy: 82.26% | Test loss: 0.40010, Test acc: 81.47%\n",
      "Epoch: 6560 | Loss: 0.39388, Accuracy: 82.27% | Test loss: 0.40008, Test acc: 81.47%\n",
      "Epoch: 6570 | Loss: 0.39385, Accuracy: 82.26% | Test loss: 0.40006, Test acc: 81.47%\n",
      "Epoch: 6580 | Loss: 0.39382, Accuracy: 82.26% | Test loss: 0.40004, Test acc: 81.46%\n",
      "Epoch: 6590 | Loss: 0.39379, Accuracy: 82.26% | Test loss: 0.40002, Test acc: 81.47%\n",
      "Epoch: 6600 | Loss: 0.39376, Accuracy: 82.26% | Test loss: 0.40000, Test acc: 81.49%\n",
      "Epoch: 6610 | Loss: 0.39374, Accuracy: 82.26% | Test loss: 0.39998, Test acc: 81.49%\n",
      "Epoch: 6620 | Loss: 0.39371, Accuracy: 82.26% | Test loss: 0.39996, Test acc: 81.49%\n",
      "Epoch: 6630 | Loss: 0.39368, Accuracy: 82.26% | Test loss: 0.39994, Test acc: 81.50%\n",
      "Epoch: 6640 | Loss: 0.39365, Accuracy: 82.25% | Test loss: 0.39992, Test acc: 81.50%\n",
      "Epoch: 6650 | Loss: 0.39363, Accuracy: 82.24% | Test loss: 0.39990, Test acc: 81.50%\n",
      "Epoch: 6660 | Loss: 0.39360, Accuracy: 82.24% | Test loss: 0.39988, Test acc: 81.49%\n",
      "Epoch: 6670 | Loss: 0.39357, Accuracy: 82.24% | Test loss: 0.39986, Test acc: 81.50%\n",
      "Epoch: 6680 | Loss: 0.39355, Accuracy: 82.24% | Test loss: 0.39984, Test acc: 81.50%\n",
      "Epoch: 6690 | Loss: 0.39352, Accuracy: 82.25% | Test loss: 0.39982, Test acc: 81.50%\n",
      "Epoch: 6700 | Loss: 0.39349, Accuracy: 82.26% | Test loss: 0.39980, Test acc: 81.50%\n",
      "Epoch: 6710 | Loss: 0.39346, Accuracy: 82.27% | Test loss: 0.39978, Test acc: 81.50%\n",
      "Epoch: 6720 | Loss: 0.39344, Accuracy: 82.27% | Test loss: 0.39976, Test acc: 81.49%\n",
      "Epoch: 6730 | Loss: 0.39341, Accuracy: 82.28% | Test loss: 0.39974, Test acc: 81.48%\n",
      "Epoch: 6740 | Loss: 0.39339, Accuracy: 82.28% | Test loss: 0.39973, Test acc: 81.47%\n",
      "Epoch: 6750 | Loss: 0.39336, Accuracy: 82.28% | Test loss: 0.39971, Test acc: 81.47%\n",
      "Epoch: 6760 | Loss: 0.39333, Accuracy: 82.29% | Test loss: 0.39969, Test acc: 81.47%\n",
      "Epoch: 6770 | Loss: 0.39331, Accuracy: 82.28% | Test loss: 0.39967, Test acc: 81.49%\n",
      "Epoch: 6780 | Loss: 0.39328, Accuracy: 82.27% | Test loss: 0.39965, Test acc: 81.50%\n",
      "Epoch: 6790 | Loss: 0.39325, Accuracy: 82.27% | Test loss: 0.39963, Test acc: 81.50%\n",
      "Epoch: 6800 | Loss: 0.39323, Accuracy: 82.27% | Test loss: 0.39961, Test acc: 81.49%\n",
      "Epoch: 6810 | Loss: 0.39320, Accuracy: 82.27% | Test loss: 0.39959, Test acc: 81.47%\n",
      "Epoch: 6820 | Loss: 0.39318, Accuracy: 82.27% | Test loss: 0.39958, Test acc: 81.46%\n",
      "Epoch: 6830 | Loss: 0.39315, Accuracy: 82.26% | Test loss: 0.39956, Test acc: 81.46%\n",
      "Epoch: 6840 | Loss: 0.39313, Accuracy: 82.26% | Test loss: 0.39954, Test acc: 81.46%\n",
      "Epoch: 6850 | Loss: 0.39310, Accuracy: 82.27% | Test loss: 0.39952, Test acc: 81.46%\n",
      "Epoch: 6860 | Loss: 0.39307, Accuracy: 82.27% | Test loss: 0.39950, Test acc: 81.46%\n",
      "Epoch: 6870 | Loss: 0.39305, Accuracy: 82.27% | Test loss: 0.39949, Test acc: 81.48%\n",
      "Epoch: 6880 | Loss: 0.39302, Accuracy: 82.27% | Test loss: 0.39947, Test acc: 81.50%\n",
      "Epoch: 6890 | Loss: 0.39300, Accuracy: 82.27% | Test loss: 0.39945, Test acc: 81.51%\n",
      "Epoch: 6900 | Loss: 0.39297, Accuracy: 82.27% | Test loss: 0.39943, Test acc: 81.50%\n",
      "Epoch: 6910 | Loss: 0.39295, Accuracy: 82.27% | Test loss: 0.39941, Test acc: 81.50%\n",
      "Epoch: 6920 | Loss: 0.39292, Accuracy: 82.27% | Test loss: 0.39940, Test acc: 81.50%\n",
      "Epoch: 6930 | Loss: 0.39290, Accuracy: 82.28% | Test loss: 0.39938, Test acc: 81.48%\n",
      "Epoch: 6940 | Loss: 0.39287, Accuracy: 82.28% | Test loss: 0.39936, Test acc: 81.47%\n",
      "Epoch: 6950 | Loss: 0.39285, Accuracy: 82.29% | Test loss: 0.39934, Test acc: 81.47%\n",
      "Epoch: 6960 | Loss: 0.39283, Accuracy: 82.29% | Test loss: 0.39933, Test acc: 81.46%\n",
      "Epoch: 6970 | Loss: 0.39280, Accuracy: 82.29% | Test loss: 0.39931, Test acc: 81.46%\n",
      "Epoch: 6980 | Loss: 0.39278, Accuracy: 82.29% | Test loss: 0.39929, Test acc: 81.47%\n",
      "Epoch: 6990 | Loss: 0.39275, Accuracy: 82.29% | Test loss: 0.39927, Test acc: 81.49%\n",
      "Epoch: 7000 | Loss: 0.39273, Accuracy: 82.29% | Test loss: 0.39926, Test acc: 81.50%\n",
      "Epoch: 7010 | Loss: 0.39270, Accuracy: 82.29% | Test loss: 0.39924, Test acc: 81.51%\n",
      "Epoch: 7020 | Loss: 0.39268, Accuracy: 82.29% | Test loss: 0.39922, Test acc: 81.52%\n",
      "Epoch: 7030 | Loss: 0.39266, Accuracy: 82.29% | Test loss: 0.39921, Test acc: 81.50%\n",
      "Epoch: 7040 | Loss: 0.39263, Accuracy: 82.30% | Test loss: 0.39919, Test acc: 81.50%\n",
      "Epoch: 7050 | Loss: 0.39261, Accuracy: 82.30% | Test loss: 0.39917, Test acc: 81.50%\n",
      "Epoch: 7060 | Loss: 0.39258, Accuracy: 82.31% | Test loss: 0.39916, Test acc: 81.50%\n",
      "Epoch: 7070 | Loss: 0.39256, Accuracy: 82.31% | Test loss: 0.39914, Test acc: 81.49%\n",
      "Epoch: 7080 | Loss: 0.39254, Accuracy: 82.32% | Test loss: 0.39912, Test acc: 81.50%\n",
      "Epoch: 7090 | Loss: 0.39251, Accuracy: 82.31% | Test loss: 0.39911, Test acc: 81.50%\n",
      "Epoch: 7100 | Loss: 0.39249, Accuracy: 82.31% | Test loss: 0.39909, Test acc: 81.50%\n",
      "Epoch: 7110 | Loss: 0.39247, Accuracy: 82.31% | Test loss: 0.39907, Test acc: 81.51%\n",
      "Epoch: 7120 | Loss: 0.39244, Accuracy: 82.31% | Test loss: 0.39906, Test acc: 81.54%\n",
      "Epoch: 7130 | Loss: 0.39242, Accuracy: 82.31% | Test loss: 0.39904, Test acc: 81.54%\n",
      "Epoch: 7140 | Loss: 0.39240, Accuracy: 82.31% | Test loss: 0.39902, Test acc: 81.55%\n",
      "Epoch: 7150 | Loss: 0.39237, Accuracy: 82.31% | Test loss: 0.39901, Test acc: 81.55%\n",
      "Epoch: 7160 | Loss: 0.39235, Accuracy: 82.31% | Test loss: 0.39899, Test acc: 81.55%\n",
      "Epoch: 7170 | Loss: 0.39233, Accuracy: 82.33% | Test loss: 0.39897, Test acc: 81.55%\n",
      "Epoch: 7180 | Loss: 0.39231, Accuracy: 82.32% | Test loss: 0.39896, Test acc: 81.56%\n",
      "Epoch: 7190 | Loss: 0.39228, Accuracy: 82.33% | Test loss: 0.39894, Test acc: 81.55%\n",
      "Epoch: 7200 | Loss: 0.39226, Accuracy: 82.32% | Test loss: 0.39893, Test acc: 81.55%\n",
      "Epoch: 7210 | Loss: 0.39224, Accuracy: 82.32% | Test loss: 0.39891, Test acc: 81.54%\n",
      "Epoch: 7220 | Loss: 0.39221, Accuracy: 82.32% | Test loss: 0.39889, Test acc: 81.54%\n",
      "Epoch: 7230 | Loss: 0.39219, Accuracy: 82.32% | Test loss: 0.39888, Test acc: 81.56%\n",
      "Epoch: 7240 | Loss: 0.39217, Accuracy: 82.33% | Test loss: 0.39886, Test acc: 81.56%\n",
      "Epoch: 7250 | Loss: 0.39215, Accuracy: 82.33% | Test loss: 0.39885, Test acc: 81.57%\n",
      "Epoch: 7260 | Loss: 0.39213, Accuracy: 82.32% | Test loss: 0.39883, Test acc: 81.57%\n",
      "Epoch: 7270 | Loss: 0.39210, Accuracy: 82.32% | Test loss: 0.39882, Test acc: 81.57%\n",
      "Epoch: 7280 | Loss: 0.39208, Accuracy: 82.33% | Test loss: 0.39880, Test acc: 81.58%\n",
      "Epoch: 7290 | Loss: 0.39206, Accuracy: 82.33% | Test loss: 0.39878, Test acc: 81.58%\n",
      "Epoch: 7300 | Loss: 0.39204, Accuracy: 82.33% | Test loss: 0.39877, Test acc: 81.58%\n",
      "Epoch: 7310 | Loss: 0.39202, Accuracy: 82.33% | Test loss: 0.39875, Test acc: 81.57%\n",
      "Epoch: 7320 | Loss: 0.39199, Accuracy: 82.32% | Test loss: 0.39874, Test acc: 81.58%\n",
      "Epoch: 7330 | Loss: 0.39197, Accuracy: 82.32% | Test loss: 0.39872, Test acc: 81.58%\n",
      "Epoch: 7340 | Loss: 0.39195, Accuracy: 82.32% | Test loss: 0.39871, Test acc: 81.57%\n",
      "Epoch: 7350 | Loss: 0.39193, Accuracy: 82.31% | Test loss: 0.39869, Test acc: 81.58%\n",
      "Epoch: 7360 | Loss: 0.39191, Accuracy: 82.32% | Test loss: 0.39868, Test acc: 81.58%\n",
      "Epoch: 7370 | Loss: 0.39189, Accuracy: 82.31% | Test loss: 0.39866, Test acc: 81.58%\n",
      "Epoch: 7380 | Loss: 0.39186, Accuracy: 82.30% | Test loss: 0.39865, Test acc: 81.57%\n",
      "Epoch: 7390 | Loss: 0.39184, Accuracy: 82.30% | Test loss: 0.39863, Test acc: 81.57%\n",
      "Epoch: 7400 | Loss: 0.39182, Accuracy: 82.30% | Test loss: 0.39862, Test acc: 81.57%\n",
      "Epoch: 7410 | Loss: 0.39180, Accuracy: 82.30% | Test loss: 0.39860, Test acc: 81.57%\n",
      "Epoch: 7420 | Loss: 0.39178, Accuracy: 82.30% | Test loss: 0.39859, Test acc: 81.57%\n",
      "Epoch: 7430 | Loss: 0.39176, Accuracy: 82.31% | Test loss: 0.39857, Test acc: 81.55%\n",
      "Epoch: 7440 | Loss: 0.39174, Accuracy: 82.31% | Test loss: 0.39856, Test acc: 81.55%\n",
      "Epoch: 7450 | Loss: 0.39172, Accuracy: 82.31% | Test loss: 0.39854, Test acc: 81.55%\n",
      "Epoch: 7460 | Loss: 0.39169, Accuracy: 82.32% | Test loss: 0.39853, Test acc: 81.56%\n",
      "Epoch: 7470 | Loss: 0.39167, Accuracy: 82.32% | Test loss: 0.39851, Test acc: 81.55%\n",
      "Epoch: 7480 | Loss: 0.39165, Accuracy: 82.31% | Test loss: 0.39850, Test acc: 81.56%\n",
      "Epoch: 7490 | Loss: 0.39163, Accuracy: 82.33% | Test loss: 0.39848, Test acc: 81.57%\n",
      "Epoch: 7500 | Loss: 0.39161, Accuracy: 82.33% | Test loss: 0.39847, Test acc: 81.57%\n",
      "Epoch: 7510 | Loss: 0.39159, Accuracy: 82.33% | Test loss: 0.39845, Test acc: 81.57%\n",
      "Epoch: 7520 | Loss: 0.39157, Accuracy: 82.33% | Test loss: 0.39844, Test acc: 81.57%\n",
      "Epoch: 7530 | Loss: 0.39155, Accuracy: 82.34% | Test loss: 0.39843, Test acc: 81.58%\n",
      "Epoch: 7540 | Loss: 0.39153, Accuracy: 82.34% | Test loss: 0.39841, Test acc: 81.58%\n",
      "Epoch: 7550 | Loss: 0.39151, Accuracy: 82.35% | Test loss: 0.39840, Test acc: 81.57%\n",
      "Epoch: 7560 | Loss: 0.39149, Accuracy: 82.36% | Test loss: 0.39838, Test acc: 81.58%\n",
      "Epoch: 7570 | Loss: 0.39147, Accuracy: 82.35% | Test loss: 0.39837, Test acc: 81.59%\n",
      "Epoch: 7580 | Loss: 0.39145, Accuracy: 82.36% | Test loss: 0.39835, Test acc: 81.58%\n",
      "Epoch: 7590 | Loss: 0.39143, Accuracy: 82.35% | Test loss: 0.39834, Test acc: 81.58%\n",
      "Epoch: 7600 | Loss: 0.39141, Accuracy: 82.36% | Test loss: 0.39833, Test acc: 81.58%\n",
      "Epoch: 7610 | Loss: 0.39139, Accuracy: 82.35% | Test loss: 0.39831, Test acc: 81.58%\n",
      "Epoch: 7620 | Loss: 0.39137, Accuracy: 82.36% | Test loss: 0.39830, Test acc: 81.58%\n",
      "Epoch: 7630 | Loss: 0.39135, Accuracy: 82.35% | Test loss: 0.39828, Test acc: 81.60%\n",
      "Epoch: 7640 | Loss: 0.39133, Accuracy: 82.35% | Test loss: 0.39827, Test acc: 81.59%\n",
      "Epoch: 7650 | Loss: 0.39131, Accuracy: 82.35% | Test loss: 0.39826, Test acc: 81.59%\n",
      "Epoch: 7660 | Loss: 0.39129, Accuracy: 82.34% | Test loss: 0.39824, Test acc: 81.59%\n",
      "Epoch: 7670 | Loss: 0.39127, Accuracy: 82.34% | Test loss: 0.39823, Test acc: 81.60%\n",
      "Epoch: 7680 | Loss: 0.39125, Accuracy: 82.33% | Test loss: 0.39822, Test acc: 81.60%\n",
      "Epoch: 7690 | Loss: 0.39123, Accuracy: 82.33% | Test loss: 0.39820, Test acc: 81.58%\n",
      "Epoch: 7700 | Loss: 0.39121, Accuracy: 82.34% | Test loss: 0.39819, Test acc: 81.58%\n",
      "Epoch: 7710 | Loss: 0.39119, Accuracy: 82.34% | Test loss: 0.39818, Test acc: 81.59%\n",
      "Epoch: 7720 | Loss: 0.39117, Accuracy: 82.33% | Test loss: 0.39816, Test acc: 81.58%\n",
      "Epoch: 7730 | Loss: 0.39115, Accuracy: 82.33% | Test loss: 0.39815, Test acc: 81.58%\n",
      "Epoch: 7740 | Loss: 0.39114, Accuracy: 82.33% | Test loss: 0.39814, Test acc: 81.60%\n",
      "Epoch: 7750 | Loss: 0.39112, Accuracy: 82.33% | Test loss: 0.39812, Test acc: 81.61%\n",
      "Epoch: 7760 | Loss: 0.39110, Accuracy: 82.33% | Test loss: 0.39811, Test acc: 81.60%\n",
      "Epoch: 7770 | Loss: 0.39108, Accuracy: 82.33% | Test loss: 0.39810, Test acc: 81.60%\n",
      "Epoch: 7780 | Loss: 0.39106, Accuracy: 82.33% | Test loss: 0.39808, Test acc: 81.60%\n",
      "Epoch: 7790 | Loss: 0.39104, Accuracy: 82.32% | Test loss: 0.39807, Test acc: 81.60%\n",
      "Epoch: 7800 | Loss: 0.39102, Accuracy: 82.33% | Test loss: 0.39806, Test acc: 81.61%\n",
      "Epoch: 7810 | Loss: 0.39100, Accuracy: 82.32% | Test loss: 0.39804, Test acc: 81.60%\n",
      "Epoch: 7820 | Loss: 0.39098, Accuracy: 82.33% | Test loss: 0.39803, Test acc: 81.59%\n",
      "Epoch: 7830 | Loss: 0.39097, Accuracy: 82.33% | Test loss: 0.39802, Test acc: 81.59%\n",
      "Epoch: 7840 | Loss: 0.39095, Accuracy: 82.33% | Test loss: 0.39800, Test acc: 81.60%\n",
      "Epoch: 7850 | Loss: 0.39093, Accuracy: 82.33% | Test loss: 0.39799, Test acc: 81.62%\n",
      "Epoch: 7860 | Loss: 0.39091, Accuracy: 82.33% | Test loss: 0.39798, Test acc: 81.62%\n",
      "Epoch: 7870 | Loss: 0.39089, Accuracy: 82.34% | Test loss: 0.39797, Test acc: 81.62%\n",
      "Epoch: 7880 | Loss: 0.39087, Accuracy: 82.34% | Test loss: 0.39795, Test acc: 81.60%\n",
      "Epoch: 7890 | Loss: 0.39086, Accuracy: 82.34% | Test loss: 0.39794, Test acc: 81.60%\n",
      "Epoch: 7900 | Loss: 0.39084, Accuracy: 82.33% | Test loss: 0.39793, Test acc: 81.62%\n",
      "Epoch: 7910 | Loss: 0.39082, Accuracy: 82.33% | Test loss: 0.39791, Test acc: 81.62%\n",
      "Epoch: 7920 | Loss: 0.39080, Accuracy: 82.33% | Test loss: 0.39790, Test acc: 81.64%\n",
      "Epoch: 7930 | Loss: 0.39078, Accuracy: 82.32% | Test loss: 0.39789, Test acc: 81.63%\n",
      "Epoch: 7940 | Loss: 0.39077, Accuracy: 82.33% | Test loss: 0.39788, Test acc: 81.63%\n",
      "Epoch: 7950 | Loss: 0.39075, Accuracy: 82.34% | Test loss: 0.39787, Test acc: 81.64%\n",
      "Epoch: 7960 | Loss: 0.39073, Accuracy: 82.34% | Test loss: 0.39785, Test acc: 81.63%\n",
      "Epoch: 7970 | Loss: 0.39071, Accuracy: 82.34% | Test loss: 0.39784, Test acc: 81.63%\n",
      "Epoch: 7980 | Loss: 0.39069, Accuracy: 82.34% | Test loss: 0.39783, Test acc: 81.62%\n",
      "Epoch: 7990 | Loss: 0.39068, Accuracy: 82.34% | Test loss: 0.39782, Test acc: 81.61%\n",
      "Epoch: 8000 | Loss: 0.39066, Accuracy: 82.34% | Test loss: 0.39780, Test acc: 81.60%\n",
      "Epoch: 8010 | Loss: 0.39064, Accuracy: 82.34% | Test loss: 0.39779, Test acc: 81.60%\n",
      "Epoch: 8020 | Loss: 0.39062, Accuracy: 82.34% | Test loss: 0.39778, Test acc: 81.60%\n",
      "Epoch: 8030 | Loss: 0.39061, Accuracy: 82.34% | Test loss: 0.39777, Test acc: 81.62%\n",
      "Epoch: 8040 | Loss: 0.39059, Accuracy: 82.34% | Test loss: 0.39776, Test acc: 81.62%\n",
      "Epoch: 8050 | Loss: 0.39057, Accuracy: 82.34% | Test loss: 0.39774, Test acc: 81.63%\n",
      "Epoch: 8060 | Loss: 0.39055, Accuracy: 82.33% | Test loss: 0.39773, Test acc: 81.64%\n",
      "Epoch: 8070 | Loss: 0.39054, Accuracy: 82.33% | Test loss: 0.39772, Test acc: 81.64%\n",
      "Epoch: 8080 | Loss: 0.39052, Accuracy: 82.33% | Test loss: 0.39771, Test acc: 81.64%\n",
      "Epoch: 8090 | Loss: 0.39050, Accuracy: 82.32% | Test loss: 0.39770, Test acc: 81.64%\n",
      "Epoch: 8100 | Loss: 0.39048, Accuracy: 82.32% | Test loss: 0.39768, Test acc: 81.65%\n",
      "Epoch: 8110 | Loss: 0.39047, Accuracy: 82.32% | Test loss: 0.39767, Test acc: 81.65%\n",
      "Epoch: 8120 | Loss: 0.39045, Accuracy: 82.33% | Test loss: 0.39766, Test acc: 81.64%\n",
      "Epoch: 8130 | Loss: 0.39043, Accuracy: 82.32% | Test loss: 0.39765, Test acc: 81.64%\n",
      "Epoch: 8140 | Loss: 0.39042, Accuracy: 82.32% | Test loss: 0.39764, Test acc: 81.64%\n",
      "Epoch: 8150 | Loss: 0.39040, Accuracy: 82.33% | Test loss: 0.39763, Test acc: 81.65%\n",
      "Epoch: 8160 | Loss: 0.39038, Accuracy: 82.33% | Test loss: 0.39762, Test acc: 81.65%\n",
      "Epoch: 8170 | Loss: 0.39037, Accuracy: 82.33% | Test loss: 0.39760, Test acc: 81.66%\n",
      "Epoch: 8180 | Loss: 0.39035, Accuracy: 82.33% | Test loss: 0.39759, Test acc: 81.65%\n",
      "Epoch: 8190 | Loss: 0.39033, Accuracy: 82.33% | Test loss: 0.39758, Test acc: 81.65%\n",
      "Epoch: 8200 | Loss: 0.39032, Accuracy: 82.33% | Test loss: 0.39757, Test acc: 81.65%\n",
      "Epoch: 8210 | Loss: 0.39030, Accuracy: 82.33% | Test loss: 0.39756, Test acc: 81.65%\n",
      "Epoch: 8220 | Loss: 0.39028, Accuracy: 82.33% | Test loss: 0.39755, Test acc: 81.65%\n",
      "Epoch: 8230 | Loss: 0.39027, Accuracy: 82.33% | Test loss: 0.39754, Test acc: 81.65%\n",
      "Epoch: 8240 | Loss: 0.39025, Accuracy: 82.32% | Test loss: 0.39753, Test acc: 81.64%\n",
      "Epoch: 8250 | Loss: 0.39023, Accuracy: 82.32% | Test loss: 0.39751, Test acc: 81.65%\n",
      "Epoch: 8260 | Loss: 0.39022, Accuracy: 82.32% | Test loss: 0.39750, Test acc: 81.65%\n",
      "Epoch: 8270 | Loss: 0.39020, Accuracy: 82.32% | Test loss: 0.39749, Test acc: 81.64%\n",
      "Epoch: 8280 | Loss: 0.39019, Accuracy: 82.33% | Test loss: 0.39748, Test acc: 81.65%\n",
      "Epoch: 8290 | Loss: 0.39017, Accuracy: 82.33% | Test loss: 0.39747, Test acc: 81.65%\n",
      "Epoch: 8300 | Loss: 0.39015, Accuracy: 82.32% | Test loss: 0.39746, Test acc: 81.66%\n",
      "Epoch: 8310 | Loss: 0.39014, Accuracy: 82.32% | Test loss: 0.39745, Test acc: 81.66%\n",
      "Epoch: 8320 | Loss: 0.39012, Accuracy: 82.32% | Test loss: 0.39744, Test acc: 81.66%\n",
      "Epoch: 8330 | Loss: 0.39011, Accuracy: 82.32% | Test loss: 0.39743, Test acc: 81.66%\n",
      "Epoch: 8340 | Loss: 0.39009, Accuracy: 82.32% | Test loss: 0.39742, Test acc: 81.64%\n",
      "Epoch: 8350 | Loss: 0.39007, Accuracy: 82.32% | Test loss: 0.39741, Test acc: 81.64%\n",
      "Epoch: 8360 | Loss: 0.39006, Accuracy: 82.33% | Test loss: 0.39740, Test acc: 81.64%\n",
      "Epoch: 8370 | Loss: 0.39004, Accuracy: 82.33% | Test loss: 0.39739, Test acc: 81.64%\n",
      "Epoch: 8380 | Loss: 0.39003, Accuracy: 82.33% | Test loss: 0.39737, Test acc: 81.64%\n",
      "Epoch: 8390 | Loss: 0.39001, Accuracy: 82.34% | Test loss: 0.39736, Test acc: 81.64%\n",
      "Epoch: 8400 | Loss: 0.39000, Accuracy: 82.34% | Test loss: 0.39735, Test acc: 81.63%\n",
      "Epoch: 8410 | Loss: 0.38998, Accuracy: 82.34% | Test loss: 0.39734, Test acc: 81.64%\n",
      "Epoch: 8420 | Loss: 0.38996, Accuracy: 82.34% | Test loss: 0.39733, Test acc: 81.64%\n",
      "Epoch: 8430 | Loss: 0.38995, Accuracy: 82.35% | Test loss: 0.39732, Test acc: 81.64%\n",
      "Epoch: 8440 | Loss: 0.38993, Accuracy: 82.35% | Test loss: 0.39731, Test acc: 81.62%\n",
      "Epoch: 8450 | Loss: 0.38992, Accuracy: 82.36% | Test loss: 0.39730, Test acc: 81.62%\n",
      "Epoch: 8460 | Loss: 0.38990, Accuracy: 82.37% | Test loss: 0.39729, Test acc: 81.62%\n",
      "Epoch: 8470 | Loss: 0.38989, Accuracy: 82.36% | Test loss: 0.39728, Test acc: 81.62%\n",
      "Epoch: 8480 | Loss: 0.38987, Accuracy: 82.36% | Test loss: 0.39727, Test acc: 81.62%\n",
      "Epoch: 8490 | Loss: 0.38986, Accuracy: 82.36% | Test loss: 0.39726, Test acc: 81.62%\n",
      "Epoch: 8500 | Loss: 0.38984, Accuracy: 82.36% | Test loss: 0.39725, Test acc: 81.62%\n",
      "Epoch: 8510 | Loss: 0.38983, Accuracy: 82.36% | Test loss: 0.39724, Test acc: 81.62%\n",
      "Epoch: 8520 | Loss: 0.38981, Accuracy: 82.36% | Test loss: 0.39723, Test acc: 81.62%\n",
      "Epoch: 8530 | Loss: 0.38980, Accuracy: 82.36% | Test loss: 0.39722, Test acc: 81.62%\n",
      "Epoch: 8540 | Loss: 0.38978, Accuracy: 82.36% | Test loss: 0.39721, Test acc: 81.62%\n",
      "Epoch: 8550 | Loss: 0.38977, Accuracy: 82.36% | Test loss: 0.39720, Test acc: 81.62%\n",
      "Epoch: 8560 | Loss: 0.38975, Accuracy: 82.36% | Test loss: 0.39719, Test acc: 81.63%\n",
      "Epoch: 8570 | Loss: 0.38974, Accuracy: 82.36% | Test loss: 0.39718, Test acc: 81.63%\n",
      "Epoch: 8580 | Loss: 0.38972, Accuracy: 82.35% | Test loss: 0.39717, Test acc: 81.63%\n",
      "Epoch: 8590 | Loss: 0.38971, Accuracy: 82.35% | Test loss: 0.39717, Test acc: 81.63%\n",
      "Epoch: 8600 | Loss: 0.38970, Accuracy: 82.36% | Test loss: 0.39716, Test acc: 81.65%\n",
      "Epoch: 8610 | Loss: 0.38968, Accuracy: 82.37% | Test loss: 0.39715, Test acc: 81.65%\n",
      "Epoch: 8620 | Loss: 0.38967, Accuracy: 82.38% | Test loss: 0.39714, Test acc: 81.64%\n",
      "Epoch: 8630 | Loss: 0.38965, Accuracy: 82.38% | Test loss: 0.39713, Test acc: 81.64%\n",
      "Epoch: 8640 | Loss: 0.38964, Accuracy: 82.38% | Test loss: 0.39712, Test acc: 81.62%\n",
      "Epoch: 8650 | Loss: 0.38962, Accuracy: 82.38% | Test loss: 0.39711, Test acc: 81.62%\n",
      "Epoch: 8660 | Loss: 0.38961, Accuracy: 82.38% | Test loss: 0.39710, Test acc: 81.62%\n",
      "Epoch: 8670 | Loss: 0.38960, Accuracy: 82.38% | Test loss: 0.39709, Test acc: 81.62%\n",
      "Epoch: 8680 | Loss: 0.38958, Accuracy: 82.39% | Test loss: 0.39708, Test acc: 81.62%\n",
      "Epoch: 8690 | Loss: 0.38957, Accuracy: 82.38% | Test loss: 0.39707, Test acc: 81.62%\n",
      "Epoch: 8700 | Loss: 0.38955, Accuracy: 82.39% | Test loss: 0.39706, Test acc: 81.61%\n",
      "Epoch: 8710 | Loss: 0.38954, Accuracy: 82.39% | Test loss: 0.39705, Test acc: 81.61%\n",
      "Epoch: 8720 | Loss: 0.38952, Accuracy: 82.39% | Test loss: 0.39705, Test acc: 81.60%\n",
      "Epoch: 8730 | Loss: 0.38951, Accuracy: 82.39% | Test loss: 0.39704, Test acc: 81.61%\n",
      "Epoch: 8740 | Loss: 0.38950, Accuracy: 82.39% | Test loss: 0.39703, Test acc: 81.60%\n",
      "Epoch: 8750 | Loss: 0.38948, Accuracy: 82.40% | Test loss: 0.39702, Test acc: 81.59%\n",
      "Epoch: 8760 | Loss: 0.38947, Accuracy: 82.40% | Test loss: 0.39701, Test acc: 81.59%\n",
      "Epoch: 8770 | Loss: 0.38946, Accuracy: 82.41% | Test loss: 0.39700, Test acc: 81.58%\n",
      "Epoch: 8780 | Loss: 0.38944, Accuracy: 82.40% | Test loss: 0.39699, Test acc: 81.58%\n",
      "Epoch: 8790 | Loss: 0.38943, Accuracy: 82.40% | Test loss: 0.39698, Test acc: 81.58%\n",
      "Epoch: 8800 | Loss: 0.38942, Accuracy: 82.41% | Test loss: 0.39698, Test acc: 81.58%\n",
      "Epoch: 8810 | Loss: 0.38940, Accuracy: 82.41% | Test loss: 0.39697, Test acc: 81.55%\n",
      "Epoch: 8820 | Loss: 0.38939, Accuracy: 82.41% | Test loss: 0.39696, Test acc: 81.55%\n",
      "Epoch: 8830 | Loss: 0.38937, Accuracy: 82.40% | Test loss: 0.39695, Test acc: 81.55%\n",
      "Epoch: 8840 | Loss: 0.38936, Accuracy: 82.39% | Test loss: 0.39694, Test acc: 81.56%\n",
      "Epoch: 8850 | Loss: 0.38935, Accuracy: 82.39% | Test loss: 0.39693, Test acc: 81.56%\n",
      "Epoch: 8860 | Loss: 0.38933, Accuracy: 82.39% | Test loss: 0.39692, Test acc: 81.56%\n",
      "Epoch: 8870 | Loss: 0.38932, Accuracy: 82.39% | Test loss: 0.39692, Test acc: 81.57%\n",
      "Epoch: 8880 | Loss: 0.38931, Accuracy: 82.40% | Test loss: 0.39691, Test acc: 81.56%\n",
      "Epoch: 8890 | Loss: 0.38930, Accuracy: 82.41% | Test loss: 0.39690, Test acc: 81.56%\n",
      "Epoch: 8900 | Loss: 0.38928, Accuracy: 82.41% | Test loss: 0.39689, Test acc: 81.56%\n",
      "Epoch: 8910 | Loss: 0.38927, Accuracy: 82.41% | Test loss: 0.39688, Test acc: 81.57%\n",
      "Epoch: 8920 | Loss: 0.38926, Accuracy: 82.41% | Test loss: 0.39688, Test acc: 81.57%\n",
      "Epoch: 8930 | Loss: 0.38924, Accuracy: 82.41% | Test loss: 0.39687, Test acc: 81.57%\n",
      "Epoch: 8940 | Loss: 0.38923, Accuracy: 82.41% | Test loss: 0.39686, Test acc: 81.58%\n",
      "Epoch: 8950 | Loss: 0.38922, Accuracy: 82.41% | Test loss: 0.39685, Test acc: 81.58%\n",
      "Epoch: 8960 | Loss: 0.38920, Accuracy: 82.42% | Test loss: 0.39684, Test acc: 81.58%\n",
      "Epoch: 8970 | Loss: 0.38919, Accuracy: 82.42% | Test loss: 0.39684, Test acc: 81.58%\n",
      "Epoch: 8980 | Loss: 0.38918, Accuracy: 82.42% | Test loss: 0.39683, Test acc: 81.59%\n",
      "Epoch: 8990 | Loss: 0.38917, Accuracy: 82.41% | Test loss: 0.39682, Test acc: 81.60%\n",
      "Epoch: 9000 | Loss: 0.38915, Accuracy: 82.41% | Test loss: 0.39681, Test acc: 81.59%\n",
      "Epoch: 9010 | Loss: 0.38914, Accuracy: 82.40% | Test loss: 0.39680, Test acc: 81.60%\n",
      "Epoch: 9020 | Loss: 0.38913, Accuracy: 82.41% | Test loss: 0.39680, Test acc: 81.60%\n",
      "Epoch: 9030 | Loss: 0.38912, Accuracy: 82.41% | Test loss: 0.39679, Test acc: 81.60%\n",
      "Epoch: 9040 | Loss: 0.38910, Accuracy: 82.41% | Test loss: 0.39678, Test acc: 81.58%\n",
      "Epoch: 9050 | Loss: 0.38909, Accuracy: 82.41% | Test loss: 0.39677, Test acc: 81.59%\n",
      "Epoch: 9060 | Loss: 0.38908, Accuracy: 82.41% | Test loss: 0.39677, Test acc: 81.58%\n",
      "Epoch: 9070 | Loss: 0.38907, Accuracy: 82.42% | Test loss: 0.39676, Test acc: 81.58%\n",
      "Epoch: 9080 | Loss: 0.38906, Accuracy: 82.42% | Test loss: 0.39675, Test acc: 81.58%\n",
      "Epoch: 9090 | Loss: 0.38904, Accuracy: 82.43% | Test loss: 0.39675, Test acc: 81.59%\n",
      "Epoch: 9100 | Loss: 0.38903, Accuracy: 82.44% | Test loss: 0.39674, Test acc: 81.59%\n",
      "Epoch: 9110 | Loss: 0.38902, Accuracy: 82.45% | Test loss: 0.39673, Test acc: 81.59%\n",
      "Epoch: 9120 | Loss: 0.38901, Accuracy: 82.46% | Test loss: 0.39672, Test acc: 81.58%\n",
      "Epoch: 9130 | Loss: 0.38900, Accuracy: 82.46% | Test loss: 0.39672, Test acc: 81.58%\n",
      "Epoch: 9140 | Loss: 0.38898, Accuracy: 82.46% | Test loss: 0.39671, Test acc: 81.56%\n",
      "Epoch: 9150 | Loss: 0.38897, Accuracy: 82.46% | Test loss: 0.39670, Test acc: 81.53%\n",
      "Epoch: 9160 | Loss: 0.38896, Accuracy: 82.45% | Test loss: 0.39670, Test acc: 81.53%\n",
      "Epoch: 9170 | Loss: 0.38895, Accuracy: 82.45% | Test loss: 0.39669, Test acc: 81.54%\n",
      "Epoch: 9180 | Loss: 0.38894, Accuracy: 82.45% | Test loss: 0.39668, Test acc: 81.53%\n",
      "Epoch: 9190 | Loss: 0.38892, Accuracy: 82.45% | Test loss: 0.39667, Test acc: 81.54%\n",
      "Epoch: 9200 | Loss: 0.38891, Accuracy: 82.45% | Test loss: 0.39667, Test acc: 81.55%\n",
      "Epoch: 9210 | Loss: 0.38890, Accuracy: 82.45% | Test loss: 0.39666, Test acc: 81.55%\n",
      "Epoch: 9220 | Loss: 0.38889, Accuracy: 82.45% | Test loss: 0.39665, Test acc: 81.56%\n",
      "Epoch: 9230 | Loss: 0.38888, Accuracy: 82.46% | Test loss: 0.39665, Test acc: 81.56%\n",
      "Epoch: 9240 | Loss: 0.38887, Accuracy: 82.46% | Test loss: 0.39664, Test acc: 81.56%\n",
      "Epoch: 9250 | Loss: 0.38886, Accuracy: 82.46% | Test loss: 0.39663, Test acc: 81.55%\n",
      "Epoch: 9260 | Loss: 0.38884, Accuracy: 82.46% | Test loss: 0.39663, Test acc: 81.55%\n",
      "Epoch: 9270 | Loss: 0.38883, Accuracy: 82.46% | Test loss: 0.39662, Test acc: 81.54%\n",
      "Epoch: 9280 | Loss: 0.38882, Accuracy: 82.47% | Test loss: 0.39661, Test acc: 81.54%\n",
      "Epoch: 9290 | Loss: 0.38881, Accuracy: 82.47% | Test loss: 0.39661, Test acc: 81.54%\n",
      "Epoch: 9300 | Loss: 0.38880, Accuracy: 82.47% | Test loss: 0.39660, Test acc: 81.54%\n",
      "Epoch: 9310 | Loss: 0.38879, Accuracy: 82.47% | Test loss: 0.39660, Test acc: 81.55%\n",
      "Epoch: 9320 | Loss: 0.38878, Accuracy: 82.47% | Test loss: 0.39659, Test acc: 81.54%\n",
      "Epoch: 9330 | Loss: 0.38877, Accuracy: 82.47% | Test loss: 0.39658, Test acc: 81.54%\n",
      "Epoch: 9340 | Loss: 0.38875, Accuracy: 82.48% | Test loss: 0.39658, Test acc: 81.55%\n",
      "Epoch: 9350 | Loss: 0.38874, Accuracy: 82.47% | Test loss: 0.39657, Test acc: 81.55%\n",
      "Epoch: 9360 | Loss: 0.38873, Accuracy: 82.47% | Test loss: 0.39656, Test acc: 81.55%\n",
      "Epoch: 9370 | Loss: 0.38872, Accuracy: 82.48% | Test loss: 0.39656, Test acc: 81.54%\n",
      "Epoch: 9380 | Loss: 0.38871, Accuracy: 82.49% | Test loss: 0.39655, Test acc: 81.55%\n",
      "Epoch: 9390 | Loss: 0.38870, Accuracy: 82.49% | Test loss: 0.39655, Test acc: 81.55%\n",
      "Epoch: 9400 | Loss: 0.38869, Accuracy: 82.50% | Test loss: 0.39654, Test acc: 81.54%\n",
      "Epoch: 9410 | Loss: 0.38868, Accuracy: 82.50% | Test loss: 0.39653, Test acc: 81.54%\n",
      "Epoch: 9420 | Loss: 0.38867, Accuracy: 82.50% | Test loss: 0.39653, Test acc: 81.55%\n",
      "Epoch: 9430 | Loss: 0.38866, Accuracy: 82.50% | Test loss: 0.39652, Test acc: 81.54%\n",
      "Epoch: 9440 | Loss: 0.38865, Accuracy: 82.50% | Test loss: 0.39652, Test acc: 81.53%\n",
      "Epoch: 9450 | Loss: 0.38864, Accuracy: 82.50% | Test loss: 0.39651, Test acc: 81.53%\n",
      "Epoch: 9460 | Loss: 0.38863, Accuracy: 82.49% | Test loss: 0.39651, Test acc: 81.50%\n",
      "Epoch: 9470 | Loss: 0.38862, Accuracy: 82.50% | Test loss: 0.39650, Test acc: 81.50%\n",
      "Epoch: 9480 | Loss: 0.38861, Accuracy: 82.50% | Test loss: 0.39649, Test acc: 81.49%\n",
      "Epoch: 9490 | Loss: 0.38860, Accuracy: 82.50% | Test loss: 0.39649, Test acc: 81.49%\n",
      "Epoch: 9500 | Loss: 0.38859, Accuracy: 82.50% | Test loss: 0.39648, Test acc: 81.48%\n",
      "Epoch: 9510 | Loss: 0.38858, Accuracy: 82.50% | Test loss: 0.39648, Test acc: 81.47%\n",
      "Epoch: 9520 | Loss: 0.38857, Accuracy: 82.51% | Test loss: 0.39647, Test acc: 81.47%\n",
      "Epoch: 9530 | Loss: 0.38855, Accuracy: 82.51% | Test loss: 0.39647, Test acc: 81.48%\n",
      "Epoch: 9540 | Loss: 0.38854, Accuracy: 82.51% | Test loss: 0.39646, Test acc: 81.48%\n",
      "Epoch: 9550 | Loss: 0.38853, Accuracy: 82.50% | Test loss: 0.39646, Test acc: 81.47%\n",
      "Epoch: 9560 | Loss: 0.38852, Accuracy: 82.50% | Test loss: 0.39645, Test acc: 81.46%\n",
      "Epoch: 9570 | Loss: 0.38851, Accuracy: 82.50% | Test loss: 0.39645, Test acc: 81.46%\n",
      "Epoch: 9580 | Loss: 0.38851, Accuracy: 82.50% | Test loss: 0.39644, Test acc: 81.47%\n",
      "Epoch: 9590 | Loss: 0.38850, Accuracy: 82.50% | Test loss: 0.39643, Test acc: 81.47%\n",
      "Epoch: 9600 | Loss: 0.38849, Accuracy: 82.51% | Test loss: 0.39643, Test acc: 81.47%\n",
      "Epoch: 9610 | Loss: 0.38848, Accuracy: 82.51% | Test loss: 0.39642, Test acc: 81.47%\n",
      "Epoch: 9620 | Loss: 0.38847, Accuracy: 82.52% | Test loss: 0.39642, Test acc: 81.47%\n",
      "Epoch: 9630 | Loss: 0.38846, Accuracy: 82.52% | Test loss: 0.39641, Test acc: 81.46%\n",
      "Epoch: 9640 | Loss: 0.38845, Accuracy: 82.51% | Test loss: 0.39641, Test acc: 81.46%\n",
      "Epoch: 9650 | Loss: 0.38844, Accuracy: 82.52% | Test loss: 0.39640, Test acc: 81.47%\n",
      "Epoch: 9660 | Loss: 0.38843, Accuracy: 82.52% | Test loss: 0.39640, Test acc: 81.47%\n",
      "Epoch: 9670 | Loss: 0.38842, Accuracy: 82.53% | Test loss: 0.39639, Test acc: 81.47%\n",
      "Epoch: 9680 | Loss: 0.38841, Accuracy: 82.53% | Test loss: 0.39639, Test acc: 81.47%\n",
      "Epoch: 9690 | Loss: 0.38840, Accuracy: 82.53% | Test loss: 0.39639, Test acc: 81.47%\n",
      "Epoch: 9700 | Loss: 0.38839, Accuracy: 82.53% | Test loss: 0.39638, Test acc: 81.47%\n",
      "Epoch: 9710 | Loss: 0.38838, Accuracy: 82.53% | Test loss: 0.39638, Test acc: 81.48%\n",
      "Epoch: 9720 | Loss: 0.38837, Accuracy: 82.53% | Test loss: 0.39637, Test acc: 81.48%\n",
      "Epoch: 9730 | Loss: 0.38836, Accuracy: 82.53% | Test loss: 0.39637, Test acc: 81.47%\n",
      "Epoch: 9740 | Loss: 0.38835, Accuracy: 82.53% | Test loss: 0.39636, Test acc: 81.47%\n",
      "Epoch: 9750 | Loss: 0.38834, Accuracy: 82.53% | Test loss: 0.39636, Test acc: 81.48%\n",
      "Epoch: 9760 | Loss: 0.38833, Accuracy: 82.52% | Test loss: 0.39635, Test acc: 81.48%\n",
      "Epoch: 9770 | Loss: 0.38833, Accuracy: 82.52% | Test loss: 0.39635, Test acc: 81.48%\n",
      "Epoch: 9780 | Loss: 0.38832, Accuracy: 82.52% | Test loss: 0.39634, Test acc: 81.48%\n",
      "Epoch: 9790 | Loss: 0.38831, Accuracy: 82.52% | Test loss: 0.39634, Test acc: 81.47%\n",
      "Epoch: 9800 | Loss: 0.38830, Accuracy: 82.51% | Test loss: 0.39634, Test acc: 81.47%\n",
      "Epoch: 9810 | Loss: 0.38829, Accuracy: 82.52% | Test loss: 0.39633, Test acc: 81.47%\n",
      "Epoch: 9820 | Loss: 0.38828, Accuracy: 82.51% | Test loss: 0.39633, Test acc: 81.48%\n",
      "Epoch: 9830 | Loss: 0.38827, Accuracy: 82.52% | Test loss: 0.39632, Test acc: 81.47%\n",
      "Epoch: 9840 | Loss: 0.38826, Accuracy: 82.52% | Test loss: 0.39632, Test acc: 81.46%\n",
      "Epoch: 9850 | Loss: 0.38825, Accuracy: 82.53% | Test loss: 0.39631, Test acc: 81.46%\n",
      "Epoch: 9860 | Loss: 0.38825, Accuracy: 82.53% | Test loss: 0.39631, Test acc: 81.46%\n",
      "Epoch: 9870 | Loss: 0.38824, Accuracy: 82.53% | Test loss: 0.39631, Test acc: 81.46%\n",
      "Epoch: 9880 | Loss: 0.38823, Accuracy: 82.53% | Test loss: 0.39630, Test acc: 81.46%\n",
      "Epoch: 9890 | Loss: 0.38822, Accuracy: 82.53% | Test loss: 0.39630, Test acc: 81.45%\n",
      "Epoch: 9900 | Loss: 0.38821, Accuracy: 82.53% | Test loss: 0.39629, Test acc: 81.46%\n",
      "Epoch: 9910 | Loss: 0.38820, Accuracy: 82.53% | Test loss: 0.39629, Test acc: 81.46%\n",
      "Epoch: 9920 | Loss: 0.38819, Accuracy: 82.53% | Test loss: 0.39629, Test acc: 81.46%\n",
      "Epoch: 9930 | Loss: 0.38819, Accuracy: 82.54% | Test loss: 0.39628, Test acc: 81.46%\n",
      "Epoch: 9940 | Loss: 0.38818, Accuracy: 82.54% | Test loss: 0.39628, Test acc: 81.46%\n",
      "Epoch: 9950 | Loss: 0.38817, Accuracy: 82.55% | Test loss: 0.39627, Test acc: 81.46%\n",
      "Epoch: 9960 | Loss: 0.38816, Accuracy: 82.55% | Test loss: 0.39627, Test acc: 81.46%\n",
      "Epoch: 9970 | Loss: 0.38815, Accuracy: 82.55% | Test loss: 0.39627, Test acc: 81.45%\n",
      "Epoch: 9980 | Loss: 0.38815, Accuracy: 82.55% | Test loss: 0.39626, Test acc: 81.46%\n",
      "Epoch: 9990 | Loss: 0.38814, Accuracy: 82.55% | Test loss: 0.39626, Test acc: 81.46%\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(42)\n",
    "\n",
    "# # Set the number of epochs\n",
    "# epochs = 10000\n",
    "\n",
    "# # Put data to target device\n",
    "# X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "# X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# # Build training and evaluation loop\n",
    "# for epoch in range(epochs):\n",
    "#     model_0.train()\n",
    "\n",
    "#     # 1. Forward pass (model outputs raw logits)\n",
    "#     y_logits = model_0(X_train).squeeze() # squeeze to remove extra `1` dimensions, this won't work unless model and data are on same device \n",
    "#     y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labls\n",
    "  \n",
    "#     # 2. Calculate loss/accuracy\n",
    "#     # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()\n",
    "#     #                y_train) \n",
    "#     loss = loss_fn(y_logits, # Using nn.BCEWithLogitsLoss works with raw logits\n",
    "#                    y_train) \n",
    "#     acc = accuracy_fn(y_true=y_train, \n",
    "#                       y_pred=y_pred) \n",
    "\n",
    "#     # 3. Optimizer zero grad\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     # 4. Loss backwards\n",
    "#     loss.backward()\n",
    "\n",
    "#     # 5. Optimizer step\n",
    "#     optimizer.step()\n",
    "\n",
    "#     ### Testing\n",
    "#     model_0.eval()\n",
    "#     with torch.inference_mode():\n",
    "#         # 1. Forward pass\n",
    "#         test_logits = model_0(X_test).squeeze() \n",
    "#         test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "#         # 2. Caculate loss/accuracy\n",
    "#         test_loss = loss_fn(test_logits,\n",
    "#                             y_test)\n",
    "#         test_acc = accuracy_fn(y_true=y_test,\n",
    "#                                y_pred=test_pred)\n",
    "\n",
    "#     # Print out what's happening every 10 epochs\n",
    "#     if epoch % 10 == 0:\n",
    "#         print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa2d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_1 = nn.Sequential(\n",
    "    nn.Linear(in_features=384, out_features=1),\n",
    "    ).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5e684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "model_0 = Classifier_modelV0().to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = Adam(params=model_1.parameters() , lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a84369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting mlflow\n",
      "  Downloading mlflow-3.9.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting mlflow-skinny==3.9.0 (from mlflow)\n",
      "  Downloading mlflow_skinny-3.9.0-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting mlflow-tracing==3.9.0 (from mlflow)\n",
      "  Downloading mlflow_tracing-3.9.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting Flask-CORS<7 (from mlflow)\n",
      "  Downloading flask_cors-6.0.2-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting Flask<4 (from mlflow)\n",
      "  Downloading flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
      "  Downloading alembic-1.18.3-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting cryptography<47,>=43.0.0 (from mlflow)\n",
      "  Downloading cryptography-46.0.4-cp311-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting docker<8,>=4.0.0 (from mlflow)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting graphene<4 (from mlflow)\n",
      "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting huey<3,>=2.5.4 (from mlflow)\n",
      "  Downloading huey-2.6.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: matplotlib<4 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from mlflow) (3.10.8)\n",
      "Requirement already satisfied: numpy<3 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from mlflow) (2.3.5)\n",
      "Requirement already satisfied: pandas<3 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from mlflow) (2.3.3)\n",
      "Collecting pyarrow<23,>=4.0.0 (from mlflow)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: scikit-learn<2 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from mlflow) (1.8.0)\n",
      "Requirement already satisfied: scipy<2 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from mlflow) (1.16.3)\n",
      "Collecting skops<1 (from mlflow)\n",
      "  Downloading skops-0.13.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting sqlalchemy<3,>=1.4.0 (from mlflow)\n",
      "  Downloading sqlalchemy-2.0.46-cp312-cp312-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting waitress<4 (from mlflow)\n",
      "  Downloading waitress-3.0.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting cachetools<7,>=5.0.0 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading cachetools-6.2.6-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow) (8.3.1)\n",
      "Requirement already satisfied: cloudpickle<4 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow) (3.1.2)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading databricks_sdk-0.86.0-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting fastapi<1 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading fastapi-0.128.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting gitpython<4,>=3.1.9 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading gitpython-3.1.46-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting importlib_metadata!=4.7.0,<9,>=3.7.0 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading importlib_metadata-8.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-proto<3,>=1.9.0 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: packaging<26 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow) (25.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.12.0 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow) (6.33.3)\n",
      "Collecting pydantic<3,>=2.0.0 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting python-dotenv<2,>=0.19.0 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow) (6.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow) (2.32.5)\n",
      "Collecting sqlparse<1,>=0.4.0 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading sqlparse-0.5.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow) (4.15.0)\n",
      "Collecting uvicorn<1 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading uvicorn-0.40.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting Mako (from alembic!=1.10.0,<2->mlflow)\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from click<9,>=7.0->mlflow-skinny==3.9.0->mlflow) (0.4.6)\n",
      "Collecting cffi>=2.0.0 (from cryptography<47,>=43.0.0->mlflow)\n",
      "  Downloading cffi-2.0.0-cp312-cp312-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting google-auth~=2.0 (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading google_auth-2.48.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\users\\vedant\\appdata\\roaming\\python\\python312\\site-packages (from docker<8,>=4.0.0->mlflow) (310)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (2.6.2)\n",
      "Collecting starlette<1.0.0,>=0.40.0 (from fastapi<1->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading starlette-0.52.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from fastapi<1->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting annotated-doc>=0.0.2 (from fastapi<1->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting blinker>=1.9.0 (from Flask<4->mlflow)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting itsdangerous>=2.2.0 (from Flask<4->mlflow)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from Flask<4->mlflow) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from Flask<4->mlflow) (2.1.5)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from Flask<4->mlflow) (3.1.5)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in c:\\users\\vedant\\appdata\\roaming\\python\\python312\\site-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
      "Collecting zipp>=3.20 (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from matplotlib<4->mlflow) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from matplotlib<4->mlflow) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from matplotlib<4->mlflow) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from matplotlib<4->mlflow) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from matplotlib<4->mlflow) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from matplotlib<4->mlflow) (3.2.5)\n",
      "Collecting opentelemetry-semantic-conventions==0.60b1 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from pandas<3->mlflow) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from pandas<3->mlflow) (2025.3)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=2.0.0->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3,>=2.0.0->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading pydantic_core-2.41.5-cp312-cp312-win_amd64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.9.0->mlflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.9.0->mlflow) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.9.0->mlflow) (2025.11.12)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading pyasn1-0.6.2-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from scikit-learn<2->mlflow) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\vedant\\anaconda3\\envs\\pytorch_cuda_venv\\lib\\site-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
      "Collecting prettytable>=3.9 (from skops<1->mlflow)\n",
      "  Downloading prettytable-3.17.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting greenlet>=1 (from sqlalchemy<3,>=1.4.0->mlflow)\n",
      "  Downloading greenlet-3.3.1-cp312-cp312-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting anyio<5,>=3.6.2 (from starlette<1.0.0,>=0.40.0->fastapi<1->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting h11>=0.8 (from uvicorn<1->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting pycparser (from cffi>=2.0.0->cryptography<47,>=43.0.0->mlflow)\n",
      "  Downloading pycparser-3.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\vedant\\appdata\\roaming\\python\\python312\\site-packages (from prettytable>=3.9->skops<1->mlflow) (0.2.13)\n",
      "Downloading mlflow-3.9.0-py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.8/9.7 MB 6.7 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.8/9.7 MB 4.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.6/9.7 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.1/9.7 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.5/9.7 MB 4.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.2/9.7 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.3/9.7 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.1/9.7 MB 4.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.9/9.7 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.7/9.7 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 4.0 MB/s  0:00:02\n",
      "Downloading mlflow_skinny-3.9.0-py3-none-any.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.8/2.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.6/2.8 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.6/2.8 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.8/2.8 MB 3.1 MB/s  0:00:00\n",
      "Downloading mlflow_tracing-3.9.0-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ------------------------------------- -- 1.3/1.4 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 5.2 MB/s  0:00:00\n",
      "Downloading alembic-1.18.3-py3-none-any.whl (262 kB)\n",
      "Downloading cachetools-6.2.6-py3-none-any.whl (11 kB)\n",
      "Downloading cryptography-46.0.4-cp311-abi3-win_amd64.whl (3.5 MB)\n",
      "   ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.8/3.5 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.6/3.5 MB 3.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 2.4/3.5 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.5/3.5 MB 3.9 MB/s  0:00:00\n",
      "Downloading databricks_sdk-0.86.0-py3-none-any.whl (797 kB)\n",
      "   ---------------------------------------- 0.0/797.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 797.4/797.4 kB 8.6 MB/s  0:00:00\n",
      "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Downloading fastapi-0.128.6-py3-none-any.whl (103 kB)\n",
      "Downloading flask-3.1.2-py3-none-any.whl (103 kB)\n",
      "Downloading flask_cors-6.0.2-py3-none-any.whl (13 kB)\n",
      "Downloading gitpython-3.1.46-py3-none-any.whl (208 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading google_auth-2.48.0-py3-none-any.whl (236 kB)\n",
      "Downloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
      "Downloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\n",
      "Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading huey-2.6.0-py3-none-any.whl (76 kB)\n",
      "Downloading importlib_metadata-8.7.1-py3-none-any.whl (27 kB)\n",
      "Downloading opentelemetry_api-1.39.1-py3-none-any.whl (66 kB)\n",
      "Downloading opentelemetry_proto-1.39.1-py3-none-any.whl (72 kB)\n",
      "Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl (132 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl (219 kB)\n",
      "Downloading pyarrow-22.0.0-cp312-cp312-win_amd64.whl (28.0 MB)\n",
      "   ---------------------------------------- 0.0/28.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/28.0 MB 2.4 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.6/28.0 MB 3.6 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 2.4/28.0 MB 3.8 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 2.9/28.0 MB 3.8 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 3.4/28.0 MB 3.3 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 4.2/28.0 MB 3.2 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 5.0/28.0 MB 3.2 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 5.8/28.0 MB 3.3 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 6.3/28.0 MB 3.2 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 7.1/28.0 MB 3.3 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 8.1/28.0 MB 3.4 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 8.9/28.0 MB 3.4 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 9.4/28.0 MB 3.5 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 10.5/28.0 MB 3.4 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 11.0/28.0 MB 3.4 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 11.0/28.0 MB 3.4 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 11.5/28.0 MB 3.3 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 12.8/28.0 MB 3.3 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 13.6/28.0 MB 3.3 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 14.4/28.0 MB 3.4 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 15.2/28.0 MB 3.4 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 16.0/28.0 MB 3.4 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 16.8/28.0 MB 3.4 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 17.6/28.0 MB 3.4 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 18.4/28.0 MB 3.5 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 19.1/28.0 MB 3.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 20.2/28.0 MB 3.5 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 21.0/28.0 MB 3.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 21.8/28.0 MB 3.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 22.5/28.0 MB 3.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 23.6/28.0 MB 3.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 24.4/28.0 MB 3.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 25.2/28.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 26.0/28.0 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 27.0/28.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  27.8/28.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.0/28.0 MB 3.6 MB/s  0:00:07\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 0.8/2.0 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.0/2.0 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 3.2 MB/s  0:00:00\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading skops-0.13.0-py3-none-any.whl (131 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading sqlalchemy-2.0.46-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 1.0/2.1 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 4.5 MB/s  0:00:00\n",
      "Downloading sqlparse-0.5.5-py3-none-any.whl (46 kB)\n",
      "Downloading starlette-0.52.1-py3-none-any.whl (74 kB)\n",
      "Downloading anyio-4.12.1-py3-none-any.whl (113 kB)\n",
      "Downloading uvicorn-0.40.0-py3-none-any.whl (68 kB)\n",
      "Downloading waitress-3.0.2-py3-none-any.whl (56 kB)\n",
      "Downloading annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading cffi-2.0.0-cp312-cp312-win_amd64.whl (183 kB)\n",
      "Downloading greenlet-3.3.1-cp312-cp312-win_amd64.whl (227 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading prettytable-3.17.0-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.2-py3-none-any.whl (83 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Downloading pycparser-3.0-py3-none-any.whl (48 kB)\n",
      "Installing collected packages: huey, zipp, waitress, typing-inspection, sqlparse, smmap, python-dotenv, pydantic-core, pycparser, pyasn1, pyarrow, prettytable, opentelemetry-proto, Mako, itsdangerous, h11, greenlet, graphql-core, cachetools, blinker, anyio, annotated-types, annotated-doc, uvicorn, starlette, sqlalchemy, rsa, pydantic, pyasn1-modules, importlib_metadata, graphql-relay, gitdb, Flask, docker, cffi, skops, opentelemetry-api, graphene, gitpython, Flask-CORS, fastapi, cryptography, alembic, opentelemetry-semantic-conventions, google-auth, opentelemetry-sdk, databricks-sdk, mlflow-tracing, mlflow-skinny, mlflow\n",
      "\n",
      "   ----------------------------------------  0/50 [huey]\n",
      "   ----------------------------------------  0/50 [huey]\n",
      "   ----------------------------------------  0/50 [huey]\n",
      "   ----------------------------------------  0/50 [huey]\n",
      "    ---------------------------------------  1/50 [zipp]\n",
      "   - --------------------------------------  2/50 [waitress]\n",
      "   -- -------------------------------------  3/50 [typing-inspection]\n",
      "   --- ------------------------------------  4/50 [sqlparse]\n",
      "   --- ------------------------------------  4/50 [sqlparse]\n",
      "   ---- -----------------------------------  5/50 [smmap]\n",
      "   ---- -----------------------------------  5/50 [smmap]\n",
      "   ---- -----------------------------------  6/50 [python-dotenv]\n",
      "   ----- ----------------------------------  7/50 [pydantic-core]\n",
      "   ------ ---------------------------------  8/50 [pycparser]\n",
      "   ------- --------------------------------  9/50 [pyasn1]\n",
      "   ------- --------------------------------  9/50 [pyasn1]\n",
      "   ------- --------------------------------  9/50 [pyasn1]\n",
      "   ------- --------------------------------  9/50 [pyasn1]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 10/50 [pyarrow]\n",
      "   -------- ------------------------------- 11/50 [prettytable]\n",
      "   --------- ------------------------------ 12/50 [opentelemetry-proto]\n",
      "   --------- ------------------------------ 12/50 [opentelemetry-proto]\n",
      "   --------- ------------------------------ 12/50 [opentelemetry-proto]\n",
      "   ---------- ----------------------------- 13/50 [Mako]\n",
      "   ---------- ----------------------------- 13/50 [Mako]\n",
      "   ---------- ----------------------------- 13/50 [Mako]\n",
      "   ---------- ----------------------------- 13/50 [Mako]\n",
      "   ----------- ---------------------------- 14/50 [itsdangerous]\n",
      "   ------------ --------------------------- 15/50 [h11]\n",
      "   ------------ --------------------------- 16/50 [greenlet]\n",
      "   ------------ --------------------------- 16/50 [greenlet]\n",
      "   ------------ --------------------------- 16/50 [greenlet]\n",
      "   ------------- -------------------------- 17/50 [graphql-core]\n",
      "   ------------- -------------------------- 17/50 [graphql-core]\n",
      "   ------------- -------------------------- 17/50 [graphql-core]\n",
      "   ------------- -------------------------- 17/50 [graphql-core]\n",
      "   ------------- -------------------------- 17/50 [graphql-core]\n",
      "   ------------- -------------------------- 17/50 [graphql-core]\n",
      "   ------------- -------------------------- 17/50 [graphql-core]\n",
      "   ------------- -------------------------- 17/50 [graphql-core]\n",
      "   ------------- -------------------------- 17/50 [graphql-core]\n",
      "   ------------- -------------------------- 17/50 [graphql-core]\n",
      "   ------------- -------------------------- 17/50 [graphql-core]\n",
      "   ------------- -------------------------- 17/50 [graphql-core]\n",
      "   --------------- ------------------------ 19/50 [blinker]\n",
      "   ---------------- ----------------------- 20/50 [anyio]\n",
      "   ---------------- ----------------------- 20/50 [anyio]\n",
      "   ---------------- ----------------------- 20/50 [anyio]\n",
      "   ---------------- ----------------------- 20/50 [anyio]\n",
      "   ---------------- ----------------------- 20/50 [anyio]\n",
      "   ---------------- ----------------------- 21/50 [annotated-types]\n",
      "   ------------------ --------------------- 23/50 [uvicorn]\n",
      "   ------------------ --------------------- 23/50 [uvicorn]\n",
      "   ------------------ --------------------- 23/50 [uvicorn]\n",
      "   ------------------ --------------------- 23/50 [uvicorn]\n",
      "   ------------------- -------------------- 24/50 [starlette]\n",
      "   ------------------- -------------------- 24/50 [starlette]\n",
      "   ------------------- -------------------- 24/50 [starlette]\n",
      "   ------------------- -------------------- 24/50 [starlette]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 25/50 [sqlalchemy]\n",
      "   -------------------- ------------------- 26/50 [rsa]\n",
      "   --------------------- ------------------ 27/50 [pydantic]\n",
      "   --------------------- ------------------ 27/50 [pydantic]\n",
      "   --------------------- ------------------ 27/50 [pydantic]\n",
      "   --------------------- ------------------ 27/50 [pydantic]\n",
      "   --------------------- ------------------ 27/50 [pydantic]\n",
      "   --------------------- ------------------ 27/50 [pydantic]\n",
      "   --------------------- ------------------ 27/50 [pydantic]\n",
      "   --------------------- ------------------ 27/50 [pydantic]\n",
      "   --------------------- ------------------ 27/50 [pydantic]\n",
      "   --------------------- ------------------ 27/50 [pydantic]\n",
      "   --------------------- ------------------ 27/50 [pydantic]\n",
      "   --------------------- ------------------ 27/50 [pydantic]\n",
      "   --------------------- ------------------ 27/50 [pydantic]\n",
      "   ---------------------- ----------------- 28/50 [pyasn1-modules]\n",
      "   ---------------------- ----------------- 28/50 [pyasn1-modules]\n",
      "   ---------------------- ----------------- 28/50 [pyasn1-modules]\n",
      "   ---------------------- ----------------- 28/50 [pyasn1-modules]\n",
      "   ---------------------- ----------------- 28/50 [pyasn1-modules]\n",
      "   ---------------------- ----------------- 28/50 [pyasn1-modules]\n",
      "   ---------------------- ----------------- 28/50 [pyasn1-modules]\n",
      "   ---------------------- ----------------- 28/50 [pyasn1-modules]\n",
      "   ---------------------- ----------------- 28/50 [pyasn1-modules]\n",
      "   ---------------------- ----------------- 28/50 [pyasn1-modules]\n",
      "   ---------------------- ----------------- 28/50 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 29/50 [importlib_metadata]\n",
      "   ------------------------ --------------- 30/50 [graphql-relay]\n",
      "   ------------------------ --------------- 31/50 [gitdb]\n",
      "   ------------------------ --------------- 31/50 [gitdb]\n",
      "   ------------------------ --------------- 31/50 [gitdb]\n",
      "   ------------------------- -------------- 32/50 [Flask]\n",
      "   ------------------------- -------------- 32/50 [Flask]\n",
      "   ------------------------- -------------- 32/50 [Flask]\n",
      "   -------------------------- ------------- 33/50 [docker]\n",
      "   -------------------------- ------------- 33/50 [docker]\n",
      "   -------------------------- ------------- 33/50 [docker]\n",
      "   -------------------------- ------------- 33/50 [docker]\n",
      "   -------------------------- ------------- 33/50 [docker]\n",
      "   -------------------------- ------------- 33/50 [docker]\n",
      "   --------------------------- ------------ 34/50 [cffi]\n",
      "   --------------------------- ------------ 34/50 [cffi]\n",
      "   ---------------------------- ----------- 35/50 [skops]\n",
      "   ---------------------------- ----------- 35/50 [skops]\n",
      "   ---------------------------- ----------- 35/50 [skops]\n",
      "   ---------------------------- ----------- 35/50 [skops]\n",
      "   ---------------------------- ----------- 35/50 [skops]\n",
      "   ---------------------------- ----------- 36/50 [opentelemetry-api]\n",
      "   ---------------------------- ----------- 36/50 [opentelemetry-api]\n",
      "   ---------------------------- ----------- 36/50 [opentelemetry-api]\n",
      "   ----------------------------- ---------- 37/50 [graphene]\n",
      "   ----------------------------- ---------- 37/50 [graphene]\n",
      "   ----------------------------- ---------- 37/50 [graphene]\n",
      "   ----------------------------- ---------- 37/50 [graphene]\n",
      "   ----------------------------- ---------- 37/50 [graphene]\n",
      "   ----------------------------- ---------- 37/50 [graphene]\n",
      "   ----------------------------- ---------- 37/50 [graphene]\n",
      "   ----------------------------- ---------- 37/50 [graphene]\n",
      "   ----------------------------- ---------- 37/50 [graphene]\n",
      "   ------------------------------ --------- 38/50 [gitpython]\n",
      "   ------------------------------ --------- 38/50 [gitpython]\n",
      "   ------------------------------ --------- 38/50 [gitpython]\n",
      "   ------------------------------ --------- 38/50 [gitpython]\n",
      "   ------------------------------ --------- 38/50 [gitpython]\n",
      "   -------------------------------- ------- 40/50 [fastapi]\n",
      "   -------------------------------- ------- 40/50 [fastapi]\n",
      "   -------------------------------- ------- 40/50 [fastapi]\n",
      "   -------------------------------- ------- 40/50 [fastapi]\n",
      "   -------------------------------- ------- 40/50 [fastapi]\n",
      "   -------------------------------- ------- 41/50 [cryptography]\n",
      "   -------------------------------- ------- 41/50 [cryptography]\n",
      "   -------------------------------- ------- 41/50 [cryptography]\n",
      "   -------------------------------- ------- 41/50 [cryptography]\n",
      "   -------------------------------- ------- 41/50 [cryptography]\n",
      "   -------------------------------- ------- 41/50 [cryptography]\n",
      "   -------------------------------- ------- 41/50 [cryptography]\n",
      "   -------------------------------- ------- 41/50 [cryptography]\n",
      "   --------------------------------- ------ 42/50 [alembic]\n",
      "   --------------------------------- ------ 42/50 [alembic]\n",
      "   --------------------------------- ------ 42/50 [alembic]\n",
      "   --------------------------------- ------ 42/50 [alembic]\n",
      "   --------------------------------- ------ 42/50 [alembic]\n",
      "   --------------------------------- ------ 42/50 [alembic]\n",
      "   --------------------------------- ------ 42/50 [alembic]\n",
      "   --------------------------------- ------ 42/50 [alembic]\n",
      "   --------------------------------- ------ 42/50 [alembic]\n",
      "   ---------------------------- ---- 43/50 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 43/50 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 43/50 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 43/50 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 43/50 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 43/50 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 43/50 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 43/50 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 43/50 [opentelemetry-semantic-conventions]\n",
      "   ----------------------------------- ---- 44/50 [google-auth]\n",
      "   ----------------------------------- ---- 44/50 [google-auth]\n",
      "   ----------------------------------- ---- 44/50 [google-auth]\n",
      "   ----------------------------------- ---- 44/50 [google-auth]\n",
      "   ----------------------------------- ---- 44/50 [google-auth]\n",
      "   ----------------------------------- ---- 44/50 [google-auth]\n",
      "   ----------------------------------- ---- 44/50 [google-auth]\n",
      "   ------------------------------------ --- 45/50 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 45/50 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 45/50 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 45/50 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 45/50 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 46/50 [databricks-sdk]\n",
      "   ------------------------------------ --- 46/50 [databricks-sdk]\n",
      "   ------------------------------------ --- 46/50 [databricks-sdk]\n",
      "   ------------------------------------ --- 46/50 [databricks-sdk]\n",
      "   ------------------------------------ --- 46/50 [databricks-sdk]\n",
      "   ------------------------------------ --- 46/50 [databricks-sdk]\n",
      "   ------------------------------------ --- 46/50 [databricks-sdk]\n",
      "   ------------------------------------ --- 46/50 [databricks-sdk]\n",
      "   ------------------------------------ --- 46/50 [databricks-sdk]\n",
      "   ------------------------------------ --- 46/50 [databricks-sdk]\n",
      "   ------------------------------------ --- 46/50 [databricks-sdk]\n",
      "   ------------------------------------- -- 47/50 [mlflow-tracing]\n",
      "   ------------------------------------- -- 47/50 [mlflow-tracing]\n",
      "   ------------------------------------- -- 47/50 [mlflow-tracing]\n",
      "   ------------------------------------- -- 47/50 [mlflow-tracing]\n",
      "   ------------------------------------- -- 47/50 [mlflow-tracing]\n",
      "   ------------------------------------- -- 47/50 [mlflow-tracing]\n",
      "   ------------------------------------- -- 47/50 [mlflow-tracing]\n",
      "   ------------------------------------- -- 47/50 [mlflow-tracing]\n",
      "   ------------------------------------- -- 47/50 [mlflow-tracing]\n",
      "   ------------------------------------- -- 47/50 [mlflow-tracing]\n",
      "   ------------------------------------- -- 47/50 [mlflow-tracing]\n",
      "   ------------------------------------- -- 47/50 [mlflow-tracing]\n",
      "   ------------------------------------- -- 47/50 [mlflow-tracing]\n",
      "   ------------------------------------- -- 47/50 [mlflow-tracing]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   -------------------------------------- - 48/50 [mlflow-skinny]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------  49/50 [mlflow]\n",
      "   ---------------------------------------- 50/50 [mlflow]\n",
      "\n",
      "Successfully installed Flask-3.1.2 Flask-CORS-6.0.2 Mako-1.3.10 alembic-1.18.3 annotated-doc-0.0.4 annotated-types-0.7.0 anyio-4.12.1 blinker-1.9.0 cachetools-6.2.6 cffi-2.0.0 cryptography-46.0.4 databricks-sdk-0.86.0 docker-7.1.0 fastapi-0.128.6 gitdb-4.0.12 gitpython-3.1.46 google-auth-2.48.0 graphene-3.4.3 graphql-core-3.2.7 graphql-relay-3.2.0 greenlet-3.3.1 h11-0.16.0 huey-2.6.0 importlib_metadata-8.7.1 itsdangerous-2.2.0 mlflow-3.9.0 mlflow-skinny-3.9.0 mlflow-tracing-3.9.0 opentelemetry-api-1.39.1 opentelemetry-proto-1.39.1 opentelemetry-sdk-1.39.1 opentelemetry-semantic-conventions-0.60b1 prettytable-3.17.0 pyarrow-22.0.0 pyasn1-0.6.2 pyasn1-modules-0.4.2 pycparser-3.0 pydantic-2.12.5 pydantic-core-2.41.5 python-dotenv-1.2.1 rsa-4.9.1 skops-0.13.0 smmap-5.0.2 sqlalchemy-2.0.46 sqlparse-0.5.5 starlette-0.52.1 typing-inspection-0.4.2 uvicorn-0.40.0 waitress-3.0.2 zipp-3.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1f6134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/5000 [00:00<01:10, 70.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.40033, Accuracy: 81.92% | Test loss: 0.40189, Test acc: 81.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|        | 1056/5000 [00:02<00:07, 502.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 | Loss: 0.39386, Accuracy: 82.20% | Test loss: 0.39689, Test acc: 81.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|     | 2073/5000 [00:04<00:04, 597.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2000 | Loss: 0.39110, Accuracy: 82.31% | Test loss: 0.39481, Test acc: 81.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 3114/5000 [00:05<00:03, 597.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3000 | Loss: 0.38954, Accuracy: 82.43% | Test loss: 0.39366, Test acc: 81.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%| | 4086/5000 [00:07<00:01, 602.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4000 | Loss: 0.38866, Accuracy: 82.38% | Test loss: 0.39309, Test acc: 81.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:09<00:00, 547.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4999 | Loss: 0.38818, Accuracy: 82.40% | Test loss: 0.39285, Test acc: 81.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Put data to target device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Build training and evaluation loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model_1.train()\n",
    "\n",
    "    # 1. Forward pass (model outputs raw logits)\n",
    "    y_logits = model_1(X_train).squeeze() # squeeze to remove extra `1` dimensions, this won't work unless model and data are on same device \n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labls\n",
    "  \n",
    "    # 2. Calculate loss/accuracy\n",
    "    # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()\n",
    "    #                y_train) \n",
    "    loss = loss_fn(y_logits, # Using nn.BCEWithLogitsLoss works with raw logits\n",
    "                   y_train) \n",
    "    acc = accuracy_fn(y_true=y_train, \n",
    "                      y_pred=y_pred) \n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model_1.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass\n",
    "        test_logits = model_1(X_test).squeeze() \n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "        # 2. Caculate loss/accuracy\n",
    "        test_loss = loss_fn(test_logits,\n",
    "                            y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test,\n",
    "                               y_pred=test_pred)\n",
    "\n",
    "    # Print out what's happening every 10 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n",
    "print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_cuda_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
